from enum import Enum
from typing import Type, Tuple, Any, Dict, List, Optional, OrderedDict, Union, TypeVar, Iterator, Iterable, Sequence, SupportsInt, overload, get_origin, get_args, Callable, TypeAlias
from dataclasses import dataclass, field
import torch
from torch._C import Node, Graph
from utils import typeify, _print_value, _short_dtype, first
from ansi.color import bg, fg
from ansi.color.fx import reset
import copy
import inspect

TorchType: TypeAlias = 'torch._C.JitType'

def _is_optional(field):
    """
    Returns whether the annotation is generated by typing.Optional[...]
    """
    return get_origin(field) is Union and \
           type(None) in get_args(field)

def _scalar_torch_type(tp: Type) -> TorchType:
    """
    Given the given scalar type, return the corresponding Torch type.
    """
    NoneType = type(None)
    if issubclass(tp, int):
        return torch.IntType.get()
    elif issubclass(tp, float):
        return torch.FloatType.get()
    elif issubclass(tp, bool):
        return torch.BoolType.get()
    elif tp == NoneType:
        return torch.NoneType.get()
    elif issubclass(tp, str):
        return torch.StringType.get()
    raise NotImplementedError(f"_scalar_torch_type for {tp}")

def _to_torch_type(ann: Any, samples: List[Any]) -> TorchType:
    """
    Converts the given parameter into its corresponding Torch JIT type
    """

    #print(p)

    origin = get_origin(ann)
    args = get_args(ann)

    if origin is None:
        if ann is None or ann is inspect._empty:
            # Do it based on the type seen
            if len(samples) == 0:
                # NO samples and no annotation
                return torch.AnyType()
            else:
                def get_type(sample) -> TorchType:
                    if isinstance(sample, torch.Tensor):
                        return torch.TensorType.create_from_tensor(sample)
                    elif sample is None:
                        return torch.NoneType.get()

                    raise RuntimeError(f"TODO: get_type {type(sample)}")
                
                current: Optional[TorchType] = None
                for t in [get_type(sample) for sample in samples]:
                    if current is None:
                        current = t
                    else:
                        if str(current) != str(t):
                            raise RuntimeError(f"wrong type {current} {t}")

                assert current is not None
                return current

            raise RuntimeError(f"empty {_print_value(samples)}")
        elif issubclass(ann, torch.Tensor):
            return torch.TensorType.create_from_tensor(ann())  
        elif issubclass(ann, torch.FloatTensor):
            return torch.TensorType.create_from_tensor(ann())
        elif ann is bool:
            return torch.BoolType.get()
        else:
            raise RuntimeError(f"TODO: no origin for {ann}")
    elif _is_optional(ann):
        contained,_ = args
        sub_samples = [s for s in samples if s is not None]
        #print("contained", contained)
        return torch.OptionalType(_to_torch_type(contained, sub_samples))
    elif origin == tuple:
        def get_el_type(i: int) -> TorchType:
            sub_samples = [s[i] for s in samples]
            return _to_torch_type(args[i], sub_samples)

        return torch.TupleType([get_el_type(i) for i in range(len(args))])

        raise RuntimeError(f"TODO: handle optional {contained}")


    print(ann, type(ann), get_origin(ann), get_args(ann))

    raise RuntimeError(f"_to_torch_type for {ann} {get_origin(ann)} {get_args(ann)}")

class Origin(Enum):
    UNKNOWN = 0
    SELF = 1
    ARG = 2
    DEFAULT_ARG = 3
    LOCAL = 4
    CONST_PROP = 5
    SAMPLE = 6

Tp = TypeVar('Tp')

class VariableInfo:
    """
    Holds the static information known about a variable.
    """
    name: str = ""
    owner: 'Variables'
    origin: Origin = Origin.UNKNOWN

    def print_value(self) -> str:
        raise RuntimeError(f"Must override print_value() for class {type(self).__name__}")

    def const_type(self) -> type:
        raise RuntimeError(f"Must override const_type() for class {type(self).__name__}")

    def torch_type(self) -> TorchType:
        raise RuntimeError(f"Must override torch_type() for class {type(self).__name__}")

    def is_const(self) -> bool:
        raise RuntimeError(f"Must override is_const() for class {type(self).__name__}")

    def const_value(self) -> Any:
        if self.is_const():
            raise RuntimeError(f"Must override const_value() if is_const() can be true for type {type(self)}")
        else:
            raise RuntimeError("Attempt to access const value for non-const variable")

    def is_none(self) -> bool:
        """
        Return true if this is a constant None value
        """
        return self.const_type() == type(None)

    def is_optional(self) -> bool:
        return False

    def is_tensor(self) -> bool:
        return False

    def is_sequence(self) -> bool:
        return False

    def is_sequence_chunk(self) -> bool:
        return False

    def sequence_length_is_const(self) -> bool:
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_const_length(self) -> int:
        if self.sequence_length_is_const():
            raise RuntimeError(f"Must override if sequence_length_is_const() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_length(self) -> 'VariableInfo':
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_element_at_index(self, i: int) -> 'VariableInfo':
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def element_type_is_const(self) -> bool:
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")
        
    def element_const_type(self) -> type:
        if self.element_type_is_const():
            raise RuntimeError(f"Must override if element_const_type() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def tensor_dtype_is_const(self) -> bool:
        return self.tensor_const_dtype() is not None

    def tensor_const_dtype(self) -> Optional[torch.dtype]:
        if self.is_tensor():
            raise RuntimeError(f"Must override tensor_const_dtype() if is_tensor() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call tensor_const_dtype() for non-tensor type {type(self).__name__} {self}")

    def tensor_dtype(self) -> 'VariableInfo':
        """
        Returns the variable containing the dtype of this tensor.
        """
        raise RuntimeError(f"Attempt to call tensor_dtype() for non-tensor type {type(self).__name__} {self}")

    def tensor_device_is_const(self) -> bool:
        return self.tensor_const_device() is not None

    def tensor_const_device(self) -> Optional[torch.device]:
        if self.is_tensor():
            raise RuntimeError(f"Must override const_device() if is_tensor() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call const_device() for non-tensor type {type(self).__name__} {self}")

    def tensor_device(self) -> 'VariableInfo':
        """
        Returns the variable containing the dtype of this tensor.
        """
        raise RuntimeError(f"Attempt to call const_device() for non-tensor type {type(self).__name__} {self}")

    def tensor_shape(self) -> 'VariableInfo':
        """
        Returns the variable containing the shape of this tensor.
        """
        if self.is_tensor():
            raise RuntimeError(f"Must override tensor_shape() if is_tensor() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call tensor_shape() for non-tensor type {type(self).__name__} {self}")

    # Which node produced, and which nodes read, this value.  The integers are the sequence
    # of nodes in the graph.
    produced_by: Optional[Node|Graph] = None
    produced: int = 0
    first_consumed: int = 10000
    last_consumed: int = 0

    def __init__(self, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        self.name = name
        self.owner = owner
        self.origin = origin
        self.produced_by = produced_by
        self.produced = produced

    def typed_const_value(self, tp: Type[Tp]) -> Optional[Tp]:
        """
        If this variable is a constant, check that its and instance of the given type and return it.
        If it's not a constant, return None.
        Used to extract values of constant arguments from VariableInfo during constant propagation.
        """
        if not self.is_const():
            return None
        if not isinstance(self.const_value(), tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value())} of value {self.const_value()}")
        return self.const_value()

    def typed_const_nonnull_value(self, tp: Type[Tp]) -> Tp:
        """
        Assert that this variable is a constant, of the given type, and return that value.
        Used to extract values of constant arguments from VariableInfo during constant propagation.
        """
        if not self.is_const():
            raise RuntimeError(f"Expected constant value but got {self}")
        if not isinstance(self.const_value(), tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value())} of value {self.const_value()}")
        return self.const_value()

    def typed_default_value(self, default: Tp) -> Optional[Tp]:
        """
        If this variable is a constant and not None, check that its and instance of the given type and return it.
        If it's a constant and None, then return the default.
        If it's not a constant, return None.
        Used to extract values of defaulted constant arguments from VariableInfo during constant propagation.
        """
        tp = type(default)
        if not self.is_const():
            return None
        if self.const_value() is None:
            return default
        if not isinstance(self.const_value(), tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value())} of value {self.const_value()}")
        return self.const_value()

    def duplicate(self) -> 'VariableInfo':
        """
        Duplicate operator.  Needed to create another version because deepcopy(x) doesn't work and some objects are
        mutable.
        """
        raise RuntimeError(f"TODO: duplicate() for {type(self)}")
        result = copy.copy(self)
        if self.seq_els is not None:
            result.seq_els = [el.deepcopy() for el in self.seq_els]
        if self.seq_length is not None:
            result.seq_length = copy.copy(self.seq_length)
        if self.tensor_shape is not None:
            result.tensor_shape = copy.deepcopy(self.tensor_shape)
        return result

    def renamed(self, new_name: str) -> 'VariableInfo':
        res = self.duplicate()
        res.name = new_name
        return res

    def is_homogeneous(self, other: 'VariableInfo'):
        """
        Are these homogeneous, meaning that they can be combined into a single
        VariableInfo that covers both of them.
        """
        if self.const_type() != other.const_type():
            return False
        if self.is_const() and other.is_const():
            return self.const_value() == other.const_value()
        if self.const_type() == torch.Tensor:
            type1 = self.const_dtype()
            type2 = other.const_dtype()

            if type1 != type2:
                # TODO: they may be homogeneous; there may be a type that covers both
                return False

            device1 = self.const_device
            device2 = other.const_device

            if device1 != device2:
                return False

            return True
        elif self.const_type() == tuple or self.const_type() == list:
            raise RuntimeError("is_homogeneous for sequence")

        return True

    def combine(self, other: 'VariableInfo') -> 'VariableInfo':
        """
        Combine the two VariableInfos together to create one that covers both.
        """
        raise RuntimeError(f"need to implement combine() on {type(self).__name__}")

        raise NotImplementedError(f"combine(): {self} with {other}")

        if other.is_optional:
            self.is_optional = True

        if other.const_type() != self.const_type():
            raise RuntimeError("TODO: combine with two different types")

        if self.is_const() and not other.is_const():
            self.const_value = None
            self.is_const = False

        if self.const_type() == torch.Tensor:
            if self.const_dtype != other.const_dtype:
                self.const_dtype = None
            if self.const_device != other.const_device:
                self.const_device = None
            if other.tensor_shape() is not None and self.tensor_shape() is not None:
                for sh in other.tensor_shape().lengths.values():
                    self.tensor_shape().add(sh)
        elif self.const_type() == tuple or self.const_type() == list:
            raise RuntimeError("TODO: combine sequences")

        self.produced = min(self.produced, other.produced)
        self.first_consumed = min(self.first_consumed, other.first_consumed)
        self.last_consumed = max(self.last_consumed, other.last_consumed)

class ConstantVariable(VariableInfo):
    # Anything which is constant

    value: Any

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: Any):
        super().__init__(name, owner, origin, produced_by, produced)
        self.value = value

    def __repr__(self) -> str:
        return f"{self.name}: const {type(self.value).__name__} = {self.print_value()}"

    # Iterator class that wraps values in a VariableInfo
    class MyIterator:
        it: Iterator
        owner: VariableInfo
        i: int
        def __init__(self, it: Iterator, owner: VariableInfo):
            self.it = it
            self.owner = owner
            self.i = 0
        def __next__(self):
            val = next(self.it)
            result = self.owner.owner.constant(name=self.owner.name+"#el"+str(self.i),origin=self.owner.origin,value=val,produced_by=self.owner.produced_by,produced=self.owner.produced)
            self.i += 1
            return result

    def __iter__(self) -> MyIterator:
        return ConstantVariable.MyIterator(iter(self.value),self)

    def __len__(self) -> int:
        return len(self.value)

    def __getitem__(self, item) -> VariableInfo:
        return self.owner.constant(name=self.name+"#el"+str(item),origin=self.origin,value=self.value[item],produced_by=self.produced_by,produced=self.produced)

    def print_value(self) -> str:
        return _print_value(self.value)

    def is_const(self) -> bool:
        return True

    def const_type(self) -> type:
        return type(self.value)

    def torch_type(self) -> TorchType:
        return _to_torch_type(None, [self.value])

    def const_value(self) -> Any:
        return self.value

    def is_sequence(self) -> bool:
        return isinstance(self.value, list) or isinstance(self.value, tuple)

    def sequence_length_is_const(self) -> bool:
        return True

    def is_tensor(self) -> bool:
        return isinstance(self.value, torch.Tensor)

    def tensor_dtype(self) -> VariableInfo:
        if isinstance(self.value, torch.Tensor):
            return self.owner.constant(name=self.name + ".#dtype", value=self.value.dtype, origin=self.origin, produced=self.produced, produced_by=self.produced_by)
        raise RuntimeError(f"Attempt to get tensor_dtype from non-tensor constant value {self}")

    def tensor_const_dtype(self) -> Optional[torch.dtype]:
        if isinstance(self.value, torch.Tensor):
            return self.value.dtype
        raise RuntimeError(f"cannot get tensor dtype for non-tensor constant value {self}")

    def tensor_device(self) -> VariableInfo:
        if isinstance(self.value, torch.Tensor):
            return self.owner.constant(name=self.name + ".#device", value=self.value.device, origin=self.origin, produced=self.produced, produced_by=self.produced_by)
        raise RuntimeError(f"Attempt to get tensor_device from non-tensor constant value {self}")

    def tensor_const_device(self) -> Optional[torch.device]:
        if isinstance(self.value, torch.Tensor):
            return self.value.device
        raise RuntimeError("cannot get tensor device for non-tensor constant value")

    def tensor_shape(self) -> VariableInfo:
        if isinstance(self.value, torch.Tensor):
            return self.owner.constant(name=self.name+".#shape", origin=self.origin, value=list(self.value.shape),
                                       produced_by=self.produced_by,produced=self.produced)
            return self.value.device
        raise RuntimeError("cannot get tensor device for non-tensor constant value")

    def duplicate(self) -> VariableInfo: return ConstantVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)

    def combine(self, other: 'VariableInfo') -> 'VariableInfo':
        type1 = self.const_type()
        type2 = other.const_type()
        if type1 != type2:
            raise NotImplementedError(f"combine differing types {type1} {type2}")

        if isinstance(other, ConstantVariable):
            raise NotImplementedError(f"combine {self} {other}")
        elif isinstance(other, ScalarVariable):
            return other.duplicate()
        else:
            raise NotImplementedError(f"combine {self} {other}")

class TensorVariable(VariableInfo):
    # The following are for Tensors.  We model the data type, device and shape separately.
    dtype: VariableInfo
    device: VariableInfo
    shape: VariableInfo

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 dtype: VariableInfo, device: VariableInfo, shape: VariableInfo):
        super().__init__(name, owner, origin, produced_by,produced)
        assert issubclass(dtype.const_type(), torch.dtype)
        assert issubclass(device.const_type(), torch.device)
        print("shape", shape)
        assert shape.is_sequence()
        self.dtype = dtype
        self.device = device
        self.shape = shape

    def __repr__(self) -> str:
        return f"{self.name}: tensor {_print_tensor_info(self)}"

    def print_value(self) -> str: return _print_tensor_info(self)
    def const_type(self) -> type: return torch.Tensor
    def torch_type(self) -> TorchType: raise NotImplementedError()  # TODO
    def is_const(self) -> bool: return False
    def is_tensor(self) -> bool: return True
    def tensor_dtype(self) -> VariableInfo: return self.dtype
    def tensor_device(self) -> VariableInfo: return self.device
    def tensor_const_dtype(self) -> Optional[torch.dtype]:
        if self.dtype.is_const():
            return self.dtype.const_value()
        return None
    def tensor_const_device(self) -> Optional[torch.device]:
        if self.device.is_const():
            return self.device.const_value()
        return None
    def tensor_shape(self) -> VariableInfo: return self.shape

    def duplicate(self) -> VariableInfo:
        return TensorVariable(name=self.name, owner=self.owner, origin=self.origin, produced_by=self.produced_by,
                                produced=self.produced, dtype=self.dtype.duplicate(), device=self.device.duplicate(), shape=self.shape.duplicate())

    @staticmethod
    def sampled(name: str, tp: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':

        val_counts: Dict[Any, int] = {}
        for v in samples:
            if v in val_counts:
                val_counts[v] += 1
            else:
                val_counts[v] = 1

        tensor_dtypes: List[torch.dtype] = []
        tensor_devices: List[torch.device] = []
        tensor_shapes: List[List[int]] = []

        for s in samples:
            assert isinstance(s, torch.Tensor)
            tensor_dtypes.append(s.dtype)
            tensor_devices.append(s.device)
            tensor_shapes.append(list(s.shape))

        dtype = vars.sampled(name=name+".#dtype", torch_type=torch.IntType.get(), samples=tensor_dtypes, produced_by=produced_by, produced=produced)
        device = vars.sampled(name=name+".#device", torch_type=torch.DeviceObjType.get(), samples=tensor_devices, produced_by=produced_by, produced=produced)
        shape = TensorShapeVariable.sampled(name=name+".#shape", torch_type=torch.ListType(torch.IntType.get()), samples=tensor_shapes, produced_by=produced_by, produced=produced, vars=vars)

        return vars.tensor(name=name, origin=Origin.SAMPLE, dtype=dtype, device=device, shape=shape, produced_by=produced_by, produced=produced)

class SequenceChunk(VariableInfo):
    # This is a variable length chunk of unknown elements, each of which has a schema
    # that represents its value.

    el: VariableInfo
    length: VariableInfo

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 el: VariableInfo, length: VariableInfo):
        super().__init__(name, owner, origin, produced_by,produced)
        self.el = el
        self.length = length

    def print_value(self) -> str:
        result = "..." + self.el.print_value() + "[" + self.length.print_value() + "]..."
        return result

    def is_sequence_chunk(self) -> bool: return True


class SequenceVariable(VariableInfo):

    # The following are for sequences (lists and tuples).  They are modeled as a series of VariableInfo values, which
    # each represent either a list item or a variable length chunk of elements in a list.
    tp: Type # normally tuple or list
    seq_els: List[VariableInfo]

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 tp: Type, seq_els: List[VariableInfo]):
        super().__init__(name, owner, origin, produced_by,produced)
        self.tp = tp
        self.seq_els = seq_els

    def print_value(self) -> str:
        brackets = "<>"
        if issubclass(self.tp, tuple):
            brackets = "()"
        elif issubclass(self.tp, list):
            brackets = "[]"
        else:
            raise RuntimeError("unknown sequence type")
        result = brackets[0] + ','.join([e.print_value() for e in self.seq_els]) + brackets[1]
        return result

    def is_const(self) -> bool: return False
    def is_tensor(self) -> bool: return False
    def is_sequence(self) -> bool: return True
    def const_type(self) -> type: return self.tp
    def torch_type(self) -> TorchType:
        torch_types = [el.torch_type() for el in self.seq_els]
        if issubclass(self.tp, list):
            common: TorchType = _common_torch_type(*torch_types)
            return torch.ListType(common)
        elif issubclass(self.tp, tuple):
            return torch.TupleType(torch_types)
        else:
            raise RuntimeError("Unknown sequence torch type")

    def sequence_length(self) -> 'VariableInfo': raise NotImplementedError()
    def sequence_length_is_const(self) -> bool:
        for el in self.seq_els:
            if el.is_sequence_chunk() and not el.sequence_length_is_const():
                return False
        return True

    def sequence_const_length(self) -> int:
        res = 0
        for el in self.seq_els:
            if el.is_sequence_chunk():
                res += el.sequence_const_length()
            else:
                res += 1
        return res

    def sequence_element_at_index(self, i: int) -> VariableInfo:
        assert self.seq_els is not None
        return self.seq_els[i]

    def __iter__(self) -> Iterator:
        assert self.seq_els is not None
        return iter(self.seq_els)

    def __next__(self, iter: Iterator) -> Optional[Iterator]:
        raise NotImplementedError("TODO")

    def __getitem__(self, item: int) -> VariableInfo:
        assert self.seq_els is not None
        return self.seq_els[item]

    def duplicate(self) -> VariableInfo:
        new_seq_els = list([e.duplicate() for e in self.seq_els])
        return SequenceVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,
                                tp=self.tp,seq_els=new_seq_els)

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':
        print("torch_type", torch_type)

        assert isinstance(torch_type, torch.TupleType)
        els = torch_type.elements()

        length_counts: Dict[int, int] = {}
        length_samples: List[int] = []
        too_long = 0
        too_short = 0

        for v in samples:
            l = len(v)
            length_samples.append(l)
            if l < len(els):
                too_short += 1
            if l > len(els):
                too_long += 1
            if l in length_counts:
                length_counts[l] += 1
            else:
                length_counts[l] = 1

        print("length_counts", length_counts)

        if len(els) == 1 and too_long > 0:
            # It was written as tuple[x] but actually it means list[x] as a tuple
            all_samples = []
            for v in samples:
                all_samples.extend(v)

            el_info = vars.sampled(name=name+".#els", torch_type=els[0], samples=all_samples, produced_by=produced_by, produced=produced)
            len_info = IntVariable.sampled(name=name+".#length", torch_type=torch.IntType.get(), samples=length_samples, produced_by=produced_by, produced=produced, vars=vars)
            return vars.homogeneous_sequence(name=name, origin=Origin.SAMPLE, tp=tuple, produced_by=produced_by, produced=produced, length=len_info, values=el_info)

        if len(length_counts) > 1:
            raise NotImplementedError("multiple lengths")

        l = len(samples[0])
        el_samples: List[List[Any]] = [[] for _ in range(l)]
        for v in samples:
            for s,el in zip(el_samples,v):
                s.append(el)

        el_info = [vars.sampled(name=name+".#el"+str(i),torch_type=ttp, samples=s, produced_by=produced_by, produced=produced) for i,(ttp,s) in enumerate(zip(els, el_samples))]

        return vars.inhomogeneous_sequence(name=name, origin=Origin.SAMPLE, tp=tuple, produced_by=produced_by, produced=produced, values=el_info)


class TensorShapeVariable(VariableInfo):
    dims: List[VariableInfo]

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 dims: List[VariableInfo]):
        super().__init__(name, owner, origin, produced_by,produced)
        self.dims = dims
        for d in dims:
            assert isinstance(d, VariableInfo)

    def __repr__(self) -> str:
        return f"{self.name}: {self.print_value()}"

    def __iter__(self) -> Iterator: return iter(self.dims)
    def __next__(self, iter: Iterator) -> Optional[Iterator]: return next(iter)
    def __len__(self) -> int: return len(self.dims)
    def __getitem__(self, item: int) -> VariableInfo: return self.dims[item]

    def print_value(self) -> str:
        return f"[{','.join([x.print_value() for x in self.dims])}]"

    def is_const(self) -> bool:
        for d in self.dims:
            if not d.is_const():
                return False
        return True

    def const_type(self) -> type: return list
    def torch_type(self) -> TorchType: return torch.ListType(torch.IntType.get())

    def const_value(self) -> Any:
        return list([d.const_value() for d in self.dims])

    def is_sequence(self) -> bool:
        return True

    def sequence_length(self) -> 'VariableInfo': return self.seq_length

    def sequence_length_is_const(self) -> bool:
        return True

    def sequence_const_length(self) -> int:
        return len(self.dims)

    def sequence_element_at_index(self, i: int) -> VariableInfo:
        return self.dims[i]

    def duplicate(self) -> VariableInfo: return TensorShapeVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,dims=list([d.duplicate() for d in self.dims]))

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':

        length_counts: Dict[int, int] = {}

        for v in samples:
            l = len(v)
            if l in length_counts:
                length_counts[l] += 1
            else:
                length_counts[l] = 1

        if len(length_counts) > 1:
            raise NotImplementedError("multiple lengths")

        l = len(samples[0])

        subsamples = [[] for _ in range(l)]
        stp = torch_type.getElementType()

        for s in samples:
            assert isinstance(s, list)
            for i,ls in enumerate(s):
                subsamples[i].append(ls)

        dims = [vars.sampled(name=name+".#dim"+str(i), torch_type=stp, samples=ss, produced_by=produced_by, produced=produced) for i,ss in enumerate(subsamples)]

        return TensorShapeVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced, dims=dims)

class ScalarVariable(VariableInfo):
    # Anything which is a scalar value with a known type

    type: Type

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 type: Type):
        assert not issubclass(type, torch.Tensor)
        super().__init__(name, owner, origin, produced_by,produced)
        self.type = type

    def __repr__(self) -> str: return f"{self.name}:  {self.type.__name__}"
    def print_value(self) -> str: return f"<{self.name}: {self.type.__name__}>"
    def is_const(self) -> bool: return False
    def const_type(self) -> Type: return self.type
    def torch_type(self) -> TorchType: return _scalar_torch_type(self.type)
    def is_tensor(self) -> bool: return False
    def duplicate(self) -> VariableInfo: return ScalarVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,type=self.type)

    def combine(self, other: 'VariableInfo') -> 'VariableInfo':
        type1 = self.const_type()
        type2 = other.const_type()
        if type1 != type2:
            raise NotImplementedError(f"combine ScalarVariables of differing types {type1} {type2}")
        return self.duplicate()

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':

        type_samples: Dict[type, List[Any]] = {}
        for v in samples:
            t = type(v)
            if t in type_samples:
                type_samples[t].append(v)
            else:
                type_samples[t] = [v]

        if len(type_samples) > 1:
            print("type_samples", type_samples)
            raise NotImplementedError("multiple types")

        for t,s in type_samples.items():
            if t == int:
                return IntVariable.sampled(name, torch_type, s, produced_by, produced, vars)
            raise NotImplementedError("scalar of type", t)

        raise NotImplementedError()

class IntVariable(ScalarVariable):

    min_value: Optional[int] = None
    max_value: Optional[int] = None

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 min_value: Optional[int] = None, max_value: Optional[int] = None):
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, type=int)
        self.min_value = min_value
        self.max_value = max_value

    def __repr__(self) -> str: return f"{self.name}:  {self.print_value()}"
    def print_value(self) -> str:
        result = "int"
        if self.min_value is not None or self.max_value is not None:
            result += "{"
            if self.min_value is not None:
                result += str(self.min_value)
            result += ":"
            if self.max_value is not None:
                result += str(self.max_value)
            result += "}"
        return result

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> VariableInfo:
        min_value = min(samples)
        max_value = max(samples)
        assert isinstance(torch_type, torch.IntType)
        return IntVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced, min_value=min_value, max_value=max_value)

class OptionalVariable(VariableInfo):
    # Variable or None

    some: VariableInfo

    def __init__(self, some: VariableInfo):
        super().__init__(some.name, some.owner, some.origin, some.produced_by, some.produced)
        self.some = some

    def __repr__(self) -> str: return f"{self.name}:  optional {self.print_value()}"
    def print_value(self) -> str: return self.some.print_value() + "?"
    def is_const(self) -> bool: return False
    def const_type(self) -> Type: return object
    def is_tensor(self) -> bool: return False
    def duplicate(self) -> VariableInfo: return OptionalVariable(some=self.some.duplicate())

    @staticmethod
    def sampled(name: str, tp: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> VariableInfo:

        val_counts: Dict[Any, int] = {}
        for v in samples:
            if v in val_counts:
                val_counts[v] += 1
            else:
                val_counts[v] = 1

        # Filter out None from the list
        no_nones = [s for s in samples if s is not None]

        if len(no_nones) == 0:
            return vars.constant(name=name, origin=Origin.SAMPLE, value=None, produced_by=produced_by, produced=produced)

        contained_type = tp.getElementType()
        contained = vars.sampled(name + ".#some", contained_type, no_nones, produced_by=produced_by, produced=produced)

        if len(no_nones) == len(samples):
            return contained

        result = OptionalVariable(contained)
        return result


def _print_var_fields(name: str, origin: Any, const: Any, produced: Any, first_consumed: Any, last_consumed: Any, produced_by: Any, tp: Any, const_value: Any) -> str:
    return f"{name:20} {origin:10} {const:6} {produced:5} {first_consumed:5} {last_consumed:5} {produced_by:15} {tp:12} {const_value}"

def _print_var_names() -> str:
    return _print_var_fields("name", "origin", "const", "prod", "first", "last", "node", "type", "value")

def _print_tensor_info(info: VariableInfo) -> str:
    """
    Return information of a tensor: dtype, shape, device
    """
    dt = info.tensor_const_dtype()
    if dt is None:
        value_str = "<dtype?>"
    else:
        value_str = _short_dtype(dt)

    value_str += _short_info_str(info.tensor_shape())

    dev = info.tensor_const_device()
    if dev is None:
        value_str += "<device?>"
    else:
        value_str += str(dev)
    return value_str

def _short_info_str(info: VariableInfo) -> str:
    if info.is_const():
        return _print_value(info.const_value())
    elif info.is_tensor():
        return _print_tensor_info(info)
    elif info.is_sequence():
        value_str = ""
        open = "[" if info.const_type() == list else '('
        close = "]" if info.const_type() == list else ')'

        if info.sequence_length_is_const():
            el_strs: List[str] = []
            l = info.sequence_const_length()
            for i in range(l):
                el_strs.append(_short_info_str(info.sequence_element_at_index(i)))
            value_str = open + ", ".join(el_strs) + close
        elif info.seq_length is not None:
            assert info.seq_el is not None
            if info.const_type() == list:
                value_str = _short_info_str(info.seq_el) + str(info.seq_length)
            else:
                value_str = "(" + ",".join([_short_info_str(info.seq_el)] * info.seq_length.min)
                if info.seq_length.max > info.seq_length.min:
                    value_str += "..." + ")" + str(info.seq_length)
                else:
                    value_str += ")"
        else:
            print("unhandled list/sequence: info", info)
            raise RuntimeError("TODO: print unhandled list/sequence case")
        return value_str
    else:
        assert info.const_type() is not None
        return "<" + info.name + ": " + info.const_type().__name__ + ">"

def _print_var(name: str, info: VariableInfo) -> str:
    produced_kind = ''
    if isinstance(info.produced_by, Node):
        produced_kind = info.produced_by.kind().replace("aten::", "").replace("prim::", "")
    elif isinstance(info.produced_by, Graph):
        produced_kind = '<graph>'
    else:
        produced_kind = str(info.produced_by)

    type_str = ''
    if info.const_type() is not None:
        type_str = info.const_type().__name__

    value_str: str = ""
    if info.is_const():
        value_str = _print_value(info.const_value())
    elif info.const_type() == torch.Tensor:
        value_str = _print_tensor_info(info)
    elif info.const_type() == list or info.const_type() == tuple:
        value_str = info.print_value()
        if False:
            open = "[" if info.const_type() == list else '('
            close = "]" if info.const_type() == list else ')'
            if info.seq_els is not None:
                el_strs: List[str] = []
                for el in info.seq_els:
                    el_strs.append(_short_info_str(el))
                value_str = open + ", ".join(el_strs) + close
            elif info.seq_length is not None:
                assert info.seq_el is not None
                if info.const_type() == list:
                    value_str = _short_info_str(info.seq_el) + str(info.seq_length)
                else:
                    value_str = "(" + ",".join([_short_info_str(info.seq_el)] * info.seq_length.min)
                    if info.seq_length.max > info.seq_length.min:
                        value_str += "..." + ")" + str(info.seq_length)
                    else:
                        value_str += ")"
            else:
                print("unhandled list/sequence: info", info)
                raise RuntimeError("TODO: print unhandled list/sequence case")

    else:
        pass

    return _print_var_fields(name, info.origin.name, info.is_const(), info.produced, info.first_consumed,
                             info.last_consumed, produced_kind, type_str, value_str)

@dataclass
class Variables:
    vars: OrderedDict[str, VariableInfo] = field(default_factory=OrderedDict)

    def __len__(self) -> int:
        return len(self.vars)

    def add(self, v: VariableInfo):
        assert v.produced_by is None or isinstance(v.produced_by, Node) or isinstance(v.produced_by, Graph)
        assert len(v.name) > 0
        assert v.name not in self.vars
        self.vars[v.name] = v

    def add_constant(self, n: str, v: Any, node: Node, i: int):
        assert isinstance(node, Node)
        info = VariableInfo(n, Origin.CONST_PROP, True, v, node, i)
        self.add(info)

    def get(self, n: str, i: int) -> VariableInfo:
        result = self.vars[n]
        if i < result.first_consumed:
            result.first_consumed = i
        if i > result.last_consumed:
            result.last_consumed = i
        return result

    def renamed(self, var: VariableInfo, new_name: str) -> VariableInfo:
        new_var = var.renamed(new_name)
        new_var.owner = self
        self.add(new_var)
        return new_var

    def unify(self, value: Any, *vars: VariableInfo):
        """
        Say that all of the given variables have the given value.
        """
        print("TODO: unify")

    def alias(self, *vars: VariableInfo):
        """
        Say that all of the given variables have the same value (each is an alias of the other one).
        """
        if len(vars) < 2:
            return

        known_values = []
        for v in vars:
            if v.is_const():
                val = v.const_value()
                if len(known_values) == 0:
                    known_values.append(val)
                elif known_values[0] == val:
                    pass # values should be the same if they are being unified
                else:
                    raise RuntimeError(f"Error: attempt to unify differing values: {val} and {known_values[0]}")

        if len(known_values) == 0:
            return

        for v in vars:
            if not v.is_const():
                print(f"TODO: record alias {v} = {known_values[0]}")
                # TODO Record the alias
                pass


    def new_frame(self) -> 'Variables':
        result = Variables()
        for s,i in self.vars.items():
            i2 = i.duplicate()
            i2.owner = self
            result.add(i2)
        return result

    def dump_vars(self, indent: str = '', start_at: int = 0):
        print(f"{indent} {fg.boldblack}{_print_var_names()}{reset}")
        for i,(name,info) in enumerate(self.vars.items()):
            if i < start_at:
                continue
            print(indent, _print_var(name, info))

    def argument(self, *, name: str, produced_by: Optional[Node|Graph], observed: 'ArgumentData', torch_type: TorchType) -> VariableInfo:
        """
        Return the variable info for an argument.  This will specialize based on the observed
        values.
        """

        summary = observed.summarize()

        if summary.is_const():
            val = summary.const_value()
            return ConstantVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1, value=val)

        wrap: Callable[[VariableInfo], VariableInfo] = lambda x: x

        is_optional = summary.is_optional()
        if is_optional:
            summary = summary.non_optional()
            wrap = lambda x: OptionalVariable(x)

        dtype: Optional[torch.dtype] = summary.get_dtype()
        device: Optional[torch.device] = summary.get_device()
        shape: Optional[TensorShapes] = summary.get_shape()
        tp: Optional[Type] = summary.get_type()
        ttp: Optional[TorchType] = summary.get_torch_type()

        tp,ttp,dtype,device,shape = _unify_types(summary, torch_type)

        def constant_or_scalar(name: str, value: Optional[Tp], type: Type[Tp]) -> VariableInfo:
            if value is not None:
                assert isinstance(value, type)
                return ConstantVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                           value=value)
            else:
                return ScalarVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                         type=type)

        assert tp is not None
        if issubclass(tp, torch.Tensor):
            dtypevi = constant_or_scalar(name+"#.dtype", dtype, torch.dtype)
            devicevi = constant_or_scalar(name+".#device", device, torch.device)
            assert shape is not None

            def shape_to_var(name: str, shape: TensorShape) -> List[VariableInfo]:

                def dim_to_var(name: str, dim: ShapeRange) -> VariableInfo:
                    if dim.is_const():
                        return ConstantVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1, value=dim.const_value())
                    else:
                        return ScalarVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                         type=int)

                dims: List[VariableInfo] = []

                for i,dim in enumerate(shape.dims):
                    dims.append(dim_to_var(name + f"[{i}]", dim))

                return dims


            

            dims = shape_to_var(name+".#shape", shape.unique_length())

            shapevi = TensorShapeVariable(name=name+".#shape", owner=self,origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                          dims=dims)


            if dtype is not None:
                dtypevi = ConstantVariable(name=name+".#dtype", owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                           value=dtype)
            else:
                dtypevi = ScalarVariable(name=name+".#dtype", owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                         type=torch.dtype)

            return wrap(TensorVariable(name=name, owner=self, origin=Origin.ARG,
                                  produced_by=produced_by, produced=-1, dtype=dtypevi, device=devicevi,shape=shapevi))
        elif issubclass(tp, tuple) or issubclass(tp, list):
            if isinstance(ttp, torch.TupleType):
                els = ttp.elements()
                nels = len(els)
                assert len(observed.tuple_args) == nels
                el_args: List[VariableInfo] = []

                for i,el in enumerate(els):
                    el_observed = observed.tuple_args[i]
                    el_arg = self.argument(name=name+".#el" + str(i), produced_by=produced_by, observed=el_observed, torch_type=el)
                    el_args.append(el_arg)

                nels = self.constant(name=name+".#length", origin=Origin.ARG, value=nels, produced_by=produced_by,produced=-1)

                return wrap(SequenceVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                            tp=tp, seq_length=nels, seq_els=el_args, seq_el=None))
            else:
                raise NotImplementedError("TODO: list args")

            return wrap(SequenceVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1))
            raise NotImplementedError("TODO: implement list or tuple arguments")
        else:
            raise NotImplementedError(f"TODO: implement other types of arguments {ttp} {tp}")

        result = ArgumentVariable(name=name, owner=self, origin=Origin.ARG,
                              produced_by=produced_by, produced=-1, arg=summary)

        return result

    def sampled(self, name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return a VariableInfo that has samples from the various sets of values provided.
        """
        if len(samples) == 0:
            raise RuntimeError("can't call sampled() with no samples")
        elif len(samples) == 1:
            raise RuntimeError("single sample doesn't allow us to determine constness")
        
        val_counts: Dict[Any, int] = {}
        for v in samples:
            if v in val_counts:
                val_counts[v] += 1
            else:
                val_counts[v] = 1

        if len(val_counts) == 1:
            # It's a constant
            return self.constant(name=name, origin=Origin.SAMPLE, value=list(val_counts.keys())[0], produced_by=produced_by, produced=produced)

        if isinstance(torch_type, torch.TensorType):
            return TensorVariable.sampled(name, torch_type, samples, produced_by, produced, self)
        elif isinstance(torch_type, torch.OptionalType):
            return OptionalVariable.sampled(name, torch_type, samples, produced_by, produced, self)
        elif isinstance(torch_type, torch.TupleType):
            return SequenceVariable.sampled(name, torch_type, samples, produced_by, produced, self)
            contained = torch_type.containedTypes()
        elif isinstance(torch_type, torch.IntType):
            return ScalarVariable.sampled(name, torch_type, samples, produced_by, produced, self)
            print("scalar type", torch_type.scalarType())
            raise NotImplementedError()
        else:
            raise RuntimeError(f"Unknown torch_type {torch_type}")

        raise NotImplementedError()

    def local(self, *, name: str, origin: Origin, tp: Type, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a local variable.  This should not be used for
        tensors.
        """
        assert not issubclass(tp, torch.Tensor), "Tensors should use the tensor method, not local"

        return ScalarVariable(name=name, owner=self, type=tp, origin=origin,
                            produced_by=produced_by, produced=produced)

    def tensor(self, *, name: str, origin: Origin,
               dtype: VariableInfo, device: VariableInfo, shape: VariableInfo,
               produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a tensor valued variable.
        """
        return TensorVariable(name=name, owner=self, origin=origin,
                            dtype=dtype, device=device, shape=shape,
                            produced_by=produced_by, produced=produced)

    def tensor_shape(self, *, name: str, origin: Origin, dims: List[VariableInfo], produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a tensor shape variable.
        """
        all_const = all(map(lambda x: x.is_const(), dims))
        if all_const:
            for x in dims:
                #print("dims", dims)                
                assert issubclass(x.const_type(), int)
            const_value = list([x.const_value() for x in dims])
            return self.constant(name=name, origin=origin, produced_by=produced_by, produced=produced,value=const_value)
        else:
            return TensorShapeVariable(owner=self, name=name, origin=origin, produced_by=produced_by, produced=produced,dims=dims)

    def any(self, *, name: str, origin: Origin, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for something that could be any type (nothing static is known
        about it).
        """
        return VariableInfo(name=name, origin=origin, is_const=False,
                            produced_by=produced_by, produced=produced)

    def homogeneous_sequence(self, *, name: str, origin: Origin, tp: Type[Sequence], produced_by: Optional[Node|Graph], produced: int,
                             length: VariableInfo, values: VariableInfo) -> VariableInfo:
        """
        Create the VariableInfo for a homogeneous sequence (tuple or list) with a fixed or
        variable length content of an instance of a single type.
        """
        els=[SequenceChunk(name=name+".#el", owner=self, origin=origin, produced_by=produced_by, produced=produced, length=length, el=values)]
        return SequenceVariable(name=name, owner=self, origin=origin, tp=tp,
                            seq_els=els, produced_by=produced_by, produced=produced)

    def inhomogeneous_sequence(self, *, name: str, origin: Origin, tp: Type[Sequence], produced_by: Optional[Node|Graph], produced: int,
                               values: List[VariableInfo]) -> VariableInfo:
        """
        Create the VariableInfo for an inhomogeneous sequence (tuple or list) with a fixed
        length and each element having a different type.
        """
        #print("inhomogeneous sequence")
        #for i,v in enumerate(values):
        #    print(_print_var(str(i), v))
        return SequenceVariable(name=name, owner=self, origin=origin, tp=tp, seq_els=values,
                            produced_by=produced_by, produced=produced)

    def constant(self, *, name: str, origin: Origin, value: Any, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a constant.  This will fill in all of the ancilliary
        information.
        """

        return ConstantVariable(name=name, owner=self, origin=origin, value=value,
                            produced_by=produced_by, produced=produced)
