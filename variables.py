from enum import Enum
from typing import Type, Tuple, Any, Dict, List, Optional, OrderedDict, Union, TypeVar, Iterator, Iterable, Sequence, SupportsInt, overload, get_origin, get_args, Callable, TypeAlias, Set
from dataclasses import dataclass, field
import torch
from torch._C import Node, Graph
from utils import typeify, _print_value, _short_dtype, first
from ansi.color import bg, fg
from ansi.color.fx import reset  # pyright: ignore
import copy
import inspect
from runtimes import print_elapsed
from torch.nn import Module
from tensor_comparisons import TensorDifference
import operator

TorchType: TypeAlias = 'torch._C.JitType' # pyright: ignore

def _is_optional(field):
    """
    Returns whether the annotation is generated by typing.Optional[...]
    """
    return get_origin(field) is Union and \
           type(None) in get_args(field)

def _scalar_torch_type(tp: Type) -> TorchType:
    """
    Given the given scalar type, return the corresponding Torch type.
    """
    NoneType = type(None)
    if issubclass(tp, int):
        return torch.IntType.get()
    elif issubclass(tp, float):
        return torch.FloatType.get()
    elif issubclass(tp, bool):
        return torch.BoolType.get()
    elif tp == NoneType:
        return torch.NoneType.get()
    elif issubclass(tp, str):
        return torch.StringType.get()
    elif issubclass(tp, torch.device):
        return torch.DeviceObjType.get()
    elif issubclass(tp, torch.dtype):
        return torch.IntType.get()
    raise NotImplementedError(f"_scalar_torch_type for {tp}")

def _to_torch_type(ann: Any, samples: List[Any]) -> TorchType:
    """
    Converts the given parameter into its corresponding Torch JIT type
    """

    #print(p)

    origin = get_origin(ann)
    args = get_args(ann)

    if origin is None:
        if ann is None or ann is inspect._empty:
            # Do it based on the type seen
            if len(samples) == 0:
                # NO samples and no annotation
                return torch.AnyType()
            else:
                def get_type(sample) -> TorchType:
                    if isinstance(sample, torch.Tensor):
                        return torch.TensorType.create_from_tensor(sample)
                    elif sample is None:
                        return torch.NoneType.get()
                    else:
                        return _scalar_torch_type(type(sample))

                    raise RuntimeError(f"TODO: get_type {type(sample)}")
                
                current: Optional[TorchType] = None
                for t in [get_type(sample) for sample in samples]:
                    if current is None:
                        current = t
                    else:
                        if str(current) != str(t):
                            raise RuntimeError(f"wrong type {current} {t}")

                assert current is not None
                return current

            raise RuntimeError(f"empty {_print_value(samples)}")
        elif issubclass(ann, torch.Tensor):
            return torch.TensorType.create_from_tensor(ann())  
        elif issubclass(ann, torch.FloatTensor):
            return torch.TensorType.create_from_tensor(ann())
        elif ann is bool:
            return torch.BoolType.get()
        else:
            raise RuntimeError(f"TODO: no origin for {ann}")
    elif _is_optional(ann):
        contained,_ = args
        sub_samples = [s for s in samples if s is not None]
        #print("contained", contained)
        return torch.OptionalType(_to_torch_type(contained, sub_samples))
    elif origin == tuple:
        def get_el_type(i: int) -> TorchType:
            sub_samples = [s[i] for s in samples]
            return _to_torch_type(args[i], sub_samples)

        return torch.TupleType([get_el_type(i) for i in range(len(args))])

        raise RuntimeError(f"TODO: handle optional {contained}")


    print(ann, type(ann), get_origin(ann), get_args(ann))

    raise RuntimeError(f"_to_torch_type for {ann} {get_origin(ann)} {get_args(ann)}")

def _common_torch_type(*types: TorchType) -> TorchType:
    raise NotImplementedError()

class Origin(Enum):
    UNKNOWN = 0
    SELF = 1
    ARG = 2
    DEFAULT_ARG = 3
    LOCAL = 4
    CONST_PROP = 5
    SAMPLE = 6
    EXPRESSION = 7

Tp = TypeVar('Tp')

class VariableInfo:
    """
    Holds the static information known about a variable.
    """
    name: str = ""
    owner: 'Variables'
    origin: Origin = Origin.UNKNOWN

    def _as_type(self, t: Type[Tp]) -> Tp:
        if not isinstance(self, t):
            raise RuntimeError(f"Attempt to convert to {t.__name__} {type(self).__name__} {self}")
        return self

    def print_value(self) -> str:
        raise RuntimeError(f"Must override print_value() for class {type(self).__name__}")

    def const_type(self) -> type:
        raise RuntimeError(f"Must override const_type() for class {type(self).__name__}")

    def torch_type(self) -> TorchType:
        raise RuntimeError(f"Must override torch_type() for class {type(self).__name__}")

    def is_const(self) -> bool:
        raise RuntimeError(f"Must override is_const() for class {type(self).__name__}")

    def const_value(self) -> Any:
        if self.is_const():
            raise RuntimeError(f"Must override const_value() if is_const() can be true for type {type(self)}")
        else:
            raise RuntimeError("Attempt to access const value for non-const variable")

    def is_none(self) -> bool:
        """
        Return true if this is a constant None value
        """
        return self.const_type() == type(None)

    def is_optional(self) -> bool:
        return False

    def non_optional(self) -> 'VariableInfo':
        raise RuntimeError(f"Must override non_optional() type {type(self)}")

    def is_tensor(self) -> bool:
        return False

    def is_sequence(self) -> bool:
        return False

    def is_sequence_chunk(self) -> bool:
        return False

    def sequence_length_is_const(self) -> bool:
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_const_length(self) -> int:
        if self.sequence_length_is_const():
            raise RuntimeError(f"Must override if sequence_length_is_const() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_length(self) -> 'VariableInfo':
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_element_at_index(self, i: int) -> 'VariableInfo':
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_chunks(self) -> Iterator['VariableInfo']:
        """
        Returns a list of either elements or sequence chunks, or order, that together makes up the sequence's
        known and unknown elements.  This will always succeed on all sequences, not matter whether the length
        is known or not.
        """
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_chunk_schema(self) -> 'VariableInfo':
        """
        For sequence chunks (which may be of non-unitary length), returns the schema of the elements.
        """
        if self.is_sequence_chunk():
            raise RuntimeError(f"Must override if is_sequence_chunk() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence-chunk type {type(self).__name__} {self}")

    def element_type_is_const(self) -> bool:
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")
        
    def element_const_type(self) -> type:
        if self.element_type_is_const():
            raise RuntimeError(f"Must override if element_const_type() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def tensor_dtype_is_const(self) -> bool:
        return self.tensor_const_dtype() is not None

    def tensor_const_dtype(self) -> Optional[torch.dtype]:
        if self.is_tensor():
            raise RuntimeError(f"Must override tensor_const_dtype() if is_tensor() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call tensor_const_dtype() for non-tensor type {type(self).__name__} {self}")

    def tensor_dtype(self) -> 'VariableInfo':
        """
        Returns the variable containing the dtype of this tensor.
        """
        raise RuntimeError(f"Attempt to call tensor_dtype() for non-tensor type {type(self).__name__} {self}")

    def tensor_device_is_const(self) -> bool:
        return self.tensor_const_device() is not None

    def tensor_const_device(self) -> Optional[torch.device]:
        if self.is_tensor():
            raise RuntimeError(f"Must override const_device() if is_tensor() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call const_device() for non-tensor type {type(self).__name__} {self}")

    def tensor_device(self) -> 'VariableInfo':
        """
        Returns the variable containing the dtype of this tensor.
        """
        raise RuntimeError(f"Attempt to call const_device() for non-tensor type {type(self).__name__} {self}")

    def tensor_shape(self) -> 'TensorShapeVariable':
        """
        Returns the variable containing the shape of this tensor.
        """
        if self.is_tensor():
            raise RuntimeError(f"Must override tensor_shape() if is_tensor() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call tensor_shape() for non-tensor type {type(self).__name__} {self}")

    def is_scalar(self) -> bool:
        return False
    
    def as_sequence(self) -> 'SequenceVariable': return self._as_type(SequenceVariable)
    def as_tensor(self) -> 'TensorVariable': return self._as_type(TensorVariable)
    def as_scalar(self) -> 'ScalarVariable': return self._as_type(ScalarVariable)
    def as_int(self) -> 'IntVariable': return self._as_type(IntVariable)
    def as_float(self) -> 'FloatVariable': return self._as_type(FloatVariable)
    def as_device(self) -> 'DeviceVariable': return self._as_type(DeviceVariable)
    def as_data_type(self) -> 'DataTypeVariable': return self._as_type(DataTypeVariable)
    def as_shape(self) -> 'TensorShapeVariable': return self._as_type(TensorShapeVariable)


    def observe(self, observed: List[Any]) -> 'VariableInfo':
        """
        Provide feedback that the given value was observed during execution, for
        profile-guided optimization.  This may mutate it in place.
        """
        raise RuntimeError(f"Must override observe() for class {type(self).__name__}")

    @staticmethod
    def covering(vars: 'Variables', name: str, produced_by: Optional[Node|Graph], produced: int, infos: Sequence['VariableInfo']) -> 'VariableInfo':
        raise RuntimeError(f"Need to implement covering for types {infos}")

    # Which node produced, and which nodes read, this value.  The integers are the sequence
    # of nodes in the graph.
    produced_by: Optional[Node|Graph] = None
    produced: int = 0
    first_consumed: int = 10000
    last_consumed: int = 0

    def __init__(self, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        self.name = name
        self.owner = owner
        self.origin = origin
        self.produced_by = produced_by
        self.produced = produced

    def typed_const_value(self, tp: Type[Tp]) -> Optional[Tp]:
        """
        If this variable is a constant, check that its and instance of the given type and return it.
        If it's not a constant, return None.
        Used to extract values of constant arguments from VariableInfo during constant propagation.
        """
        if not self.is_const():
            return None
        if not isinstance(self.const_value(), tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value())} of value {self.const_value()}")
        return self.const_value()

    def typed_const_nonnull_value(self, tp: Type[Tp]) -> Tp:
        """
        Assert that this variable is a constant, of the given type, and return that value.
        Used to extract values of constant arguments from VariableInfo during constant propagation.
        """
        if not self.is_const():
            raise RuntimeError(f"Expected constant value but got {self}")
        if not isinstance(self.const_value(), tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value())} of value {self.const_value()}")
        return self.const_value()

    def typed_default_value(self, default: Tp) -> Optional[Tp]:
        """
        If this variable is a constant and not None, check that its and instance of the given type and return it.
        If it's a constant and None, then return the default.
        If it's not a constant, return None.
        Used to extract values of defaulted constant arguments from VariableInfo during constant propagation.
        """
        tp = type(default)
        if not self.is_const():
            return None
        if self.const_value() is None:
            return default
        if not isinstance(self.const_value(), tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value())} of value {self.const_value()}")
        return self.const_value()

    def duplicate(self) -> 'VariableInfo':
        """
        Duplicate operator.  Needed to create another version because deepcopy(x) doesn't work and some objects are
        mutable.
        """
        raise RuntimeError(f"TODO: duplicate() for {type(self)}")

    def renamed(self, new_name: str) -> 'VariableInfo':
        res = self.duplicate()
        res.name = new_name
        return res

    def __add__(self, other) -> 'VariableInfo':
        return self.owner.expression('aten::add',)
        raise NotImplementedError()

class TorchOperator:

    def is_block(self) -> bool: ...

    # Is this a constant operation?  In other words, will it always return the same value
    # given the same inputs, and doesn't have any side-effects?
    def is_const(self) -> bool: ...

    # Perform constant propagation for the operation
    def const_prop(self, produced: int, inputs: List[VariableInfo], vars: 'Variables') -> Tuple[VariableInfo] | VariableInfo: ...

_torch_operators: Dict[Tuple[str,int], Callable[[Node],TorchOperator]] = {}

def get_torch_operator(node: Node) -> TorchOperator:
    arity = node.inputsSize()
    #print(f"getting operator {node.kind()} with arity {arity}")
    key = (node.kind(), int(arity))
    if key in _torch_operators:
        return _torch_operators[key](node)
    key = (node.kind(), -1)
    return _torch_operators[key](node)

def torch_operator(kind: str, arity: int = -1):
    def do_operator(klass):
        key = (kind,arity)
        assert key not in _torch_operators
        _torch_operators[key] = klass
        return klass
    return do_operator


class ___ConstantVariable(VariableInfo):
    # Anything which is constant

    value: Any

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: Any):
        super().__init__(name, owner, origin, produced_by, produced)
        self.value = value

    def __repr__(self) -> str:
        return f"{self.name}: const {type(self.value).__name__} = {self.print_value()}"

    # Iterator class that wraps values in a VariableInfo
    class MyIterator:
        it: Iterator
        owner: VariableInfo
        i: int
        def __init__(self, it: Iterator, owner: VariableInfo):
            self.it = it
            self.owner = owner
            self.i = 0
        def __next__(self):
            val = next(self.it)
            result = self.owner.owner.constant(name=self.owner.name+"#el"+str(self.i),origin=self.owner.origin,value=val,produced_by=self.owner.produced_by,produced=self.owner.produced)
            self.i += 1
            return result

    def __iter__(self) -> MyIterator:
        return ConstantVariable.MyIterator(iter(self.value),self)

    def __len__(self) -> int:
        return len(self.value)

    def __getitem__(self, item) -> VariableInfo:
        return self.owner.constant(name=self.name+"#el"+str(item),origin=self.origin,value=self.value[item],produced_by=self.produced_by,produced=self.produced)

    def print_value(self) -> str:
        return _print_value(self.value)

    def is_const(self) -> bool:
        return True

    def const_type(self) -> type:
        return type(self.value)

    def torch_type(self) -> TorchType:
        return _to_torch_type(None, [self.value])

    def const_value(self) -> Any:
        return self.value

    def is_sequence(self) -> bool:
        return isinstance(self.value, list) or isinstance(self.value, tuple)

    def sequence_length_is_const(self) -> bool:
        return True

    def is_tensor(self) -> bool:
        return isinstance(self.value, torch.Tensor)

    def tensor_dtype(self) -> VariableInfo:
        if isinstance(self.value, torch.Tensor):
            return self.owner.constant(name=self.name + ".#dtype", value=self.value.dtype, origin=self.origin, produced=self.produced, produced_by=self.produced_by)
        raise RuntimeError(f"Attempt to get tensor_dtype from non-tensor constant value {self}")

    def tensor_const_dtype(self) -> Optional[torch.dtype]:
        if isinstance(self.value, torch.Tensor):
            return self.value.dtype
        raise RuntimeError(f"cannot get tensor dtype for non-tensor constant value {self}")

    def tensor_device(self) -> VariableInfo:
        if isinstance(self.value, torch.Tensor):
            return self.owner.constant(name=self.name + ".#device", value=self.value.device, origin=self.origin, produced=self.produced, produced_by=self.produced_by)
        raise RuntimeError(f"Attempt to get tensor_device from non-tensor constant value {self}")

    def tensor_const_device(self) -> Optional[torch.device]:
        if isinstance(self.value, torch.Tensor):
            return self.value.device
        raise RuntimeError("cannot get tensor device for non-tensor constant value")

    def tensor_shape(self) -> VariableInfo:
        if isinstance(self.value, torch.Tensor):
            return self.owner.constant(name=self.name+".#shape", origin=self.origin, value=list(self.value.shape),
                                       produced_by=self.produced_by,produced=self.produced)
            return self.value.device
        raise RuntimeError("cannot get tensor device for non-tensor constant value")

    def duplicate(self) -> VariableInfo: return ConstantVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)

    def observe(self, observed: List[Any]) -> VariableInfo:
        for o in observed:
            if id(o) == id(self.value):
                continue
            if isinstance(o, torch.Tensor):
                diff = TensorDifference.between(o, self.value)
                if diff:
                    raise RuntimeError(f"observed tensor differs from const prop: ulps {diff.ulps}: {_print_value(o)} vs {_print_value(self.value)}")
            else:
                if o != self.value:
                    raise RuntimeError(f"observed constant {o} differs from const prop {self.value}")
        return self

    def __add__(self, other) -> 'VariableInfo':
        res = self.value + other
        if isinstance(res, VariableInfo):
            return res
        return ConstantVariable(name=f"#add({self.name}{other.name})", owner=self.owner, origin=Origin.EXPRESSION, produced_by=None,produced=-1,value=res)

    def __radd__(self, other) -> 'VariableInfo':
        res = other + self.value
        if isinstance(res, VariableInfo):
            return res
        return ConstantVariable(name=f"#add({self.name}{other.name})", owner=self.owner, origin=Origin.EXPRESSION, produced_by=None,produced=-1,value=res)

class NoneVariable(VariableInfo):
    """
    A variable of type None.  Since the value is singular, it's both const and acts without a value.
    """
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        super().__init__(name, owner, origin, produced_by,produced)

    def __repr__(self) -> str: return f"{self.name}:  None"
    def print_value(self) -> str: return f"None"
    def is_const(self) -> bool: return True
    def const_value(self) -> Any: return None
    def const_type(self) -> Type: return type(None)
    def torch_type(self) -> TorchType: return torch.NoneType.get()
    def duplicate(self) -> 'VariableInfo': return NoneVariable(name=self.name, owner=self.owner, origin=self.origin, produced_by=self.produced_by, produced=self.produced)
    def observe(self, observed: List[Any]) -> 'NoneVariable':
        for o in observed:
            if o is not None:
                raise RuntimeError(f"Non-None value {o} observed in None context")
        return self

class ScalarVariable(VariableInfo):
    # Anything which is a scalar value with a known type

    type: Type

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 type: Type):
        assert not issubclass(type, torch.Tensor)
        super().__init__(name, owner, origin, produced_by,produced)
        self.type = type

    def __repr__(self) -> str: return f"{self.name}:  {self.print_value()}"
    def print_value(self) -> str:
        result = f"<{self.type.__name__}"
        if self.is_const():
            result += "=" + str(self.const_value())
        result += ">"
        return result
    def is_const(self) -> bool: return False
    def const_type(self) -> Type: return self.type
    def torch_type(self) -> TorchType: return _scalar_torch_type(self.type)
    def is_tensor(self) -> bool: return False

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':

        type_samples: Dict[type, List[Any]] = {}
        for v in samples:
            t = type(v)
            if t in type_samples:
                type_samples[t].append(v)
            else:
                type_samples[t] = [v]

        if len(type_samples) > 1:
            print("type_samples", type_samples)
            raise NotImplementedError("multiple types")

        for t,s in type_samples.items():
            if t == int:
                return IntVariable.sampled(name, torch_type, s, produced_by, produced, vars)
            elif t == torch.dtype:
                return DataTypeVariable.sampled(name, torch_type, s, produced_by, produced, vars)
            elif t == torch.device:
                return DeviceVariable.sampled(name, torch_type, s, produced_by, produced, vars)
            raise NotImplementedError("scalar of type", t)

        raise NotImplementedError()

    def observe(self, observed: List[Any]) -> 'ScalarVariable':
        for o in observed:
            assert isinstance(o, self.type)
        return self

class IntVariable(ScalarVariable):

    min_value: Optional[int] = None
    max_value: Optional[int] = None

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 min_value: Optional[int] = None, max_value: Optional[int] = None):
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, type=int)
        self.min_value = min_value
        self.max_value = max_value

    def __repr__(self) -> str: return f"{self.name}:  {self.print_value()}"
    def print_value(self) -> str:
        result = "int"
        if self.min_value is not None or self.max_value is not None:
            if self.min_value == self.max_value:
                result += "=" + str(self.min_value)
            else:
                result += "{"
                if self.min_value is not None:
                    result += str(self.min_value)
                result += ":"
                if self.max_value is not None:
                    result += str(self.max_value)
                result += "}"
        return result

    def const_value(self) -> int: return super().const_value()

    def duplicate(self) -> VariableInfo: return IntVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,min_value=self.min_value,max_value=self.max_value)

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> VariableInfo:
        min_value = min(samples)
        max_value = max(samples)
        assert isinstance(torch_type, torch.IntType)
        return IntVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced, min_value=min_value, max_value=max_value)

    def observe(self, observed: List[Any]) -> 'IntVariable':
        for o in observed:
            if not isinstance(o, int):
                raise RuntimeError(f"observed value {o} of type {type(o).__name__} when looking for int {self}")
            assert isinstance(o, int)
            if self.min_value is not None and o < self.min_value:
                raise RuntimeError(f"observed incompatible value {o} for {self}")
            if self.max_value is not None and o > self.max_value:
                raise RuntimeError(f"observed incompatible value {o} for {self}")
        # TODO: record observation?
        return self

    @staticmethod
    def covering(vars: 'Variables', name: str, produced_by: Optional[Node|Graph], produced: int, infos: Sequence['VariableInfo']) -> 'IntVariable':
        assert isinstance(infos[0], IntVariable)
        min_value: Optional[int] = infos[0].min_value
        max_value: Optional[int] = infos[0].max_value
        for i in infos:
            assert isinstance(i, IntVariable)
            min_value = None if i.min_value is None or min_value is None else min(i.min_value, min_value)
            max_value = None if i.max_value is None or max_value is None else max(i.max_value, max_value)
            print("covering int variable", i, "min_value", min_value, "max_value", max_value)

        return IntVariable(name=name, owner=vars, origin=Origin.CONST_PROP, produced_by=produced_by, produced=produced, min_value=min_value, max_value=max_value)

    def __add__(self, other) -> 'VariableInfo':
        return self.owner.expression(op="aten::add", args=[self, other], name=self.name + "#__add__", origin=self.origin, produced_by=None, produced=-1)

    def __radd__(self, other) -> 'VariableInfo':
        res = other + self.value
        if isinstance(res, VariableInfo):
            return res
        return ConstantVariable(name=f"#add({self.name}{other.name})", owner=self.owner, origin=Origin.EXPRESSION, produced_by=None,produced=-1,value=res)

def _observe_constant(self: Tp, observed: List[Any]) -> Tp:
    for o in observed:
        if o != self.value: # pyright: ignore
            raise RuntimeError("observed value {o} incompatible with constant {self}")
    return self

class ConstantInt(IntVariable):
    value: int
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: int):
        self.value = value
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, min_value=value, max_value=value)

    def is_const(self) -> bool: return True
    def const_value(self) -> int: return self.value
    def duplicate(self) -> VariableInfo: return ConstantInt(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)
    def observe(self, observed: List[Any]) -> 'ConstantInt': return _observe_constant(self, observed)

class FloatVariable(ScalarVariable):
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, type=float)

    def duplicate(self) -> 'FloatVariable': return copy.copy(self)
    def observe(self, observed: List[Any]) -> 'FloatVariable':
        result = super().observe(observed)
        assert isinstance(result, FloatVariable)
        return result

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'FloatVariable':
        assert isinstance(torch_type, torch.FloatType)
        return FloatVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced)

class ConstantFloat(FloatVariable):
    value: float
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: float):
        self.value = value
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced)
    def is_const(self) -> bool: return True
    def const_value(self) -> float: return self.value
    def duplicate(self) -> 'ConstantFloat': return copy.copy(self)
    def observe(self, observed: List[Any]) -> 'ConstantFloat': return _observe_constant(self, observed)

class StringVariable(ScalarVariable):
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, type=str)
    def duplicate(self) -> 'StringVariable': return copy.copy(self)
    def observe(self, observed: List[Any]) -> 'StringVariable':
        result = super().observe(observed)
        assert isinstance(result, StringVariable)
        return result

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'StringVariable':
        assert isinstance(torch_type, torch.StringType)
        return StringVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced)

class ConstantString(StringVariable):
    value: str
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: str):
        self.value = value
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced)
    def is_const(self) -> bool: return True
    def const_value(self) -> str: return self.value
    def duplicate(self) -> 'ConstantString': return copy.copy(self)
    def observe(self, observed: List[Any]) -> 'ConstantString': return _observe_constant(self, observed)

class BooleanVariable(ScalarVariable):
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, type=bool)

    def duplicate(self) -> 'BooleanVariable': return copy.copy(self)
    def observe(self, observed: List[Any]) -> 'BooleanVariable':
        result = super().observe(observed)
        assert isinstance(result, BooleanVariable)
        return result

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'BooleanVariable':
        assert isinstance(torch_type, torch.FloatType)
        return BooleanVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced)

class DataTypeVariable(ScalarVariable):
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, type=torch.dtype)

    def duplicate(self) -> 'DataTypeVariable': return copy.copy(self)
    def observe(self, observed: List[Any]) -> 'DataTypeVariable':
        result = super().observe(observed)
        assert isinstance(result, DataTypeVariable)
        return result

class ConstantDataType(DataTypeVariable):
    value: torch.dtype

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: torch.dtype):
        self.value = value
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced)

    def is_const(self) -> bool: return True
    def const_value(self) -> torch.dtype: return self.value
    def duplicate(self) -> VariableInfo: return ConstantDataType(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)
    def observe(self, observed: List[Any]) -> 'ConstantDataType': return _observe_constant(self, observed)

class DeviceVariable(ScalarVariable):
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, type=torch.device)

    def duplicate(self) -> VariableInfo: return copy.copy(self)
    def observe(self, observed: List[Any]) -> 'DeviceVariable':
        result = super().observe(observed)
        assert isinstance(result, DeviceVariable)
        return result

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'DeviceVariable':
        assert isinstance(torch_type, torch.DeviceObjType)
        return DeviceVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced)

class ConstantDevice(DeviceVariable):
    value: torch.device

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: torch.device):
        self.value = value
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced)

    def is_const(self) -> bool: return True
    def const_value(self) -> torch.device: return self.value
    def duplicate(self) -> VariableInfo: return type(self)(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)
    def observe(self, observed: List[Any]) -> 'ConstantDevice': return _observe_constant(self, observed)

class ConstantValue(ScalarVariable):
    """
    Used for any value which doesn't match one of the other types
    """
    value: Any

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: torch.device):
        self.value = value
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, type=type(value))

    def is_const(self) -> bool: return True
    def const_value(self) -> Any: return self.value
    def duplicate(self) -> VariableInfo: return type(self)(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)

    def observe(self, observed: List[Any]) -> 'ConstantValue':
        for o in observed:
            if id(o) == id(self.value):
                continue
            if o != self.value:
                raise RuntimeError("Specialized type {self} observed non-compatible value {o}")

        return self

class SequenceChunk(VariableInfo):
    # This is a variable length chunk of unknown elements, each of which has a schema
    # that represents its value.

    el: VariableInfo
    length: VariableInfo

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 el: VariableInfo, length: VariableInfo):
        super().__init__(name, owner, origin, produced_by,produced)
        self.el = el
        self.length = length

    def print_value(self) -> str:
        result = "..." + self.el.print_value() + "[" + self.length.print_value() + "]..."
        return result

    def is_sequence_chunk(self) -> bool: return True


class SequenceVariable(VariableInfo):

    # The following are for sequences (lists and tuples).  They are modeled as a series of VariableInfo values, which
    # each represent either a list item or a variable length chunk of elements in a list.
    tp: Type # normally tuple or list
    seq_els: List[VariableInfo]

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 tp: Type, seq_els: List[VariableInfo]):
        super().__init__(name, owner, origin, produced_by,produced)
        self.tp = tp
        self.seq_els = seq_els

    def __repr__(self) -> str: return f"{self.name}:  {self.print_value()}"

    def print_value(self) -> str:
        brackets = "<>"
        if issubclass(self.tp, tuple):
            brackets = "()"
        elif issubclass(self.tp, list):
            brackets = "[]"
        else:
            raise RuntimeError("unknown sequence type")
        result = brackets[0] + ','.join([e.print_value() for e in self.seq_els]) + brackets[1]
        return result

    def is_const(self) -> bool: return False
    def is_tensor(self) -> bool: return False
    def is_sequence(self) -> bool: return True
    def const_type(self) -> type: return self.tp
    def torch_type(self) -> TorchType:
        torch_types = [el.torch_type() for el in self.seq_els]
        if issubclass(self.tp, list):
            common: TorchType = _common_torch_type(*torch_types)
            return torch.ListType(common)
        elif issubclass(self.tp, tuple):
            return torch.TupleType(torch_types) # pyright: ignore
        else:
            raise RuntimeError("Unknown sequence torch type")

    def sequence_length(self) -> 'VariableInfo': raise NotImplementedError()
    def sequence_length_is_const(self) -> bool:
        for el in self.seq_els:
            if el.is_sequence_chunk() and not el.sequence_length_is_const():
                return False
        return True

    def sequence_const_length(self) -> int:
        res = 0
        for el in self.seq_els:
            if el.is_sequence_chunk():
                res += el.sequence_const_length()
            else:
                res += 1
        return res

    def sequence_element_at_index(self, i: int) -> VariableInfo:
        assert self.seq_els is not None
        return self.seq_els[i]

    def __iter__(self) -> Iterator[VariableInfo]:
        assert self.sequence_length_is_const()
        return iter(self.seq_els)

    def __len__(self) -> int:
        return self.sequence_const_length()

    @overload
    def __getitem__(self, item: int) -> VariableInfo: ...

    @overload
    def __getitem__(self, item: slice) -> Sequence[VariableInfo]: ...

    def __getitem__(self, item) -> VariableInfo|Sequence[VariableInfo]:
        assert self.seq_els is not None
        return self.seq_els[item]

    def sequence_chunks(self) -> Iterator[VariableInfo]:
        assert self.seq_els is not None
        return iter(self.seq_els)

    def duplicate(self) -> VariableInfo:
        new_seq_els = list([e.duplicate() for e in self.seq_els])
        return SequenceVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,
                                tp=self.tp,seq_els=new_seq_els)

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':
        print("torch_type", torch_type)

        assert isinstance(torch_type, torch.TupleType)
        els = torch_type.elements()

        length_counts: Dict[int, int] = {}
        length_samples: List[int] = []
        too_long = 0
        too_short = 0

        for v in samples:
            l = len(v)
            length_samples.append(l)
            if l < len(els):
                too_short += 1
            if l > len(els):
                too_long += 1
            if l in length_counts:
                length_counts[l] += 1
            else:
                length_counts[l] = 1

        print("length_counts", length_counts)

        if len(els) == 1 and too_long > 0:
            # It was written as tuple[x] but actually it means list[x] as a tuple
            all_samples = []
            for v in samples:
                all_samples.extend(v)

            el_info = vars.sampled(name=name+".#els", torch_type=els[0], samples=all_samples, produced_by=produced_by, produced=produced)
            len_info = IntVariable.sampled(name=name+".#length", torch_type=torch.IntType.get(), samples=length_samples, produced_by=produced_by, produced=produced, vars=vars)
            return vars.homogeneous_sequence(name=name, origin=Origin.SAMPLE, tp=tuple, produced_by=produced_by, produced=produced, length=len_info, values=el_info)

        if len(length_counts) > 1:
            raise NotImplementedError("multiple lengths")

        l = len(samples[0])
        el_samples: List[List[Any]] = [[] for _ in range(l)]
        for v in samples:
            for s,el in zip(el_samples,v):
                s.append(el)

        el_info = [vars.sampled(name=name+".#el"+str(i),torch_type=ttp, samples=s, produced_by=produced_by, produced=produced) for i,(ttp,s) in enumerate(zip(els, el_samples))]

        return vars.inhomogeneous_sequence(name=name, origin=Origin.SAMPLE, tp=tuple, produced_by=produced_by, produced=produced, values=el_info)

    def observe(self, observed: List[Any]) -> VariableInfo:
        if self.sequence_length_is_const():
            l = self.sequence_const_length()
            obs = [[] for _ in range(l)]
            for o in observed:
                lo = len(o)
                if lo != l:
                    raise RuntimeError(f"observed sequence {o} of length {lo} incompatible with sequence {self} of length {l}")
                for i in range(l):
                    obs[i].append(o[i])

            for i in range(l):
                self.seq_els[i] = self.seq_els[i].observe(obs[i])
        else:
            raise NotImplementedError()
        
        return self

class TupleVariable(SequenceVariable):
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 tuple_els: List[VariableInfo]):
        for el in tuple_els:
            if el.is_sequence_chunk():
                raise RuntimeError("Tuples should not have sequence chunks")
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, tp=tuple, seq_els=tuple_els)

    def duplicate(self) -> 'TupleVariable':
        return TupleVariable(name=self.name, owner=self.owner, origin=self.origin, produced_by=self.produced_by, produced=self.produced, tuple_els=[el.duplicate() for el in self.seq_els])

class ConstantTuple(TupleVariable):
    value: tuple

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: tuple):
        self.value = value
        tuple_els = [owner.constant(name=name+".#el"+str(i), origin=origin, value=value[i], produced_by=produced_by, produced=produced) for i in range(len(value))]
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, tuple_els=tuple_els)

    def is_const(self) -> bool: return True
    def const_value(self) -> tuple: return self.value
    def duplicate(self) -> 'ConstantTuple': return type(self)(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)


class ListVariable(SequenceVariable):
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 tuple_els: List[VariableInfo]):
        for el in tuple_els:
            if el.is_sequence_chunk():
                raise RuntimeError("Tuples should not have sequence chunks")
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, tp=tuple, seq_els=tuple_els)

    def duplicate(self) -> 'ListVariable':
        return type(self)(name=self.name, owner=self.owner, origin=self.origin, produced_by=self.produced_by, produced=self.produced, tuple_els=[el.duplicate() for el in self.seq_els])

class ConstantList(ListVariable):
    value: list

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: list):
        self.value = value
        tuple_els = [owner.constant(name=name+".#el"+str(i), origin=origin, value=value[i], produced_by=produced_by, produced=produced) for i in range(len(value))]
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, tuple_els=tuple_els)

    def is_const(self) -> bool: return True
    def const_value(self) -> list: return self.value
    def duplicate(self) -> 'ConstantList': return type(self)(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)


class TensorShapeVariable(SequenceVariable):
    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 dims: List[VariableInfo]):
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by,produced=produced,tp=torch.Size,seq_els=dims)

    def __repr__(self) -> str:
        return f"{self.name}: {self.print_value()}"

    def sequence_element_at_index(self, i: int) -> IntVariable:
        result = super().sequence_element_at_index(i)
        assert isinstance(result, IntVariable)
        return result

    def __iter__(self) -> Iterator[IntVariable]:
        return super().__iter__() # pyright: ignore

    @overload
    def __getitem__(self, item: int) -> IntVariable: ...

    @overload
    def __getitem__(self, item: slice) -> Sequence[IntVariable]: ...

    def __getitem__(self, item: int|slice) -> IntVariable|Sequence[IntVariable]:
        result = super().__getitem__(item)
        if isinstance(item, int):
            if not isinstance(result, IntVariable):
                raise RuntimeError(f"TensorShape item {item} is not integer: {result}")
            return result
        elif isinstance(item, slice):
            # TODO check
            return result  # pyright: ignore
        else:
            raise NotImplementedError()

    #def __iter__(self) -> Iterator: return iter(self.dims)
    #def __next__(self, iter: Iterator) -> Optional[Iterator]: return next(iter)
    #def __len__(self) -> int: return len(self.dims)
    #def __getitem__(self, item: int) -> VariableInfo: return self.dims[item]

    #def print_value(self) -> str:
    #    return f"[{','.join([x.print_value() for x in self.dims])}]"

    #def is_const(self) -> bool:
    #    for d in self.dims:
    #        if not d.is_const():
    #            return False
    #    return True

    #def const_type(self) -> type: return list
    #def torch_type(self) -> TorchType: return torch.ListType(torch.IntType.get())

    #def const_value(self) -> Any:
    #    return list([d.const_value() for d in self.dims])

    #def is_sequence(self) -> bool:
    #    return True

    #def sequence_length(self) -> 'VariableInfo': return self.owner.constant(name=self.name+".#length", origin=self.origin, produced_by=self.produced_by, produced=self.produced, value=len(self.dims))

    #def sequence_length_is_const(self) -> bool:
    #    return True

    #def sequence_const_length(self) -> int:
    #    return len(self.dims)

    #def sequence_element_at_index(self, i: int) -> VariableInfo:
    #    return self.dims[i]

    def duplicate(self) -> VariableInfo: return TensorShapeVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,dims=self.seq_els)

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':

        length_counts: Dict[int, int] = {}

        for v in samples:
            l = len(v)
            if l in length_counts:
                length_counts[l] += 1
            else:
                length_counts[l] = 1

        if len(length_counts) > 1:
            raise NotImplementedError("multiple lengths")

        l = len(samples[0])

        subsamples = [[] for _ in range(l)]
        stp = torch_type.getElementType()

        for s in samples:
            assert isinstance(s, list)
            for i,ls in enumerate(s):
                subsamples[i].append(ls)

        dims = [vars.sampled(name=name+".#dim"+str(i), torch_type=stp, samples=ss, produced_by=produced_by, produced=produced) for i,ss in enumerate(subsamples)]

        return TensorShapeVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced, dims=dims)

    def observe(self, observed: List[Any]) -> 'TensorShapeVariable':
        observed_dims: List[List[int]] = [[] for _ in self.seq_els]
        for o in observed:
            assert len(o) == len(self.seq_els)
            for x,y in zip(o, observed_dims):
                assert isinstance(x, int)
                y.append(x)

        for i in range(len(self.seq_els)):
            self.seq_els[i] = self.seq_els[i].observe(observed_dims[i])

        return self

    @staticmethod
    def covering(vars: 'Variables', name: str, produced_by: Optional[Node|Graph], produced: int, infos: Sequence['VariableInfo']) -> 'VariableInfo':
        length_counts: Dict[int, int] = {}

        for v in infos:
            assert isinstance(v, TensorShapeVariable)
            l = len(v)
            if l in length_counts:
                length_counts[l] += 1
            else:
                length_counts[l] = 1

        if len(length_counts) > 1:
            raise NotImplementedError("multiple lengths")

        l,_ = length_counts.popitem()

        subsamples: List[List[IntVariable]] = [[] for _ in range(l)]

        for s in infos:
            assert isinstance(s, TensorShapeVariable)
            for i,ls in enumerate(s):
                subsamples[i].append(ls)

        dims = [vars.covering(name=name+".#dim"+str(i), options=ss, produced_by=produced_by, produced=produced) for i,ss in enumerate(subsamples)]

        return TensorShapeVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced, dims=dims)

class ConstantShape(TensorShapeVariable):
    value: torch.Size

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: torch.Size):
        self.value = value
        dims = [owner.constant(name=name+".#el"+str(i), origin=origin, value=value[i], produced_by=produced_by, produced=produced) for i in range(len(value))]
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, dims=dims)

    def is_const(self) -> bool: return True
    def const_value(self) -> torch.Size: return self.value
    def duplicate(self) -> VariableInfo: return type(self)(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)

class TensorVariable(VariableInfo):
    # The following are for Tensors.  We model the data type, device and shape separately.
    dtype: DataTypeVariable
    device: DeviceVariable
    shape: TensorShapeVariable

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 dtype: DataTypeVariable, device: DeviceVariable, shape: TensorShapeVariable):
        super().__init__(name, owner, origin, produced_by,produced)
        assert issubclass(dtype.const_type(), torch.dtype)
        assert issubclass(device.const_type(), torch.device)
        #print("shape", shape)
        assert shape.is_sequence()
        self.dtype = dtype
        self.device = device
        self.shape = shape

    def __repr__(self) -> str:
        return f"{self.name}: tensor {_print_tensor_info(self)}"

    def print_value(self) -> str: return _print_tensor_info(self)
    def const_type(self) -> type: return torch.Tensor
    def torch_type(self) -> TorchType: return torch.TensorType.get()
    def is_const(self) -> bool: return False
    def is_tensor(self) -> bool: return True
    def tensor_dtype(self) -> VariableInfo: return self.dtype
    def tensor_device(self) -> VariableInfo: return self.device
    def tensor_const_dtype(self) -> Optional[torch.dtype]:
        if self.dtype.is_const():
            return self.dtype.const_value()
        return None
    def tensor_const_device(self) -> Optional[torch.device]:
        if self.device.is_const():
            return self.device.const_value()
        return None
    def tensor_shape(self) -> VariableInfo: return self.shape

    def duplicate(self) -> VariableInfo:
        return TensorVariable(name=self.name, owner=self.owner, origin=self.origin, produced_by=self.produced_by,
                                produced=self.produced, dtype=self.dtype.duplicate(), device=self.device.duplicate(), shape=self.shape.duplicate())

    @staticmethod
    def sampled(name: str, tp: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':

        val_counts: Dict[Any, int] = {}
        for v in samples:
            if v in val_counts:
                val_counts[v] += 1
            else:
                val_counts[v] = 1

        tensor_dtypes: List[torch.dtype] = []
        tensor_devices: List[torch.device] = []
        tensor_shapes: List[List[int]] = []

        for s in samples:
            assert isinstance(s, torch.Tensor)
            tensor_dtypes.append(s.dtype)
            tensor_devices.append(s.device)
            tensor_shapes.append(list(s.shape))

        dtype = vars.sampled(name=name+".#dtype", torch_type=torch.IntType.get(), samples=tensor_dtypes, produced_by=produced_by, produced=produced)
        device = vars.sampled(name=name+".#device", torch_type=torch.DeviceObjType.get(), samples=tensor_devices, produced_by=produced_by, produced=produced)
        shape = TensorShapeVariable.sampled(name=name+".#shape", torch_type=torch.ListType(torch.IntType.get()), samples=tensor_shapes, produced_by=produced_by, produced=produced, vars=vars)

        return vars.tensor(name=name, origin=Origin.SAMPLE, dtype=dtype, device=device, shape=shape, produced_by=produced_by, produced=produced)

    def observe(self, observed: List[Any]) -> VariableInfo:
        devices,dtypes,shapes = [],[],[]

        for o in observed:
            assert isinstance(o, torch.Tensor)
            devices.append(o.device)
            dtypes.append(o.dtype)
            shapes.append(o.shape)

        self.device = self.device.observe(devices)
        self.dtype = self.dtype.observe(dtypes)
        self.shape = self.shape.observe(shapes)

        return self

    @staticmethod
    def covering(vars: 'Variables', name: str, produced_by: Optional[Node|Graph], produced: int, infos: Sequence['VariableInfo']) -> 'VariableInfo':
        devices,dtypes,shapes = [],[],[]

        for i in infos:
            i = i.as_tensor()
            devices.append(i.tensor_device())
            dtypes.append(i.tensor_dtype())
            shapes.append(i.tensor_shape())

        device = vars.covering(devices, name+".#device", produced_by, produced)
        dtype = vars.covering(dtypes, name+".#dtype", produced_by, produced)
        shape = vars.covering(shapes, name+".#shape", produced_by, produced)

        return vars.tensor(name=name, origin=Origin.CONST_PROP, dtype=dtype, device=device, shape=shape, produced_by=produced_by, produced=produced)

class ConstantTensor(TensorVariable):

    value: torch.Tensor

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: torch.Tensor):
        dtype=owner.constant(name=name+".#dtype", origin=origin, value=value.dtype, produced_by=produced_by, produced=produced).as_data_type()
        device=owner.constant(name=name+".#device", origin=origin, value=value.device, produced_by=produced_by, produced=produced).as_device()
        shape=owner.constant(name=name+".#shape", origin=origin, value=value.shape, produced_by=produced_by, produced=produced).as_shape()
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, dtype=dtype, device=device, shape=shape)
        self.value = value

    def is_const(self) -> bool: return True
    def const_value(self) -> Any: return self.value
    def duplicate(self) -> VariableInfo: return type(self)(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)

class OptionalVariable(VariableInfo):
    # Variable or None

    some: VariableInfo

    def __init__(self, name: str, some: VariableInfo):
        super().__init__(name, some.owner, some.origin, some.produced_by, some.produced)
        self.some = some

    def __repr__(self) -> str: return f"{self.name}:  optional {self.print_value()}"
    def print_value(self) -> str: return self.some.print_value() + "?"
    def is_const(self) -> bool: return False
    def is_optional(self) -> bool: return True
    def const_type(self) -> Type: return object
    def torch_type(self) -> TorchType: return torch.OptionalType(self.some.torch_type())
    def is_tensor(self) -> bool: return False
    def duplicate(self) -> VariableInfo: return OptionalVariable(name=self.name, some=self.some.duplicate())
    def non_optional(self) -> VariableInfo: return self.some

    @staticmethod
    def sampled(name: str, tp: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> VariableInfo:

        val_counts: Dict[Any, int] = {}
        for v in samples:
            if v in val_counts:
                val_counts[v] += 1
            else:
                val_counts[v] = 1

        # Filter out None from the list
        no_nones = [s for s in samples if s is not None]

        if len(no_nones) == 0:
            return vars.constant(name=name, origin=Origin.SAMPLE, value=None, produced_by=produced_by, produced=produced)

        contained_type = tp.getElementType()
        contained = vars.sampled(name + ".#some", contained_type, no_nones, produced_by=produced_by, produced=produced)

        if len(no_nones) == len(samples):
            return contained.renamed(name)

        result = OptionalVariable(name, contained)
        return result

    def observe(self, observed: List[Any]) -> VariableInfo:
        some_observed = []
        for o in observed:
            if o is None:
                continue
            some_observed.append(o)

        self.some = self.some.observe(some_observed)

        return self

def _print_var_fields(name: str, origin: Any, const: Any,
                      produced: Any, first_consumed: Any, last_consumed: Any,
                      produced_by: Any, tp: Any, const_value: Any) -> str:
    return f"{name:20} {origin:10} {const:6} {produced_by:15} {tp:12} {const_value}"
    #return f"{name:20} {origin:10} {const:6} {produced:5} {first_consumed:5} {last_consumed:5} {produced_by:15} {tp:12} {const_value}"

def _print_var_names() -> str:
    return _print_var_fields("name", "origin", "const",
                             "prod", "first", "last",
                             "node", "type", "value")

def _print_tensor_info(info: VariableInfo) -> str:
    """
    Return information of a tensor: dtype, shape, device
    """
    dt = info.tensor_const_dtype()
    if dt is None:
        value_str = "<dtype?>"
    else:
        value_str = _short_dtype(dt)

    value_str += _short_info_str(info.tensor_shape())

    dev = info.tensor_const_device()
    if dev is None:
        value_str += "<device?>"
    else:
        value_str += str(dev)
    return value_str

def _short_info_str(info: VariableInfo) -> str:
    if info.is_const():
        return _print_value(info.const_value())
    elif info.is_tensor():
        return _print_tensor_info(info)
    elif info.is_sequence():
        value_str = ""
        open = "[" if info.const_type() == list else '('
        close = "]" if info.const_type() == list else ')'

        if info.sequence_length_is_const():
            el_strs: List[str] = []
            l = info.sequence_const_length()
            for i in range(l):
                el = info.sequence_element_at_index(i)
                if el.is_const():
                    el_strs.append(_short_info_str(el))
                else:
                    el_strs.append(el.name + "=" + el.print_value())
            value_str = open + ", ".join(el_strs) + close
        elif info.seq_length is not None:
            assert info.seq_el is not None
            if info.const_type() == list:
                value_str = _short_info_str(info.seq_el) + str(info.seq_length)
            else:
                value_str = "(" + ",".join([_short_info_str(info.seq_el)] * info.seq_length.min)
                if info.seq_length.max > info.seq_length.min:
                    value_str += "..." + ")" + str(info.seq_length)
                else:
                    value_str += ")"
        else:
            print("unhandled list/sequence: info", info)
            raise RuntimeError("TODO: print unhandled list/sequence case")
        return value_str
    else:
        assert info.const_type() is not None
        return "<" + info.name + ": " + info.const_type().__name__ + ">"

def _print_var(name: str, info: VariableInfo) -> str:
    produced_kind = ''
    if isinstance(info.produced_by, Node):
        produced_kind = info.produced_by.kind().replace("aten::", "").replace("prim::", "")
    elif isinstance(info.produced_by, Graph):
        produced_kind = '<graph>'
    else:
        produced_kind = str(info.produced_by)

    type_str = ''
    if info.const_type() is not None:
        type_str = info.const_type().__name__

    value_str: str = ""
    if info.is_const():
        value_str = _print_value(info.const_value())
    elif info.is_optional():
        nonopt = info.non_optional()
        type_str = nonopt.const_type().__name__ + "?"
        value_str = nonopt.print_value() + "?"
    elif info.const_type() == torch.Tensor:
        value_str = _print_tensor_info(info)
    elif info.const_type() == list or info.const_type() == tuple:
        value_str = info.print_value()
        if False:
            open = "[" if info.const_type() == list else '('
            close = "]" if info.const_type() == list else ')'
            if info.seq_els is not None:
                el_strs: List[str] = []
                for el in info.seq_els:
                    el_strs.append(_short_info_str(el))
                value_str = open + ", ".join(el_strs) + close
            elif info.seq_length is not None:
                assert info.seq_el is not None
                if info.const_type() == list:
                    value_str = _short_info_str(info.seq_el) + str(info.seq_length)
                else:
                    value_str = "(" + ",".join([_short_info_str(info.seq_el)] * info.seq_length.min)
                    if info.seq_length.max > info.seq_length.min:
                        value_str += "..." + ")" + str(info.seq_length)
                    else:
                        value_str += ")"
            else:
                print("unhandled list/sequence: info", info)
                raise RuntimeError("TODO: print unhandled list/sequence case")

    else:
        pass

    return _print_var_fields(name, info.origin.name, info.is_const(), info.produced, info.first_consumed,
                             info.last_consumed, produced_kind, type_str, value_str)

class Expression(VariableInfo):
    """
    Represents the result of evaluating an expression
    """

    def __init__(self, op: str, args: List[VariableInfo], name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        super().__init__(name, owner, origin, produced_by, produced)
        arity = len(args)



class Variables:
    _vars: OrderedDict[str, VariableInfo]
    _outer: Optional['Variables']

    def __init__(self, outer: Optional['Variables'] = None):
        super().__init__()
        self._vars = OrderedDict()
        self._outer = outer

    def __len__(self) -> int:
        return len(self._vars)

    def __getitem__(self, item: str) -> VariableInfo:
        return self._vars[item]

    def items(self) -> Iterable[Tuple[str, VariableInfo]]:
        return self._vars.items()

    def keys(self) -> Iterable[str]:
        return self._vars.keys()

    def values(self) -> Iterable[VariableInfo]:
        return self._vars.values()

    def add(self, v: VariableInfo):
        assert v.produced_by is None or isinstance(v.produced_by, Node) or isinstance(v.produced_by, Graph)
        assert len(v.name) > 0
        if v.name in self._vars:
            raise RuntimeError(f"Attempt to double-add variable {v.name} {v}")
        assert v.name not in self._vars
        self._vars[v.name] = v

    def get(self, n: str, i: int) -> VariableInfo:
        if n in self._vars:
            result = self._vars[n]
            if i < result.first_consumed:
                result.first_consumed = i
            if i > result.last_consumed:
                result.last_consumed = i
            return result
        elif self._outer is not None:
            return self._outer.get(n, i)
        else:
            raise KeyError(n)

    def renamed(self, var: VariableInfo, new_name: str) -> VariableInfo:
        new_var = var.renamed(new_name)
        new_var.owner = self
        #self.add(new_var)
        return new_var

    def unify(self, value: Any, *vars: VariableInfo):
        """
        Say that all of the given variables have the given value.  Returns a variable that represents the value.
        """
        print("TODO: unify")

    def alias(self, *vars: VariableInfo) -> 'VariableInfo':
        """
        Say that all of the given variables have the same value (each is an alias of the other one).
        """
        if len(vars) < 2:
            return vars[0]

        known_values = []
        for v in vars:
            if v.is_const():
                val = v.const_value()
                if len(known_values) == 0:
                    known_values.append(val)
                elif known_values[0] == val:
                    pass # values should be the same if they are being unified
                else:
                    raise RuntimeError(f"Error: attempt to unify differing values: {val} and {known_values[0]}")

        if len(known_values) == 0:
            return vars[0]

        for v in vars:
            if not v.is_const():
                print(f"TODO: record alias {v} = {known_values[0]}")
                # TODO Record the alias
                pass

        return vars[0]

    def new_frame(self) -> 'Variables':
        return Variables(outer=self)

    def dump_vars(self, indent: str = '', start_at: int = 0):
        print(f"{indent} {fg.boldblack}{_print_var_names()}{reset}")
        for i,(name,info) in enumerate(self._vars.items()):
            if i < start_at:
                continue
            print(indent, _print_var(name, info))

    def sampled(self, name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return a VariableInfo that has samples from the various sets of values provided.
        """
        if len(samples) == 0:
            raise RuntimeError("can't call sampled() with no samples")
        elif len(samples) == 1:
            raise RuntimeError("single sample doesn't allow us to determine constness")
        
        val_counts: Dict[Any, int] = {}
        for v in samples:
            if v in val_counts:
                val_counts[v] += 1
            else:
                val_counts[v] = 1

        if len(val_counts) == 1:
            # It's a constant
            return self.constant(name=name, origin=Origin.SAMPLE, value=list(val_counts.keys())[0], produced_by=produced_by, produced=produced)

        if isinstance(torch_type, torch.TensorType):
            return TensorVariable.sampled(name, torch_type, samples, produced_by, produced, self)
        elif isinstance(torch_type, torch.OptionalType):
            return OptionalVariable.sampled(name, torch_type, samples, produced_by, produced, self)
        elif isinstance(torch_type, torch.TupleType):
            return SequenceVariable.sampled(name, torch_type, samples, produced_by, produced, self)
            contained = torch_type.containedTypes()
        elif isinstance(torch_type, torch.IntType):
            return IntVariable.sampled(name, torch_type, samples, produced_by, produced, self)
            print("scalar type", torch_type.scalarType())
            raise NotImplementedError()
        else:
            raise RuntimeError(f"Unknown torch_type {torch_type}")

        raise NotImplementedError()

    def scalar(self, *, name: str, origin: Origin, tp: Type, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return a scalar variable, specialized for the given type
        """
        if tp == int:
            return IntVariable(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced)
        elif tp == float:
            return FloatVariable(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced)
        elif tp == str:
            return StringVariable(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced)
        elif tp == bool:
            return BooleanVariable(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced)
        elif tp == torch.dtype:
            return DataTypeVariable(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced)
        elif tp == torch.device:
            return DeviceVariable(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced)
        else:
            raise RuntimeError("unknown scalar type")

    def expression(self, op: str, args: List[Any], name: str, origin: Origin, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        # Wrap each arg in a VariableInfo

        all_const = True
        values = []
        def wrap_arg(i: int, arg: Any) -> VariableInfo:
            if isinstance(arg, VariableInfo):
                if not arg.is_const():
                    all_const = False
                else:
                    values.append(arg.const_value())
                return arg
            else:
                values.append(arg)
                return self.constant(name=name+"#arg"+str(i), origin=Origin.EXPRESSION, value=arg, produced_by=None, produced=-1)
            
        wrapped_args = [wrap_arg(i, arg) for i,arg in enumerate(args)]

        graph = Graph()

        inputs = []

        for arg in wrapped_args:
            input = graph.addInput(arg.name)
            input.setType(arg.torch_type())
            inputs.append(input)

        print("inputs", inputs)

        node = graph.create(op, inputs, 1)

        print("node", node)

        operator = get_torch_operator(node)

        print("operator", operator)
        print("values", values)

        operator.const_prop()

        return Expression(op=op, args=args, name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced)

    def local(self, *, name: str, origin: Origin, tp: Type, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a local variable.  This should not be used for
        tensors.
        """
        assert not issubclass(tp, torch.Tensor), "Tensors should use the tensor method, not local"

        return self.scalar(name=name, tp=tp, origin=origin,
                            produced_by=produced_by, produced=produced)

    def tensor(self, *, name: str, origin: Origin,
               dtype: VariableInfo, device: VariableInfo, shape: VariableInfo,
               produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a tensor valued variable.
        """
        return TensorVariable(name=name, owner=self, origin=origin,
                            dtype=dtype, device=device, shape=shape,
                            produced_by=produced_by, produced=produced)

    def tensor_shape(self, *, name: str, origin: Origin, dims: List[VariableInfo], produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a tensor shape variable.
        """
        for d in dims:
            if not isinstance(d, IntVariable):
                raise RuntimeError(f"tensor_shape was not an integer: {d}")
        all_const = all(map(lambda x: x.is_const(), dims))
        if all_const:
            for x in dims:
                #print("dims", dims)                
                assert issubclass(x.const_type(), int)
            const_value = list([x.const_value() for x in dims])
            return self.constant(name=name, origin=origin, produced_by=produced_by, produced=produced,value=const_value)
        else:
            return TensorShapeVariable(owner=self, name=name, origin=origin, produced_by=produced_by, produced=produced,dims=dims)

    def any(self, *, name: str, origin: Origin, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for something that could be any type (nothing static is known
        about it).
        """
        return VariableInfo(name=name, origin=origin, is_const=False,
                            produced_by=produced_by, produced=produced)

    def homogeneous_sequence(self, *, name: str, origin: Origin, tp: Type[Sequence], produced_by: Optional[Node|Graph], produced: int,
                             length: VariableInfo, values: VariableInfo) -> VariableInfo:
        """
        Create the VariableInfo for a homogeneous sequence (tuple or list) with a fixed or
        variable length content of an instance of a single type.
        """
        els=[SequenceChunk(name=name+".#el", owner=self, origin=origin, produced_by=produced_by, produced=produced, length=length, el=values)]
        return SequenceVariable(name=name, owner=self, origin=origin, tp=tp,
                            seq_els=els, produced_by=produced_by, produced=produced)

    def inhomogeneous_sequence(self, *, name: str, origin: Origin, tp: Type[Sequence], produced_by: Optional[Node|Graph], produced: int,
                               values: List[VariableInfo]) -> VariableInfo:
        """
        Create the VariableInfo for an inhomogeneous sequence (tuple or list) with a fixed
        length and each element having a different type.
        """
        #print("inhomogeneous sequence")
        #for i,v in enumerate(values):
        #    print(_print_var(str(i), v))
        return SequenceVariable(name=name, owner=self, origin=origin, tp=tp, seq_els=values,
                            produced_by=produced_by, produced=produced)

    def constant(self, *, name: str, origin: Origin, value: Any, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a constant.  This will fill in all of the ancilliary
        information.
        """

        if value is None:
            return NoneVariable(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced)
        elif isinstance(value, int):
            return ConstantInt(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        elif isinstance(value, float):
            return ConstantFloat(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        elif isinstance(value, str):
            return ConstantString(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        elif isinstance(value, bool):
            return ConstantBool(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        elif isinstance(value, torch.Size):
            return ConstantShape(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        elif isinstance(value, tuple):
            return ConstantTuple(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        elif isinstance(value, list):
            return ConstantList(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        elif isinstance(value, torch.dtype):
            return ConstantDataType(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        elif isinstance(value, torch.device):
            return ConstantDevice(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        elif isinstance(value, torch.Tensor):
            return ConstantTensor(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
        else:
            return ConstantValue(name=name, owner=self, origin=origin, produced_by=produced_by, produced=produced, value=value)
            raise RuntimeError(f"Cannot create constant of type {type(value).__name__} {_print_value(value)}")

    def covering(self, options: Sequence[VariableInfo], name: str, produced_by: Optional[Node|Graph], produced: int) -> 'VariableInfo':
        ancestors = set(type(options[0]).mro())
        for i in range(1,len(options)):
            option = options[i]
            myancestors = set(type(option).mro())
            ancestors.intersection_update(myancestors)

        result: Type[VariableInfo] = VariableInfo
        for tp in type(options[0]).mro():
            if tp in ancestors:
                result = tp
                break

        assert issubclass(result, VariableInfo)

        print("using class", result, "for covering")

        all_const = True
        const_values = []
        for o in options:
            if o.is_const():
                const_values.append(o.const_value())
            else:
                all_const = False
        
        if all_const:
            # Can use the sampled method, which will complete constant propagation
            torch_type = _to_torch_type(None, const_values)

            unique_values = set()
            for v in const_values:
                unique_values.add(v)
            
            if len(unique_values) == 1:
                # It's a constant
                return self.constant(name=name, origin=Origin.CONST_PROP, value=unique_values.pop(), produced_by=produced_by, produced=produced)

            return result.sampled(name, torch_type, const_values, produced_by, produced, self)
        
        return result.covering(self, name, produced_by, produced, options)

    def add_covering(self, options: Sequence[VariableInfo], name: str, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """Add a variable of the given name, who could be a value given by any of the options"""
        if len(options) == 1:
            result = options[0].renamed(name)
        else:
            result = self.covering(options, name, produced_by, produced)

        result.owner = self
        self.add(result)
        return result

    def add_optional(self, options: Sequence[VariableInfo], name: str, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """Add a variable of the given name, whose value may be present and, if so, is given by any of the options"""
        if len(options) == 1:
            option = options[0]
            if option.is_optional():
                result = option.renamed(name)
            else:
                result = OptionalVariable(name=name, some=option)
            result.owner = self
            self.add(result)
            return result
        else:
            print("name", name, "options", options)
            raise NotImplementedError()

@dataclass
class Invocation:
    args: Tuple
    kwargs: OrderedDict[str, Any] = field(default_factory = OrderedDict)
    output: Tuple = field(default_factory = tuple)
    elapsed: float = 0

    def __str__(self) -> str:
        def summarize_arg(arg: Any) -> str:
            if isinstance(arg, dict):
                return str(dict({k: summarize_arg(v) for k,v in arg.items()}))
            elif isinstance(arg, tuple):
                return str(tuple(summarize_arg(v) for v in arg))
            elif isinstance(arg, list):
                return str(list([summarize_arg(v) for v in arg]))
            elif isinstance(arg, torch.Tensor):
                if arg.numel() < 10:
                    return str(arg)
                else:
                    return _print_param(arg.size(), arg.dtype, arg.device) + " " + str(arg.device)
            else:
                return str(arg)

        summarized_args = list(map(summarize_arg, self.args))
        summarized_kwargs = {k: summarize_arg(v) for k,v in self.kwargs.items()}

        return f"Invocation(elapsed={print_elapsed(self.elapsed)} args={summarized_args} kwargs={summarized_kwargs})"

@dataclass
class SummaryData:
    arg_lengths: Dict[int, int] = field(default_factory = dict)
    args: List[VariableInfo] = field(default_factory = list)
    kwargs: Dict[str, VariableInfo] = field(default_factory = dict)

    def print_args(self, indent: int = 0):
        ind = ' ' * indent
        for i in range(len(self.args)):
            arg = self.args[i]
            print(f"{ind}{i}: {arg}")

        for kw,arg in self.kwargs.items():
            print(f"{ind}{kw}: {arg}")

@dataclass
class Invocations:
    m: Module
    path: str
    sig: inspect.Signature
    calls: List[Invocation] = field(default_factory = list)
    children: Dict[str, 'Invocations'] = field(default_factory = dict)

    def __init__(self, m: Module, path: str, *,
                 sig: Optional[inspect.Signature] = None,
                 calls: Optional[List[Invocation]] = None,
                 children: Optional[Dict[str, 'Invocations']] = None):
        self.m = m
        self.path = path
        self.sig = inspect.signature(m.forward) if sig is None else sig
        self.calls = [] if calls is None else calls
        self.children = {} if children is None else children

    def total_runtime(self) -> float:
        return sum([c.elapsed for c in self.calls])

    def __str__(self) -> str:
        return f"Invocations(path={self.path} module={self.m._get_name()} runtime={print_elapsed(self.total_runtime())} ncalls={len(self.calls)} nchildren={len(self.children)})" # sig={inspect.signature(self.m.forward)})"

    def summarize(self) -> SummaryData:
        vars = Variables()
        result = SummaryData()

        max_nargs = 0
        all_kwargs: Set[str] = set()
        for c in self.calls:
            nargs = len(c.args)
            result.arg_lengths[nargs] = result.arg_lengths.get(nargs, 0) + 1
            max_nargs = max(max_nargs, nargs)
            all_kwargs.update(c.kwargs.keys())

        print("max_nargs", max_nargs)
        print("parameters=", self.sig.parameters)
        ordered_params: List[Tuple[str, inspect.Parameter]] = list(self.sig.parameters.items())

        for i in range(max_nargs):
            name,param = ordered_params[i]
            print("param", name, param)
            samples = [c.args[i] for c in self.calls if i < len(c.args[i])]

            torch_type = _to_torch_type(param.annotation, samples)
            result.args.append(vars.sampled(name=name, torch_type=torch_type, samples=samples, produced_by=None, produced=-1))
            

        for k in all_kwargs:
            param = self.sig.parameters[k]
            samples = [c.kwargs.get(k) for c in self.calls if k in c.kwargs]
            torch_type = _to_torch_type(param.annotation, samples)
            result.kwargs[k] = vars.sampled(name=k, torch_type=torch_type, samples=samples, produced_by=None, produced=-1)

        return result

