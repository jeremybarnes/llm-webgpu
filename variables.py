from enum import Enum
from typing import Type, Tuple, Any, Dict, List, Optional, OrderedDict, Union, TypeVar, Iterator, Iterable, Sequence, SupportsInt, overload, get_origin, get_args, Callable, TypeAlias
from dataclasses import dataclass, field
import torch
from torch._C import Node, Graph
from utils import typeify, _print_value, _short_dtype, first
from ansi.color import bg, fg
from ansi.color.fx import reset
import copy
import inspect

TorchType: TypeAlias = 'torch._C.JitType'

def _is_optional(field):
    """
    Returns whether the annotation is generated by typing.Optional[...]
    """
    return get_origin(field) is Union and \
           type(None) in get_args(field)

def _scalar_torch_type(tp: Type) -> TorchType:
    """
    Given the given scalar type, return the corresponding Torch type.
    """
    NoneType = type(None)
    if issubclass(tp, int):
        return torch.IntType.get()
    elif issubclass(tp, float):
        return torch.FloatType.get()
    elif issubclass(tp, bool):
        return torch.BoolType.get()
    elif tp == NoneType:
        return torch.NoneType.get()
    elif issubclass(tp, str):
        return torch.StringType.get()
    raise NotImplementedError(f"_scalar_torch_type for {tp}")

def _to_torch_type(ann: Any, samples: List[Any]) -> TorchType:
    """
    Converts the given parameter into its corresponding Torch JIT type
    """

    #print(p)

    origin = get_origin(ann)
    args = get_args(ann)

    if origin is None:
        if ann is None or ann is inspect._empty:
            # Do it based on the type seen
            if len(samples) == 0:
                # NO samples and no annotation
                return torch.AnyType()
            else:
                def get_type(sample) -> TorchType:
                    if isinstance(sample, torch.Tensor):
                        return torch.TensorType.create_from_tensor(sample)
                    elif sample is None:
                        return torch.NoneType.get()

                    raise RuntimeError(f"TODO: get_type {type(sample)}")
                
                current: Optional[TorchType] = None
                for t in [get_type(sample) for sample in samples]:
                    if current is None:
                        current = t
                    else:
                        if str(current) != str(t):
                            raise RuntimeError(f"wrong type {current} {t}")

                assert current is not None
                return current

            raise RuntimeError(f"empty {_print_value(samples)}")
        elif issubclass(ann, torch.Tensor):
            return torch.TensorType.create_from_tensor(ann())  
        elif issubclass(ann, torch.FloatTensor):
            return torch.TensorType.create_from_tensor(ann())
        elif ann is bool:
            return torch.BoolType.get()
        else:
            raise RuntimeError(f"TODO: no origin for {ann}")
    elif _is_optional(ann):
        contained,_ = args
        sub_samples = [s for s in samples if s is not None]
        #print("contained", contained)
        return torch.OptionalType(_to_torch_type(contained, sub_samples))
    elif origin == tuple:
        def get_el_type(i: int) -> TorchType:
            sub_samples = [s[i] for s in samples]
            return _to_torch_type(args[i], sub_samples)

        return torch.TupleType([get_el_type(i) for i in range(len(args))])

        raise RuntimeError(f"TODO: handle optional {contained}")


    print(ann, type(ann), get_origin(ann), get_args(ann))

    raise RuntimeError(f"_to_torch_type for {ann} {get_origin(ann)} {get_args(ann)}")

class Origin(Enum):
    UNKNOWN = 0
    SELF = 1
    ARG = 2
    DEFAULT_ARG = 3
    LOCAL = 4
    CONST_PROP = 5
    SAMPLE = 6

Tp = TypeVar('Tp')

@dataclass
class ShapeRange:
    min: int = 10000000000
    max: int = 0

    def is_empty(self) -> bool:
        """
        Is this an empty shape range?
        """
        return self.max < self.min

    def is_const(self) -> bool:
        """
        Is this shape a constant int value?
        """
        return self.min == self.max

    def const_value(self) -> int:
        """
        Return the constant shape for this dimension.

        PRE: is_const() is true.
        """
        if self.min != self.max:
            raise RuntimeError("asked for constant value with is_const() false")
        return self.min

    def broadcast_from(self, other: 'ShapeRange') -> None:
        """
        Updates this shape to broadcast from the other shape.  If they are incompatible for
        broadcasting, then it will throw.
        """
        if self.is_empty():
            self.min,self.max = other.min,other.max
            return
        if other.is_empty():
            return

        if self.min == 1 and self.max == 1:
            # This is a broadcastable dimension
            self.min,self.max = other.min,other.max
        elif other.min == 1 and other.max == 1:
            return
        elif self.min == other.min and self.max == other.max:
            return
        else:
            raise RuntimeError(f"non-broadcastable dimensions: {self} and {other}")

    def do(self, val: int):
        self.min = min(self.min, val)
        self.max = max(self.max, val)

    def add(self, val: 'ShapeRange'):
        if val.is_empty():
            return
        self.do(val.min)
        self.do(val.max)

    def __init__(self, val:Optional[Union[SupportsInt,'ShapeRange']] = None):
        if isinstance(val, ShapeRange):
            self.min = val.min
            self.max = val.max
        elif val is None:
            pass
        else:
            self.min = self.max = int(val)

    def __repr__(self) -> str:
        if self.is_empty():
            return "[*]"
        elif self.max == self.min:
            return f"[{self.max}]"
        else:
            return f"[{self.min}-{self.max}]"

@dataclass
class TensorShape:
    """
    Shape range for a fixed number of dimensions
    """
    dims: List[ShapeRange] = field(default_factory=list)

    def __init__(self, dims: Sequence[ShapeRange|SupportsInt]):
        self.dims = [ShapeRange(s) for s in dims]

    def __len__(self) -> int:
        return len(self.dims)

    @overload
    def __getitem__(self, item: int) -> ShapeRange: ...

    @overload
    def __getitem__(self, item: slice) -> Sequence[ShapeRange]: ...

    def __getitem__(self, item):
        return self.dims[item]

    def __iter__(self) -> Iterator[ShapeRange]:
        return iter(self.dims)

    def __repr__(self) -> str:
        return ''.join([str(s) for s in self.dims])

    def do(self, shape: List[int]):
        assert len(shape) == len(self)
        for dim,sh in zip(self.dims, shape):
            dim.do(sh)

    def add(self, shape: 'TensorShape'):
        assert len(shape) == len(self)
        for dim,sh in zip(self.dims, shape.dims):
            dim.add(sh)

    @staticmethod
    def from_tensor(t: torch.Tensor) -> 'TensorShape':
        """
        Return a TensorShape object from a single tensor.
        """
        return TensorShape(t.size())

    @staticmethod
    def scalar() -> 'TensorShape':
        """
        Construct a new TensorShapes object that represents a scalar.
        """
        return TensorShape([])

@dataclass
class TensorShapes:
    """
    Shape range for a variable number of dimensions.  For when something is called
    with multiple tensor dimensions.
    """

    lengths: Dict[int, TensorShape] = field(default_factory=dict)

    def add(self, shape: TensorShape):
        l = len(shape.dims)
        if l not in self.lengths:
            self.lengths[l] = shape
        else:
            self.lengths[l].add(shape)

    def do(self, size: Sequence[SupportsInt]):
        shape = TensorShape(size)
        l = len(shape)
        if l in self.lengths:
            self.lengths[l].add(shape)
        else:
            self.lengths[l] = shape

    def unique_length(self) -> TensorShape:
        assert len(self.lengths) == 1
        res = first(self.lengths.values())
        assert res is not None
        return res

    @staticmethod
    def from_tensor(t: torch.Tensor) -> 'TensorShapes':
        """
        Return a TensorShapes object from a single tensor.
        """
        result = TensorShapes()
        result.add(TensorShape.from_tensor(t))
        return result

    @staticmethod
    def from_shape(s: TensorShape | Sequence[ShapeRange | SupportsInt]) -> 'TensorShapes':
        """
        Construct a new TensorShapes object from a single TensorShape or dimension range.
        """
        if isinstance(s, TensorShape):
            return TensorShapes({len(s): s})
        else:
            return TensorShapes({len(s): TensorShape(s)})

    @staticmethod
    def scalar() -> 'TensorShapes':
        """
        Construct a new TensorShapes object that represents a scalar.
        """
        return TensorShapes({0: TensorShape.scalar()})

    def __repr__(self) -> str:
        return ' | '.join(str(shape) for len,shape in sorted(self.lengths.items()))

class Arg:
    """
    Base class for information about an argument to a function.
    """

    name: str

    def __init__(self, name: str):
        self.name = name

    def get_type(self) -> type:
        """
        Return the type of this argument, or a superclass if the type can vary.
        This should not return NoneType unless the argument is a constant None;
        instead, is_optional() should return true.
        """
        raise RuntimeError(f"Class {self.__class__.__name__} doesn't override get_type()")

    def get_torch_type(self) -> TorchType:
        """
        Return the torch JIT type of this argument, or a superclass if the type can vary.
        """
        raise RuntimeError(f"Class {self.__class__.__name__} doesn't override get_torch_type()")

    def is_optional(self) -> bool:
        """
        Return true if this is optional; in other words, if None is one of the possible
        values for the argument.  The other methods return information for the non-optional
        case.
        """
        return False

    def non_optional(self) -> 'Arg':
        """
        Returns the non-optional version of this type.  Default checks that is_optional() is
        false and returns self (which works for all non-optional types).
        """
        assert not self.is_optional()
        return self

    def get_dtype(self) -> Optional[torch.dtype]:
        """
        Return the dtype of this argument, None if it's not a tensor.
        """
        return None

    def get_device(self) -> Optional[torch.device]:
        """
        Return the device of this argument, None if it's not a tensor.
        """
        return None

    def get_shape(self) -> Optional['TensorShapes']:
        """
        Return the shape of this argument, None if it's not a tensor.
        """
        return None

    def is_const(self) -> bool:
        """
        Return whether the argument has a constant value.
        """
        return False

    def const_value(self) -> Any:
        """
        Return the constant value for the argument.
        """
        raise RuntimeError("Attempt to obtain constant value from non-constant argument")

class UnknownArg(Arg):
    def __init__(self, name: str):
        super().__init__(name)

    def __str__(self) -> str:
        return "Unknown()"

    def get_type(self) -> type: return object

    def get_torch_type(self) -> TorchType: return torch.AnyType()

class ConstantArg(Arg):
    value: Any

    def __init__(self, name: str, value: Any):
        super().__init__(name)
        self.value = value

    def __repr__(self) -> str:
        return f"Constant({self.value})"

    def get_type(self) -> type: return type(self.value)

    def get_torch_type(self) -> TorchType: return _to_torch_type(None, [self.value])

    def is_const(self) -> bool: return True

    def const_value(self) -> Any: return self.value

class TensorArg(Arg):
    """
    Describes a tensor-valued argument.
    """
    dtype: torch.dtype
    device: torch.device
    shape: TensorShapes

    def __init__(self, name: str, dtype: torch.dtype, device: torch.device, shape: TensorShapes):
        super().__init__(name)
        self.dtype = dtype
        self.device = device
        self.shape = shape

    def __repr__(self) -> str:
        return f"Tensor({self.dtype}{self.shape}{self.device})"

    def get_type(self) -> type: return torch.Tensor
    def get_torch_type(self) -> TorchType: return torch.TensorType.get().with_dtype(self.dtype) #.with_sizes(self.get_torch_sizes())
    def get_dtype(self) -> Optional[torch.dtype]: return self.dtype
    def get_device(self) -> Optional[torch.device]: return self.device
    def get_shape(self) -> Optional['TensorShapes']: return self.shape

class OptionalArg(Arg):
    """
    Describes an argument that can be either None or another value, representing
    optional or defaulted values.
    """
    value: Arg

    def __init__(self, name: str, value: Arg):
        super().__init__(name)
        self.value = value

    def __repr__(self) -> str:
        return f"Optional({self.value})"

    def is_optional(self) -> bool: return True
    def non_optional(self) -> 'Arg': return self.value
    def get_type(self) -> type: return self.value.get_type()
    def get_torch_type(self) -> TorchType: return torch.OptionalType(self.value.get_torch_type())
    def get_dtype(self) -> Optional[torch.dtype]: return self.value.get_dtype()
    def get_device(self) -> Optional[torch.device]: return self.value.get_device()
    def get_shape(self) -> Optional['TensorShapes']: return self.value.get_shape()

class TupleArg(Arg):
    """
    Fixed-length, non-homogeneous tuple.
    """

    values: List[Arg]

    def __init__(self, name, values: List[Arg]):
        super().__init__(name)
        self.values = values

    def __repr__(self) -> str:
        return f"Tuple({self.values})"

    def get_type(self) -> type: return tuple
    def get_torch_type(self) -> TorchType: return torch.TupleType([v.get_torch_type() for v in self.values])

class ListTupleArg(Arg):
    """
    Variable length, homogeneous tuple.
    """

    length: ShapeRange
    value: Arg

    def __init__(self, name: str, length: ShapeRange, value: Arg):
        super().__init__(name)
        self.length = length
        self.value = value

    def __repr__(self) -> str:
        return f"ListTuple({self.value}{self.length})"

    def get_type(self) -> type: return tuple
    def get_torch_type(self) -> TorchType: return torch.TupleType([self.value.get_torch_type()])

@dataclass
class ArgumentData:
    name: str
    torch_type: TorchType
    count: int = 0
    types: Dict[type, int] = field(default_factory = dict)
    tensor_dtypes: Dict[torch.dtype, int] = field(default_factory = dict)
    tensor_devices: Dict[torch.device, int] = field(default_factory = dict)
    tensor_shapes: Dict[torch.Size, int] = field(default_factory = dict)
    tensor_values: Dict[torch.Tensor, int] = field(default_factory = dict, repr=False)
    tuple_lengths: Dict[int, int] = field(default_factory=dict)
    tuple_args: List['ArgumentData'] = field(default_factory=list)
    list_lengths: Dict[int, int] = field(default_factory=dict)
    values: Dict[Any, int] = field(default_factory = dict, repr=False)

    def _non_optional_torch_type(self) -> TorchType:
        """
        Returns the torch_type, ignoring optional annotations.
        """
        if isinstance(self.torch_type, torch.OptionalType):
            return self.torch_type.getElementType()
        else:
            return self.torch_type

    def is_homogeneous(self, other: 'ArgumentData'):
        """
        Tells us whether this and the other type are homogeneous and can be combined without
        much loss of generality.
        """

        #print("is_homogeneous", str(self.summarize()), str(other.summarize()))

        return str(self.summarize()) == str(other.summarize())

    def make_homogeneous(self, other: 'ArgumentData') -> 'ArgumentData':
        """
        Make a version that maps onto all of the others.
        """
        v = ArgumentData()
        v.combine(self)
        v.combine(other)
        return v

    def combine(self, other: 'ArgumentData'):
        """
        Combine the two arguments, producing one which can accept either of the inputs.
        """
        self.count += other.count
        
        def combine_types(t1: TorchType, t2: TorchType) -> TorchType:
            if t1 == t2:
                return t1
            raise RuntimeError(f"TODO: combine_types {t1} {t2}")

        self.type = combine_types(self.torch_type, other.torch_type)

        def update_counts(v1: Dict[Any, int], v2: Dict[Any, int]):
            for k,v in v2.items():
                v1[k] = v1.get(k, 0) + v

        update_counts(self.types, other.types)
        update_counts(self.tensor_dtypes, other.tensor_dtypes)
        update_counts(self.tensor_devices, other.tensor_devices)
        update_counts(self.tensor_shapes, other.tensor_shapes)
        update_counts(self.tensor_values, other.tensor_values)
        update_counts(self.tuple_lengths, other.tuple_lengths)
        update_counts(self.list_lengths, other.list_lengths)
        update_counts(self.values, other.values)

        for i in range(len(other.tuple_args)):
            if i < len(self.tuple_args):
                self.tuple_args[i].combine(other.tuple_args[i])
            else:
                self.tuple_args.append(copy.copy(other.tuple_args[i]))

    def add(self, a: Any, tt: TorchType):
        """
        Add the given argument instance to the analysis.
        """
        at = type(a)

        self.count += 1
        self.types[at] = self.types.get(at, 0) + 1

        if isinstance(a, torch.Tensor):
            sh = a.shape
            dt = a.dtype
            dv = a.device
            self.tensor_shapes[sh] = self.tensor_shapes.get(sh, 0) + 1
            self.tensor_dtypes[dt] = self.tensor_dtypes.get(dt, 0) + 1
            self.tensor_devices[dv] = self.tensor_devices.get(dv, 0) + 1
            self.tensor_values[a] = self.tensor_values.get(a, 0) + 1

            if len(sh) == 0:  # scalar
                pass
        elif isinstance(a, dict):
            raise RuntimeError("TODO: dict type")
            pass
        elif isinstance(a, list):
            raise RuntimeError("TODO: list type")
            pass
        elif isinstance(a, tuple):
            print("doing tuple", typeify(a))

            tl = len(a)
            self.tuple_lengths[tl] = self.tuple_lengths.get(tl, 0) + 1

            non_optional = self._non_optional_torch_type()
            print("torch_type", self.torch_type)
            print("non_optional", non_optional)

            assert isinstance(non_optional, torch.TupleType)
            element_types = non_optional.elements()
            print("element_types", element_types)
            print("tuple_args", self.tuple_args)
            print("len(element_types)", len(element_types))
            print("len(a)", len(a))

            while len(self.tuple_args) < tl:
                if len(element_types) == 1:
                    element_type = element_types[0]
                else:
                    element_type = element_types[len(self.tuple_args)]
                self.tuple_args.append(ArgumentData(name=self.name+".#"+str(len(self.tuple_args)), torch_type=element_type))

            for i in range(tl):
                if len(element_types) == 1:
                    element_type = element_types[0]
                else:
                    element_type = element_types[i]

                print(f"adding element {i} el_type {element_type} a {typeify(a[i])}")
                self.tuple_args[i].add(a[i], element_type)
        elif isinstance(a, (float, int, str, bool)):
            self.values[a] = self.values.get(a, 0) + 1
            pass

    def summarize(self) -> Arg:
        if len(self.types) == 0:
            return UnknownArg(self.name)

        non_optional_types = self.types.copy()

        def identity(x: Arg) -> Arg:
            return x

        def make_optional(x: Arg) -> Arg:
            return OptionalArg(self.name, x)

        wrapper = identity

        if type(None) in self.types:
            if len(self.types) == 1:
                return ConstantArg(self.name, None)
            del non_optional_types[type(None)]
            wrapper = make_optional
        
        if len(non_optional_types) > 1:
            raise NotImplementedError(f"Can't handle multiple types {self.types} yet")
        first_type: type = list(non_optional_types.keys())[0]

        if len(self.values) == 1:
            first_value: Any = list(self.values.keys())[0]
            return ConstantArg(self.name, first_value)

        if issubclass(first_type, torch.Tensor):
            if len(self.tensor_dtypes) > 1:
                raise NotImplementedError("Can't handle multiple tensor types yet")
            tensor_dtype = list(self.tensor_dtypes)[0]

            if len(self.tensor_devices) > 1:
                raise NotImplementedError("Can't handle multiple tensor devices yet")
            tensor_device = list(self.tensor_devices)[0]

            shapes = TensorShapes()

            for sh,_ in self.tensor_shapes.items():
                shapes.add(TensorShape(sh))

            return wrapper(TensorArg(self.name, tensor_dtype, tensor_device, shapes))

        elif issubclass(first_type, tuple):
            if len(self.tuple_lengths) > 1:

                # Assume the tuple is like a list
                lens = ShapeRange()
                for k,l in self.tuple_lengths.items():
                    lens.do(l)

                # Homogenize the tuple length data
                arg = ArgumentData()
                for a in self.tuple_args:
                    arg.combine(a)

                return wrapper(ListTupleArg(self.name, lens, arg.summarize()))
            else:
                if len(self.tuple_args) == 0:
                    return wrapper(TupleArg(self.name, []))

                        
                try:
                    def get_homogeneous(a1: ArgumentData, a2: ArgumentData):
                        if a1.is_homogeneous(a2):
                            return a1.make_homogeneous(a2)
                        else:
                            raise RuntimeError("can't make homogeneous")

                    # Try to get a homogenous type for the tuple arguments
                    common = self.tuple_args[0]
                    for a in self.tuple_args:
                        common = get_homogeneous(common, a)
                    lens = ShapeRange()
                    lens.do(len(self.tuple_args))
                    return wrapper(ListTupleArg(self.name, lens, common.summarize()))
                except:
                    pass

                # Homogeneous length tuple, assume it's differently typed
                args = [a.summarize() for a in self.tuple_args]
                return wrapper(TupleArg(self.name, args))

        raise NotImplementedError(f"Summarize of argument of type {first_type}")

class VariableInfo:
    """
    Holds the static information known about a variable.
    """
    name: str = ""
    owner: 'Variables'
    origin: Origin = Origin.UNKNOWN

    def print_value(self) -> str:
        raise RuntimeError(f"Must override print_value() for class {type(self).__name__}")

    def const_type(self) -> type:
        raise RuntimeError(f"Must override const_type() for class {type(self).__name__}")

    def torch_type(self) -> TorchType:
        raise RuntimeError(f"Must override torch_type() for class {type(self).__name__}")

    def is_const(self) -> bool:
        raise RuntimeError(f"Must override is_const() for class {type(self).__name__}")

    def const_value(self) -> Any:
        if self.is_const():
            raise RuntimeError(f"Must override const_value() if is_const() can be true for type {type(self)}")
        else:
            raise RuntimeError("Attempt to access const value for non-const variable")

    def is_none(self) -> bool:
        """
        Return true if this is a constant None value
        """
        return self.const_type() == type(None)

    def is_optional(self) -> bool:
        return False

    def is_tensor(self) -> bool:
        return False

    def is_sequence(self) -> bool:
        return False

    def is_sequence_chunk(self) -> bool:
        return False

    def sequence_length_is_const(self) -> bool:
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_const_length(self) -> int:
        if self.sequence_length_is_const():
            raise RuntimeError(f"Must override if sequence_length_is_const() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_length(self) -> 'VariableInfo':
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def sequence_element_at_index(self, i: int) -> 'VariableInfo':
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def element_type_is_const(self) -> bool:
        if self.is_sequence():
            raise RuntimeError(f"Must override if is_sequence() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")
        
    def element_const_type(self) -> type:
        if self.element_type_is_const():
            raise RuntimeError(f"Must override if element_const_type() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call non-sequence type {type(self).__name__} {self}")

    def tensor_dtype_is_const(self) -> bool:
        return self.tensor_const_dtype() is not None

    def tensor_const_dtype(self) -> Optional[torch.dtype]:
        if self.is_tensor():
            raise RuntimeError(f"Must override tensor_const_dtype() if is_tensor() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call tensor_const_dtype() for non-tensor type {type(self).__name__} {self}")

    def tensor_dtype(self) -> 'VariableInfo':
        """
        Returns the variable containing the dtype of this tensor.
        """
        raise RuntimeError(f"Attempt to call tensor_dtype() for non-tensor type {type(self).__name__} {self}")

    def tensor_device_is_const(self) -> bool:
        return self.tensor_const_device() is not None

    def tensor_const_device(self) -> Optional[torch.device]:
        if self.is_tensor():
            raise RuntimeError(f"Must override const_device() if is_tensor() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call const_device() for non-tensor type {type(self).__name__} {self}")

    def tensor_device(self) -> 'VariableInfo':
        """
        Returns the variable containing the dtype of this tensor.
        """
        raise RuntimeError(f"Attempt to call const_device() for non-tensor type {type(self).__name__} {self}")

    def tensor_shape(self) -> 'VariableInfo':
        """
        Returns the variable containing the shape of this tensor.
        """
        if self.is_tensor():
            raise RuntimeError(f"Must override tensor_shape() if is_tensor() can be true for type {type(self)}")
        raise RuntimeError(f"Attempt to call tensor_shape() for non-tensor type {type(self).__name__} {self}")

    # Which node produced, and which nodes read, this value.  The integers are the sequence
    # of nodes in the graph.
    produced_by: Optional[Node|Graph] = None
    produced: int = 0
    first_consumed: int = 10000
    last_consumed: int = 0

    def __init__(self, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int):
        self.name = name
        self.owner = owner
        self.origin = origin
        self.produced_by = produced_by
        self.produced = produced

    def typed_const_value(self, tp: Type[Tp]) -> Optional[Tp]:
        """
        If this variable is a constant, check that its and instance of the given type and return it.
        If it's not a constant, return None.
        Used to extract values of constant arguments from VariableInfo during constant propagation.
        """
        if not self.is_const():
            return None
        if not isinstance(self.const_value(), tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value())} of value {self.const_value()}")
        return self.const_value()

    def typed_const_nonnull_value(self, tp: Type[Tp]) -> Tp:
        """
        Assert that this variable is a constant, of the given type, and return that value.
        Used to extract values of constant arguments from VariableInfo during constant propagation.
        """
        if not self.is_const():
            raise RuntimeError(f"Expected constant value but got {self}")
        if not isinstance(self.const_value(), tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value())} of value {self.const_value()}")
        return self.const_value()

    def typed_default_value(self, default: Tp) -> Optional[Tp]:
        """
        If this variable is a constant and not None, check that its and instance of the given type and return it.
        If it's a constant and None, then return the default.
        If it's not a constant, return None.
        Used to extract values of defaulted constant arguments from VariableInfo during constant propagation.
        """
        tp = type(default)
        if not self.is_const():
            return None
        if self.const_value() is None:
            return default
        if not isinstance(self.const_value(), tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value())} of value {self.const_value()}")
        return self.const_value()

    def duplicate(self) -> 'VariableInfo':
        """
        Duplicate operator.  Needed to create another version because deepcopy(x) doesn't work and some objects are
        mutable.
        """
        raise RuntimeError(f"TODO: duplicate() for {type(self)}")
        result = copy.copy(self)
        if self.seq_els is not None:
            result.seq_els = [el.deepcopy() for el in self.seq_els]
        if self.seq_length is not None:
            result.seq_length = copy.copy(self.seq_length)
        if self.tensor_shape is not None:
            result.tensor_shape = copy.deepcopy(self.tensor_shape)
        return result

    def renamed(self, new_name: str) -> 'VariableInfo':
        res = self.duplicate()
        res.name = new_name
        return res

    def is_homogeneous(self, other: 'VariableInfo'):
        """
        Are these homogeneous, meaning that they can be combined into a single
        VariableInfo that covers both of them.
        """
        if self.const_type() != other.const_type():
            return False
        if self.is_const() and other.is_const():
            return self.const_value() == other.const_value()
        if self.const_type() == torch.Tensor:
            type1 = self.const_dtype()
            type2 = other.const_dtype()

            if type1 != type2:
                # TODO: they may be homogeneous; there may be a type that covers both
                return False

            device1 = self.const_device
            device2 = other.const_device

            if device1 != device2:
                return False

            return True
        elif self.const_type() == tuple or self.const_type() == list:
            raise RuntimeError("is_homogeneous for sequence")

        return True

    def combine(self, other: 'VariableInfo') -> 'VariableInfo':
        """
        Combine the two VariableInfos together to create one that covers both.
        """
        raise RuntimeError(f"need to implement combine() on {type(self).__name__}")

        raise NotImplementedError(f"combine(): {self} with {other}")

        if other.is_optional:
            self.is_optional = True

        if other.const_type() != self.const_type():
            raise RuntimeError("TODO: combine with two different types")

        if self.is_const() and not other.is_const():
            self.const_value = None
            self.is_const = False

        if self.const_type() == torch.Tensor:
            if self.const_dtype != other.const_dtype:
                self.const_dtype = None
            if self.const_device != other.const_device:
                self.const_device = None
            if other.tensor_shape() is not None and self.tensor_shape() is not None:
                for sh in other.tensor_shape().lengths.values():
                    self.tensor_shape().add(sh)
        elif self.const_type() == tuple or self.const_type() == list:
            raise RuntimeError("TODO: combine sequences")

        self.produced = min(self.produced, other.produced)
        self.first_consumed = min(self.first_consumed, other.first_consumed)
        self.last_consumed = max(self.last_consumed, other.last_consumed)

class ConstantVariable(VariableInfo):
    # Anything which is constant

    value: Any

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 value: Any):
        super().__init__(name, owner, origin, produced_by, produced)
        self.value = value

    def __repr__(self) -> str:
        return f"{self.name}: const {type(self.value).__name__} = {self.print_value()}"

    # Iterator class that wraps values in a VariableInfo
    class MyIterator:
        it: Iterator
        owner: VariableInfo
        i: int
        def __init__(self, it: Iterator, owner: VariableInfo):
            self.it = it
            self.owner = owner
            self.i = 0
        def __next__(self):
            val = next(self.it)
            result = self.owner.owner.constant(name=self.owner.name+"#el"+str(self.i),origin=self.owner.origin,value=val,produced_by=self.owner.produced_by,produced=self.owner.produced)
            self.i += 1
            return result

    def __iter__(self) -> MyIterator:
        return ConstantVariable.MyIterator(iter(self.value),self)

    def __len__(self) -> int:
        return len(self.value)

    def __getitem__(self, item) -> VariableInfo:
        return self.owner.constant(name=self.name+"#el"+str(item),origin=self.origin,value=self.value[item],produced_by=self.produced_by,produced=self.produced)

    def print_value(self) -> str:
        return _print_value(self.value)

    def is_const(self) -> bool:
        return True

    def const_type(self) -> type:
        return type(self.value)

    def torch_type(self) -> TorchType:
        return _to_torch_type(None, [self.value])

    def const_value(self) -> Any:
        return self.value

    def is_sequence(self) -> bool:
        return isinstance(self.value, list) or isinstance(self.value, tuple)

    def sequence_length_is_const(self) -> bool:
        return True

    def is_tensor(self) -> bool:
        return isinstance(self.value, torch.Tensor)

    def tensor_dtype(self) -> VariableInfo:
        if isinstance(self.value, torch.Tensor):
            return self.owner.constant(name=self.name + ".#dtype", value=self.value.dtype, origin=self.origin, produced=self.produced, produced_by=self.produced_by)
        raise RuntimeError(f"Attempt to get tensor_dtype from non-tensor constant value {self}")

    def tensor_const_dtype(self) -> Optional[torch.dtype]:
        if isinstance(self.value, torch.Tensor):
            return self.value.dtype
        raise RuntimeError(f"cannot get tensor dtype for non-tensor constant value {self}")

    def tensor_device(self) -> VariableInfo:
        if isinstance(self.value, torch.Tensor):
            return self.owner.constant(name=self.name + ".#device", value=self.value.device, origin=self.origin, produced=self.produced, produced_by=self.produced_by)
        raise RuntimeError(f"Attempt to get tensor_device from non-tensor constant value {self}")

    def tensor_const_device(self) -> Optional[torch.device]:
        if isinstance(self.value, torch.Tensor):
            return self.value.device
        raise RuntimeError("cannot get tensor device for non-tensor constant value")

    def tensor_shape(self) -> VariableInfo:
        if isinstance(self.value, torch.Tensor):
            return self.owner.constant(name=self.name+".#shape", origin=self.origin, value=list(self.value.shape),
                                       produced_by=self.produced_by,produced=self.produced)
            return self.value.device
        raise RuntimeError("cannot get tensor device for non-tensor constant value")

    def duplicate(self) -> VariableInfo: return ConstantVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,value=self.value)

    def combine(self, other: 'VariableInfo') -> 'VariableInfo':
        type1 = self.const_type()
        type2 = other.const_type()
        if type1 != type2:
            raise NotImplementedError(f"combine differing types {type1} {type2}")

        if isinstance(other, ConstantVariable):
            raise NotImplementedError(f"combine {self} {other}")
        elif isinstance(other, ScalarVariable):
            return other.duplicate()
        else:
            raise NotImplementedError(f"combine {self} {other}")

class TensorVariable(VariableInfo):
    # The following are for Tensors.  We model the data type, device and shape separately.
    dtype: VariableInfo
    device: VariableInfo
    shape: VariableInfo

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 dtype: VariableInfo, device: VariableInfo, shape: VariableInfo):
        super().__init__(name, owner, origin, produced_by,produced)
        assert issubclass(dtype.const_type(), torch.dtype)
        assert issubclass(device.const_type(), torch.device)
        print("shape", shape)
        assert shape.is_sequence()
        self.dtype = dtype
        self.device = device
        self.shape = shape

    def __repr__(self) -> str:
        return f"{self.name}: tensor {_print_tensor_info(self)}"

    def print_value(self) -> str: return _print_tensor_info(self)
    def const_type(self) -> type: return torch.Tensor
    def torch_type(self) -> TorchType: raise NotImplementedError()  # TODO
    def is_const(self) -> bool: return False
    def is_tensor(self) -> bool: return True
    def tensor_dtype(self) -> VariableInfo: return self.dtype
    def tensor_device(self) -> VariableInfo: return self.device
    def tensor_const_dtype(self) -> Optional[torch.dtype]:
        if self.dtype.is_const():
            return self.dtype.const_value()
        return None
    def tensor_const_device(self) -> Optional[torch.device]:
        if self.device.is_const():
            return self.device.const_value()
        return None
    def tensor_shape(self) -> VariableInfo: return self.shape

    def duplicate(self) -> VariableInfo:
        return TensorVariable(name=self.name, owner=self.owner, origin=self.origin, produced_by=self.produced_by,
                                produced=self.produced, dtype=self.dtype.duplicate(), device=self.device.duplicate(), shape=self.shape.duplicate())

    @staticmethod
    def sampled(name: str, tp: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':

        val_counts: Dict[Any, int] = {}
        for v in samples:
            if v in val_counts:
                val_counts[v] += 1
            else:
                val_counts[v] = 1

        tensor_dtypes: List[torch.dtype] = []
        tensor_devices: List[torch.device] = []
        tensor_shapes: List[List[int]] = []

        for s in samples:
            assert isinstance(s, torch.Tensor)
            tensor_dtypes.append(s.dtype)
            tensor_devices.append(s.device)
            tensor_shapes.append(list(s.shape))

        dtype = vars.sampled(name=name+".#dtype", torch_type=torch.IntType.get(), samples=tensor_dtypes, produced_by=produced_by, produced=produced)
        device = vars.sampled(name=name+".#device", torch_type=torch.DeviceObjType.get(), samples=tensor_devices, produced_by=produced_by, produced=produced)
        shape = TensorShapeVariable.sampled(name=name+".#shape", torch_type=torch.ListType(torch.IntType.get()), samples=tensor_shapes, produced_by=produced_by, produced=produced, vars=vars)

        return vars.tensor(name=name, origin=Origin.SAMPLE, dtype=dtype, device=device, shape=shape, produced_by=produced_by, produced=produced)

class SequenceChunk(VariableInfo):
    # This is a variable length chunk of unknown elements, each of which has a schema
    # that represents its value.

    el: VariableInfo
    length: VariableInfo

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 el: VariableInfo, length: VariableInfo):
        super().__init__(name, owner, origin, produced_by,produced)
        self.el = el
        self.length = length

    def print_value(self) -> str:
        result = "..." + self.el.print_value() + "[" + self.length.print_value() + "]..."
        return result

    def is_sequence_chunk(self) -> bool: return True


class SequenceVariable(VariableInfo):

    # The following are for sequences (lists and tuples).  They are modeled as a series of VariableInfo values, which
    # each represent either a list item or a variable length chunk of elements in a list.
    tp: Type # normally tuple or list
    seq_els: List[VariableInfo]

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 tp: Type, seq_els: List[VariableInfo]):
        super().__init__(name, owner, origin, produced_by,produced)
        self.tp = tp
        self.seq_els = seq_els

    def print_value(self) -> str:
        brackets = "<>"
        if issubclass(self.tp, tuple):
            brackets = "()"
        elif issubclass(self.tp, list):
            brackets = "[]"
        else:
            raise RuntimeError("unknown sequence type")
        result = brackets[0] + ','.join([e.print_value() for e in self.seq_els]) + brackets[1]
        return result

    def is_const(self) -> bool: return False
    def is_tensor(self) -> bool: return False
    def is_sequence(self) -> bool: return True
    def const_type(self) -> type: return self.tp
    def torch_type(self) -> TorchType:
        torch_types = [el.torch_type() for el in self.seq_els]
        if issubclass(self.tp, list):
            common: TorchType = _common_torch_type(*torch_types)
            return torch.ListType(common)
        elif issubclass(self.tp, tuple):
            return torch.TupleType(torch_types)
        else:
            raise RuntimeError("Unknown sequence torch type")

    def sequence_length(self) -> 'VariableInfo': raise NotImplementedError()
    def sequence_length_is_const(self) -> bool:
        for el in self.seq_els:
            if el.is_sequence_chunk() and not el.sequence_length_is_const():
                return False
        return True

    def sequence_const_length(self) -> int:
        res = 0
        for el in self.seq_els:
            if el.is_sequence_chunk():
                res += el.sequence_const_length()
            else:
                res += 1
        return res

    def sequence_element_at_index(self, i: int) -> VariableInfo:
        assert self.seq_els is not None
        return self.seq_els[i]

    def __iter__(self) -> Iterator:
        assert self.seq_els is not None
        return iter(self.seq_els)

    def __next__(self, iter: Iterator) -> Optional[Iterator]:
        raise NotImplementedError("TODO")

    def __getitem__(self, item: int) -> VariableInfo:
        assert self.seq_els is not None
        return self.seq_els[item]

    def duplicate(self) -> VariableInfo:
        new_seq_els = list([e.duplicate() for e in self.seq_els])
        return SequenceVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,
                                tp=self.tp,seq_els=new_seq_els)

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':
        print("torch_type", torch_type)

        assert isinstance(torch_type, torch.TupleType)
        els = torch_type.elements()

        length_counts: Dict[int, int] = {}
        length_samples: List[int] = []
        too_long = 0
        too_short = 0

        for v in samples:
            l = len(v)
            length_samples.append(l)
            if l < len(els):
                too_short += 1
            if l > len(els):
                too_long += 1
            if l in length_counts:
                length_counts[l] += 1
            else:
                length_counts[l] = 1

        print("length_counts", length_counts)

        if len(els) == 1 and too_long > 0:
            # It was written as tuple[x] but actually it means list[x] as a tuple
            all_samples = []
            for v in samples:
                all_samples.extend(v)

            el_info = vars.sampled(name=name+".#els", torch_type=els[0], samples=all_samples, produced_by=produced_by, produced=produced)
            len_info = IntVariable.sampled(name=name+".#length", torch_type=torch.IntType.get(), samples=length_samples, produced_by=produced_by, produced=produced, vars=vars)
            return vars.homogeneous_sequence(name=name, origin=Origin.SAMPLE, tp=tuple, produced_by=produced_by, produced=produced, length=len_info, values=el_info)

        if len(length_counts) > 1:
            raise NotImplementedError("multiple lengths")

        l = len(samples[0])
        el_samples: List[List[Any]] = [[] for _ in range(l)]
        for v in samples:
            for s,el in zip(el_samples,v):
                s.append(el)

        el_info = [vars.sampled(name=name+".#el"+str(i),torch_type=ttp, samples=s, produced_by=produced_by, produced=produced) for i,(ttp,s) in enumerate(zip(els, el_samples))]

        return vars.inhomogeneous_sequence(name=name, origin=Origin.SAMPLE, tp=tuple, produced_by=produced_by, produced=produced, values=el_info)


class TensorShapeVariable(VariableInfo):
    dims: List[VariableInfo]

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 dims: List[VariableInfo]):
        super().__init__(name, owner, origin, produced_by,produced)
        self.dims = dims
        for d in dims:
            assert isinstance(d, VariableInfo)

    def __repr__(self) -> str:
        return f"{self.name}: {self.print_value()}"

    def __iter__(self) -> Iterator: return iter(self.dims)
    def __next__(self, iter: Iterator) -> Optional[Iterator]: return next(iter)
    def __len__(self) -> int: return len(self.dims)
    def __getitem__(self, item: int) -> VariableInfo: return self.dims[item]

    def print_value(self) -> str:
        return f"[{','.join([x.print_value() for x in self.dims])}]"

    def is_const(self) -> bool:
        for d in self.dims:
            if not d.is_const():
                return False
        return True

    def const_type(self) -> type: return list
    def torch_type(self) -> TorchType: return torch.ListType(torch.IntType.get())

    def const_value(self) -> Any:
        return list([d.const_value() for d in self.dims])

    def is_sequence(self) -> bool:
        return True

    def sequence_length(self) -> 'VariableInfo': return self.seq_length

    def sequence_length_is_const(self) -> bool:
        return True

    def sequence_const_length(self) -> int:
        return len(self.dims)

    def sequence_element_at_index(self, i: int) -> VariableInfo:
        return self.dims[i]

    def duplicate(self) -> VariableInfo: return TensorShapeVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,dims=list([d.duplicate() for d in self.dims]))

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':

        length_counts: Dict[int, int] = {}

        for v in samples:
            l = len(v)
            if l in length_counts:
                length_counts[l] += 1
            else:
                length_counts[l] = 1

        if len(length_counts) > 1:
            raise NotImplementedError("multiple lengths")

        l = len(samples[0])

        subsamples = [[] for _ in range(l)]
        stp = torch_type.getElementType()

        for s in samples:
            assert isinstance(s, list)
            for i,ls in enumerate(s):
                subsamples[i].append(ls)

        dims = [vars.sampled(name=name+".#dim"+str(i), torch_type=stp, samples=ss, produced_by=produced_by, produced=produced) for i,ss in enumerate(subsamples)]

        return TensorShapeVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced, dims=dims)

class ScalarVariable(VariableInfo):
    # Anything which is a scalar value with a known type

    type: Type

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 type: Type):
        assert not issubclass(type, torch.Tensor)
        super().__init__(name, owner, origin, produced_by,produced)
        self.type = type

    def __repr__(self) -> str: return f"{self.name}:  {self.type.__name__}"
    def print_value(self) -> str: return f"<{self.name}: {self.type.__name__}>"
    def is_const(self) -> bool: return False
    def const_type(self) -> Type: return self.type
    def torch_type(self) -> TorchType: return _scalar_torch_type(self.type)
    def is_tensor(self) -> bool: return False
    def duplicate(self) -> VariableInfo: return ScalarVariable(name=self.name,owner=self.owner,origin=self.origin,produced_by=self.produced_by,produced=self.produced,type=self.type)

    def combine(self, other: 'VariableInfo') -> 'VariableInfo':
        type1 = self.const_type()
        type2 = other.const_type()
        if type1 != type2:
            raise NotImplementedError(f"combine ScalarVariables of differing types {type1} {type2}")
        return self.duplicate()

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> 'VariableInfo':

        type_samples: Dict[type, List[Any]] = {}
        for v in samples:
            t = type(v)
            if t in type_samples:
                type_samples[t].append(v)
            else:
                type_samples[t] = [v]

        if len(type_samples) > 1:
            print("type_samples", type_samples)
            raise NotImplementedError("multiple types")

        for t,s in type_samples.items():
            if t == int:
                return IntVariable.sampled(name, torch_type, s, produced_by, produced, vars)
            raise NotImplementedError("scalar of type", t)

        raise NotImplementedError()

class IntVariable(ScalarVariable):

    min_value: Optional[int] = None
    max_value: Optional[int] = None

    def __init__(self, *, name: str, owner: 'Variables', origin: Origin, produced_by: Optional[Node|Graph], produced: int,
                 min_value: Optional[int] = None, max_value: Optional[int] = None):
        super().__init__(name=name, owner=owner, origin=origin, produced_by=produced_by, produced=produced, type=int)
        self.min_value = min_value
        self.max_value = max_value

    def __repr__(self) -> str: return f"{self.name}:  {self.print_value()}"
    def print_value(self) -> str:
        result = "int"
        if self.min_value is not None or self.max_value is not None:
            result += "{"
            if self.min_value is not None:
                result += str(self.min_value)
            result += ":"
            if self.max_value is not None:
                result += str(self.max_value)
            result += "}"
        return result

    @staticmethod
    def sampled(name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> VariableInfo:
        min_value = min(samples)
        max_value = max(samples)
        assert isinstance(torch_type, torch.IntType)
        return IntVariable(name=name, owner=vars, origin=Origin.SAMPLE, produced_by=produced_by, produced=produced, min_value=min_value, max_value=max_value)

class OptionalVariable(VariableInfo):
    # Variable or None

    some: VariableInfo

    def __init__(self, some: VariableInfo):
        super().__init__(some.name, some.owner, some.origin, some.produced_by, some.produced)
        self.some = some

    def __repr__(self) -> str: return f"{self.name}:  optional {self.print_value()}"
    def print_value(self) -> str: return self.some.print_value() + "?"
    def is_const(self) -> bool: return False
    def const_type(self) -> Type: return object
    def is_tensor(self) -> bool: return False
    def duplicate(self) -> VariableInfo: return OptionalVariable(some=self.some.duplicate())

    @staticmethod
    def sampled(name: str, tp: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int, vars: 'Variables') -> VariableInfo:

        val_counts: Dict[Any, int] = {}
        for v in samples:
            if v in val_counts:
                val_counts[v] += 1
            else:
                val_counts[v] = 1

        # Filter out None from the list
        no_nones = [s for s in samples if s is not None]

        if len(no_nones) == 0:
            return vars.constant(name=name, origin=Origin.SAMPLE, value=None, produced_by=produced_by, produced=produced)

        contained_type = tp.getElementType()
        contained = vars.sampled(name + ".#some", contained_type, no_nones, produced_by=produced_by, produced=produced)

        if len(no_nones) == len(samples):
            return contained

        result = OptionalVariable(contained)
        return result


def _print_var_fields(name: str, origin: Any, const: Any, produced: Any, first_consumed: Any, last_consumed: Any, produced_by: Any, tp: Any, const_value: Any) -> str:
    return f"{name:20} {origin:10} {const:6} {produced:5} {first_consumed:5} {last_consumed:5} {produced_by:15} {tp:12} {const_value}"

def _print_var_names() -> str:
    return _print_var_fields("name", "origin", "const", "prod", "first", "last", "node", "type", "value")

def _print_tensor_info(info: VariableInfo) -> str:
    """
    Return information of a tensor: dtype, shape, device
    """
    dt = info.tensor_const_dtype()
    if dt is None:
        value_str = "<dtype?>"
    else:
        value_str = _short_dtype(dt)

    value_str += _short_info_str(info.tensor_shape())

    dev = info.tensor_const_device()
    if dev is None:
        value_str += "<device?>"
    else:
        value_str += str(dev)
    return value_str

def _short_info_str(info: VariableInfo) -> str:
    if info.is_const():
        return _print_value(info.const_value())
    elif info.is_tensor():
        return _print_tensor_info(info)
    elif info.is_sequence():
        value_str = ""
        open = "[" if info.const_type() == list else '('
        close = "]" if info.const_type() == list else ')'

        if info.sequence_length_is_const():
            el_strs: List[str] = []
            l = info.sequence_const_length()
            for i in range(l):
                el_strs.append(_short_info_str(info.sequence_element_at_index(i)))
            value_str = open + ", ".join(el_strs) + close
        elif info.seq_length is not None:
            assert info.seq_el is not None
            if info.const_type() == list:
                value_str = _short_info_str(info.seq_el) + str(info.seq_length)
            else:
                value_str = "(" + ",".join([_short_info_str(info.seq_el)] * info.seq_length.min)
                if info.seq_length.max > info.seq_length.min:
                    value_str += "..." + ")" + str(info.seq_length)
                else:
                    value_str += ")"
        else:
            print("unhandled list/sequence: info", info)
            raise RuntimeError("TODO: print unhandled list/sequence case")
        return value_str
    else:
        assert info.const_type() is not None
        return "<" + info.name + ": " + info.const_type().__name__ + ">"

def _print_var(name: str, info: VariableInfo) -> str:
    produced_kind = ''
    if isinstance(info.produced_by, Node):
        produced_kind = info.produced_by.kind().replace("aten::", "").replace("prim::", "")
    elif isinstance(info.produced_by, Graph):
        produced_kind = '<graph>'
    else:
        produced_kind = str(info.produced_by)

    type_str = ''
    if info.const_type() is not None:
        type_str = info.const_type().__name__

    value_str: str = ""
    if info.is_const():
        value_str = _print_value(info.const_value())
    elif info.const_type() == torch.Tensor:
        value_str = _print_tensor_info(info)
    elif info.const_type() == list or info.const_type() == tuple:
        value_str = info.print_value()
        if False:
            open = "[" if info.const_type() == list else '('
            close = "]" if info.const_type() == list else ')'
            if info.seq_els is not None:
                el_strs: List[str] = []
                for el in info.seq_els:
                    el_strs.append(_short_info_str(el))
                value_str = open + ", ".join(el_strs) + close
            elif info.seq_length is not None:
                assert info.seq_el is not None
                if info.const_type() == list:
                    value_str = _short_info_str(info.seq_el) + str(info.seq_length)
                else:
                    value_str = "(" + ",".join([_short_info_str(info.seq_el)] * info.seq_length.min)
                    if info.seq_length.max > info.seq_length.min:
                        value_str += "..." + ")" + str(info.seq_length)
                    else:
                        value_str += ")"
            else:
                print("unhandled list/sequence: info", info)
                raise RuntimeError("TODO: print unhandled list/sequence case")

    else:
        pass

    return _print_var_fields(name, info.origin.name, info.is_const(), info.produced, info.first_consumed,
                             info.last_consumed, produced_kind, type_str, value_str)

@dataclass
class Variables:
    vars: OrderedDict[str, VariableInfo] = field(default_factory=OrderedDict)

    def __len__(self) -> int:
        return len(self.vars)

    def add(self, v: VariableInfo):
        assert v.produced_by is None or isinstance(v.produced_by, Node) or isinstance(v.produced_by, Graph)
        assert len(v.name) > 0
        assert v.name not in self.vars
        self.vars[v.name] = v

    def add_constant(self, n: str, v: Any, node: Node, i: int):
        assert isinstance(node, Node)
        info = VariableInfo(n, Origin.CONST_PROP, True, v, node, i)
        self.add(info)

    def get(self, n: str, i: int) -> VariableInfo:
        result = self.vars[n]
        if i < result.first_consumed:
            result.first_consumed = i
        if i > result.last_consumed:
            result.last_consumed = i
        return result

    def renamed(self, var: VariableInfo, new_name: str) -> VariableInfo:
        new_var = var.renamed(new_name)
        new_var.owner = self
        self.add(new_var)
        return new_var

    def unify(self, value: Any, *vars: VariableInfo):
        """
        Say that all of the given variables have the given value.
        """
        print("TODO: unify")

    def alias(self, *vars: VariableInfo):
        """
        Say that all of the given variables have the same value (each is an alias of the other one).
        """
        if len(vars) < 2:
            return

        known_values = []
        for v in vars:
            if v.is_const():
                val = v.const_value()
                if len(known_values) == 0:
                    known_values.append(val)
                elif known_values[0] == val:
                    pass # values should be the same if they are being unified
                else:
                    raise RuntimeError(f"Error: attempt to unify differing values: {val} and {known_values[0]}")

        if len(known_values) == 0:
            return

        for v in vars:
            if not v.is_const():
                print(f"TODO: record alias {v} = {known_values[0]}")
                # TODO Record the alias
                pass


    def new_frame(self) -> 'Variables':
        result = Variables()
        for s,i in self.vars.items():
            i2 = i.duplicate()
            i2.owner = self
            result.add(i2)
        return result

    def dump_vars(self, indent: str = '', start_at: int = 0):
        print(f"{indent} {fg.boldblack}{_print_var_names()}{reset}")
        for i,(name,info) in enumerate(self.vars.items()):
            if i < start_at:
                continue
            print(indent, _print_var(name, info))

    def argument(self, *, name: str, produced_by: Optional[Node|Graph], observed: 'ArgumentData', torch_type: TorchType) -> VariableInfo:
        """
        Return the variable info for an argument.  This will specialize based on the observed
        values.
        """

        summary = observed.summarize()

        if summary.is_const():
            val = summary.const_value()
            return ConstantVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1, value=val)

        wrap: Callable[[VariableInfo], VariableInfo] = lambda x: x

        is_optional = summary.is_optional()
        if is_optional:
            summary = summary.non_optional()
            wrap = lambda x: OptionalVariable(x)

        dtype: Optional[torch.dtype] = summary.get_dtype()
        device: Optional[torch.device] = summary.get_device()
        shape: Optional[TensorShapes] = summary.get_shape()
        tp: Optional[Type] = summary.get_type()
        ttp: Optional[TorchType] = summary.get_torch_type()

        tp,ttp,dtype,device,shape = _unify_types(summary, torch_type)

        def constant_or_scalar(name: str, value: Optional[Tp], type: Type[Tp]) -> VariableInfo:
            if value is not None:
                assert isinstance(value, type)
                return ConstantVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                           value=value)
            else:
                return ScalarVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                         type=type)

        assert tp is not None
        if issubclass(tp, torch.Tensor):
            dtypevi = constant_or_scalar(name+"#.dtype", dtype, torch.dtype)
            devicevi = constant_or_scalar(name+".#device", device, torch.device)
            assert shape is not None

            def shape_to_var(name: str, shape: TensorShape) -> List[VariableInfo]:

                def dim_to_var(name: str, dim: ShapeRange) -> VariableInfo:
                    if dim.is_const():
                        return ConstantVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1, value=dim.const_value())
                    else:
                        return ScalarVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                         type=int)

                dims: List[VariableInfo] = []

                for i,dim in enumerate(shape.dims):
                    dims.append(dim_to_var(name + f"[{i}]", dim))

                return dims


            

            dims = shape_to_var(name+".#shape", shape.unique_length())

            shapevi = TensorShapeVariable(name=name+".#shape", owner=self,origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                          dims=dims)


            if dtype is not None:
                dtypevi = ConstantVariable(name=name+".#dtype", owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                           value=dtype)
            else:
                dtypevi = ScalarVariable(name=name+".#dtype", owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                                         type=torch.dtype)

            return wrap(TensorVariable(name=name, owner=self, origin=Origin.ARG,
                                  produced_by=produced_by, produced=-1, dtype=dtypevi, device=devicevi,shape=shapevi))
        elif issubclass(tp, tuple) or issubclass(tp, list):
            if isinstance(ttp, torch.TupleType):
                els = ttp.elements()
                nels = len(els)
                assert len(observed.tuple_args) == nels
                el_args: List[VariableInfo] = []

                for i,el in enumerate(els):
                    el_observed = observed.tuple_args[i]
                    el_arg = self.argument(name=name+".#el" + str(i), produced_by=produced_by, observed=el_observed, torch_type=el)
                    el_args.append(el_arg)

                nels = self.constant(name=name+".#length", origin=Origin.ARG, value=nels, produced_by=produced_by,produced=-1)

                return wrap(SequenceVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1,
                            tp=tp, seq_length=nels, seq_els=el_args, seq_el=None))
            else:
                raise NotImplementedError("TODO: list args")

            return wrap(SequenceVariable(name=name, owner=self, origin=Origin.ARG, produced_by=produced_by, produced=-1))
            raise NotImplementedError("TODO: implement list or tuple arguments")
        else:
            raise NotImplementedError(f"TODO: implement other types of arguments {ttp} {tp}")

        result = ArgumentVariable(name=name, owner=self, origin=Origin.ARG,
                              produced_by=produced_by, produced=-1, arg=summary)

        return result

    def sampled(self, name: str, torch_type: TorchType, samples: List[Any], produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return a VariableInfo that has samples from the various sets of values provided.
        """
        if len(samples) == 0:
            raise RuntimeError("can't call sampled() with no samples")
        elif len(samples) == 1:
            raise RuntimeError("single sample doesn't allow us to determine constness")
        
        val_counts: Dict[Any, int] = {}
        for v in samples:
            if v in val_counts:
                val_counts[v] += 1
            else:
                val_counts[v] = 1

        if len(val_counts) == 1:
            # It's a constant
            return self.constant(name=name, origin=Origin.SAMPLE, value=list(val_counts.keys())[0], produced_by=produced_by, produced=produced)

        if isinstance(torch_type, torch.TensorType):
            return TensorVariable.sampled(name, torch_type, samples, produced_by, produced, self)
        elif isinstance(torch_type, torch.OptionalType):
            return OptionalVariable.sampled(name, torch_type, samples, produced_by, produced, self)
        elif isinstance(torch_type, torch.TupleType):
            return SequenceVariable.sampled(name, torch_type, samples, produced_by, produced, self)
            contained = torch_type.containedTypes()
        elif isinstance(torch_type, torch.IntType):
            return ScalarVariable.sampled(name, torch_type, samples, produced_by, produced, self)
            print("scalar type", torch_type.scalarType())
            raise NotImplementedError()
        else:
            raise RuntimeError(f"Unknown torch_type {torch_type}")

        raise NotImplementedError()

    def local(self, *, name: str, origin: Origin, tp: Type, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a local variable.  This should not be used for
        tensors.
        """
        assert not issubclass(tp, torch.Tensor), "Tensors should use the tensor method, not local"

        return ScalarVariable(name=name, owner=self, type=tp, origin=origin,
                            produced_by=produced_by, produced=produced)

    def tensor(self, *, name: str, origin: Origin,
               dtype: VariableInfo, device: VariableInfo, shape: VariableInfo,
               produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a tensor valued variable.
        """
        return TensorVariable(name=name, owner=self, origin=origin,
                            dtype=dtype, device=device, shape=shape,
                            produced_by=produced_by, produced=produced)

    def tensor_shape(self, *, name: str, origin: Origin, dims: List[VariableInfo], produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a tensor shape variable.
        """
        all_const = all(map(lambda x: x.is_const(), dims))
        if all_const:
            for x in dims:
                #print("dims", dims)                
                assert issubclass(x.const_type(), int)
            const_value = list([x.const_value() for x in dims])
            return self.constant(name=name, origin=origin, produced_by=produced_by, produced=produced,value=const_value)
        else:
            return TensorShapeVariable(owner=self, name=name, origin=origin, produced_by=produced_by, produced=produced,dims=dims)

    def any(self, *, name: str, origin: Origin, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for something that could be any type (nothing static is known
        about it).
        """
        return VariableInfo(name=name, origin=origin, is_const=False,
                            produced_by=produced_by, produced=produced)

    def homogeneous_sequence(self, *, name: str, origin: Origin, tp: Type[Sequence], produced_by: Optional[Node|Graph], produced: int,
                             length: VariableInfo, values: VariableInfo) -> VariableInfo:
        """
        Create the VariableInfo for a homogeneous sequence (tuple or list) with a fixed or
        variable length content of an instance of a single type.
        """
        els=[SequenceChunk(name=name+".#el", owner=self, origin=origin, produced_by=produced_by, produced=produced, length=length, el=values)]
        return SequenceVariable(name=name, owner=self, origin=origin, tp=tp,
                            seq_els=els, produced_by=produced_by, produced=produced)

    def inhomogeneous_sequence(self, *, name: str, origin: Origin, tp: Type[Sequence], produced_by: Optional[Node|Graph], produced: int,
                               values: List[VariableInfo]) -> VariableInfo:
        """
        Create the VariableInfo for an inhomogeneous sequence (tuple or list) with a fixed
        length and each element having a different type.
        """
        #print("inhomogeneous sequence")
        #for i,v in enumerate(values):
        #    print(_print_var(str(i), v))
        return SequenceVariable(name=name, owner=self, origin=origin, tp=tp, seq_els=values,
                            produced_by=produced_by, produced=produced)

    def constant(self, *, name: str, origin: Origin, value: Any, produced_by: Optional[Node|Graph], produced: int) -> VariableInfo:
        """
        Return the variable info for a constant.  This will fill in all of the ancilliary
        information.
        """

        return ConstantVariable(name=name, owner=self, origin=origin, value=value,
                            produced_by=produced_by, produced=produced)


def _unify_types(summary: Arg, torch_type: TorchType) -> Tuple[Type, TorchType, Optional[torch.dtype], Optional[torch.device], Optional[TensorShapes]]:

    #print("torch_type", type(torch_type), torch_type, dir(torch_type), torch_type.annotation_str)
    #print("summary", summary)

    if isinstance(torch_type, torch.TensorType):
        tp: Optional[Type] = summary.get_type()
        ttp: Optional[TorchType] = summary.get_torch_type()
        dtype: Optional[torch.dtype] = summary.get_dtype()
        device: Optional[torch.device] = summary.get_device()
        shape: Optional[TensorShapes] = summary.get_shape()
        return tp,ttp,dtype,device,shape
    elif isinstance(torch_type, torch.OptionalType):
        contained = torch_type.getElementType()

        #print("contained", torch_type.getElementType())
        tp,ttp,dtype,device,shape = _unify_types(summary.non_optional(), contained)
        #print(f"tp {tp} torch_type {torch_type} dtype {dtype} device {device} shape {shape}")
        if summary.is_optional():
            return object,torch_type,None,None,None
            # Optional...
            raise RuntimeError("TOOD: Both Optional")
        else:
            # Not actually optional, as we've never observed without a value
            return _unify_types(summary, contained)
            raise RuntimeError("TODO: Optional")
    elif isinstance(torch_type, torch.TupleType):
        contained = torch_type.containedTypes()
        print("contained", contained)
        print("contained dir", dir(contained))
        if isinstance(summary, ListTupleArg):
            # Unify tuple with each argument
            for i in range(len(contained)):
                st = contained[i]
                tp,ttp,dtype,device,shape = _unify_types(summary.value, st)
                # TODO: return something meaningful here...
            return tuple,torch_type,None,None,None
        elif isinstance(summary, TupleArg):
            # Unify each element of the tuple
            assert len(summary.values) == len(contained)
            for c,v in zip(contained,summary.values):
                _unify_types(v,c)
            return tuple,torch_type,None,None,None
        else:
            raise RuntimeError(f"Torch is tuple but argument is {summary}")
    else:
        raise RuntimeError(f"Unknown torch_type {torch_type}")
