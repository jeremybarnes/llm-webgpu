from enum import Enum
from typing import Type, Tuple, Any, Dict, List, Optional, OrderedDict, Union, TypeVar, Iterator, Iterable, Sequence, SupportsInt, overload, get_origin, get_args
from dataclasses import dataclass, field
import torch
from torch._C import Node
from utils import typeify, _print_value, _short_dtype
from ansi.color import bg, fg
from ansi.color.fx import reset
import copy
import inspect

def _is_optional(field):
    """
    Returns whether the annotation is generated by typing.Optional[...]
    """
    return get_origin(field) is Union and \
           type(None) in get_args(field)

def _to_torch_type(ann: Any, samples: List[Any]) -> 'torch._C.JitType':
    """
    Converts the given parameter into its corresponding Torch JIT type
    """

    #print(p)

    origin = get_origin(ann)
    args = get_args(ann)

    if origin is None:
        if ann is None or ann is inspect._empty:
            # Do it based on the type seen
            if len(samples) == 0:
                # NO samples and no annotation
                return torch.AnyType()
            else:
                def get_type(sample) -> 'torch._C.JitType':
                    if isinstance(sample, torch.Tensor):
                        return torch.TensorType.create_from_tensor(sample)
                    elif sample is None:
                        return torch.NoneType.get()

                    raise RuntimeError(f"TODO: get_type {type(sample)}")
                
                current: Optional['torch._C.JitType'] = None
                for t in [get_type(sample) for sample in samples]:
                    if current is None:
                        current = t
                    else:
                        if str(current) != str(t):
                            raise RuntimeError(f"wrong type {current} {t}")

                assert current is not None
                return current

            raise RuntimeError(f"empty {_print_value(samples)}")
        elif issubclass(ann, torch.Tensor):
            return torch.TensorType.create_from_tensor(ann())  
        elif issubclass(ann, torch.FloatTensor):
            return torch.TensorType.create_from_tensor(ann())
        elif ann is bool:
            return torch.BoolType.get()
        else:
            raise RuntimeError(f"TODO: no origin for {ann}")
    elif _is_optional(ann):
        contained,_ = args
        sub_samples = [s for s in samples if s is not None]
        #print("contained", contained)
        return torch.OptionalType(_to_torch_type(contained, sub_samples))
    elif origin == tuple:
        def get_el_type(i: int) -> 'torch._C.JitType':
            sub_samples = [s[i] for s in samples]
            return _to_torch_type(args[i], sub_samples)

        return torch.TupleType([get_el_type(i) for i in range(len(args))])

        raise RuntimeError(f"TODO: handle optional {contained}")


    print(ann, type(ann), get_origin(ann), get_args(ann))

    raise RuntimeError(f"_to_torch_type for {ann} {get_origin(ann)} {get_args(ann)}")

class Origin(Enum):
    UNKNOWN = 0
    SELF = 1
    ARG = 2
    DEFAULT_ARG = 3
    LOCAL = 4
    CONST_PROP = 5

class Constness(Enum):
    UNKNOWN = 0
    VAR = 1
    CONST_TYPE = 2

Tp = TypeVar('Tp')

@dataclass
class ShapeRange:
    min: int = 10000000000
    max: int = 0

    def is_empty(self) -> bool:
        """
        Is this an empty shape range?
        """
        return self.max < self.min

    def is_const(self) -> bool:
        """
        Is this shape a constant int value?
        """
        return self.min == self.max

    def const_value(self) -> int:
        """
        Return the constant shape for this dimension.

        PRE: is_const() is true.
        """
        if self.min != self.max:
            raise RuntimeError("asked for constant value with is_const() false")
        return self.min

    def broadcast_from(self, other: 'ShapeRange') -> None:
        """
        Updates this shape to broadcast from the other shape.  If they are incompatible for
        broadcasting, then it will throw.
        """
        if self.is_empty():
            self.min,self.max = other.min,other.max
            return
        if other.is_empty():
            return

        if self.min == 1 and self.max == 1:
            # This is a broadcastable dimension
            self.min,self.max = other.min,other.max
        elif other.min == 1 and other.max == 1:
            return
        elif self.min == other.min and self.max == other.max:
            return
        else:
            raise RuntimeError(f"non-broadcastable dimensions: {self} and {other}")

    def do(self, val: int):
        self.min = min(self.min, val)
        self.max = max(self.max, val)

    def add(self, val: 'ShapeRange'):
        if val.is_empty():
            return
        self.do(val.min)
        self.do(val.max)

    def __init__(self, val:Optional[Union[SupportsInt,'ShapeRange']] = None):
        if isinstance(val, ShapeRange):
            self.min = val.min
            self.max = val.max
        elif val is None:
            pass
        else:
            self.min = self.max = int(val)

    def __repr__(self) -> str:
        if self.is_empty():
            return "[*]"
        elif self.max == self.min:
            return f"[{self.max}]"
        else:
            return f"[{self.min}-{self.max}]"

@dataclass
class TensorShape:
    """
    Shape range for a fixed number of dimensions
    """
    dims: List[ShapeRange] = field(default_factory=list)

    def __init__(self, dims: Sequence[ShapeRange|SupportsInt]):
        self.dims = [ShapeRange(s) for s in dims]

    def __len__(self) -> int:
        return len(self.dims)

    @overload
    def __getitem__(self, item: int) -> ShapeRange: ...

    @overload
    def __getitem__(self, item: slice) -> Sequence[ShapeRange]: ...

    def __getitem__(self, item):
        return self.dims[item]

    def __iter__(self) -> Iterator[ShapeRange]:
        return iter(self.dims)

    def __repr__(self) -> str:
        return ''.join([str(s) for s in self.dims])

    def do(self, shape: List[int]):
        assert len(shape) == len(self)
        for dim,sh in zip(self.dims, shape):
            dim.do(sh)

    def add(self, shape: 'TensorShape'):
        assert len(shape) == len(self)
        for dim,sh in zip(self.dims, shape.dims):
            dim.add(sh)

    @staticmethod
    def from_tensor(t: torch.Tensor) -> 'TensorShape':
        """
        Return a TensorShape object from a single tensor.
        """
        return TensorShape(t.size())

    @staticmethod
    def scalar() -> 'TensorShape':
        """
        Construct a new TensorShapes object that represents a scalar.
        """
        return TensorShape([])

@dataclass
class TensorShapes:
    """
    Shape range for a variable number of dimensions.  For when something is called
    with multiple tensor dimensions.
    """

    lengths: Dict[int, TensorShape] = field(default_factory=dict)

    def add(self, shape: TensorShape):
        l = len(shape.dims)
        if l not in self.lengths:
            self.lengths[l] = shape
        else:
            self.lengths[l].add(shape)

    def do(self, size: Sequence[SupportsInt]):
        shape = TensorShape(size)
        l = len(shape)
        if l in self.lengths:
            self.lengths[l].add(shape)
        else:
            self.lengths[l] = shape

    @staticmethod
    def from_tensor(t: torch.Tensor) -> 'TensorShapes':
        """
        Return a TensorShapes object from a single tensor.
        """
        result = TensorShapes()
        result.add(TensorShape.from_tensor(t))
        return result

    @staticmethod
    def from_shape(s: TensorShape | Sequence[ShapeRange | SupportsInt]) -> 'TensorShapes':
        """
        Construct a new TensorShapes object from a single TensorShape or dimension range.
        """
        if isinstance(s, TensorShape):
            return TensorShapes({len(s): s})
        else:
            return TensorShapes({len(s): TensorShape(s)})

    @staticmethod
    def scalar() -> 'TensorShapes':
        """
        Construct a new TensorShapes object that represents a scalar.
        """
        return TensorShapes({0: TensorShape.scalar()})

    def __repr__(self) -> str:
        return ' | '.join(str(shape) for len,shape in sorted(self.lengths.items()))

class Arg:
    """
    Base class for information about an argument to a function.
    """

    def get_type(self) -> type:
        """
        Return the type of this argument, or a superclass if the type can vary.
        This should not return NoneType unless the argument is a constant None;
        instead, is_optional() should return true.
        """
        raise RuntimeError(f"Class {self.__class__.__name__} doesn't override get_type()")

    def get_torch_type(self) -> 'torch._C.JitType':
        """
        Return the torch JIT type of this argument, or a superclass if the type can vary.
        """
        raise RuntimeError(f"Class {self.__class__.__name__} doesn't override get_torch_type()")

    def is_optional(self) -> bool:
        """
        Return true if this is optional; in other words, if None is one of the possible
        values for the argument.  The other methods return information for the non-optional
        case.
        """
        return False

    def non_optional(self) -> 'Arg':
        """
        Returns the non-optional version of this type.  Default checks that is_optional() is
        false and returns self (which works for all non-optional types).
        """
        assert not self.is_optional()
        return self

    def get_dtype(self) -> Optional[torch.dtype]:
        """
        Return the dtype of this argument, None if it's not a tensor.
        """
        return None

    def get_device(self) -> Optional[torch.device]:
        """
        Return the device of this argument, None if it's not a tensor.
        """
        return None

    def get_shape(self) -> Optional['TensorShapes']:
        """
        Return the shape of this argument, None if it's not a tensor.
        """
        return None

class UnknownArg(Arg):
    def __str__(self) -> str:
        return "Unknown()"

    def get_type(self) -> type: return object

    def get_torch_type(self) -> 'torch._C.JitType': return torch.AnyType()

class ConstantArg(Arg):
    value: Any

    def __init__(self, value: Any):
        self.value = value

    def __repr__(self) -> str:
        return f"Constant({self.value})"

    def get_type(self) -> type: return type(self.value)

    def get_torch_type(self) -> 'torch._C.JitType': return _to_torch_type(None, [self.value])

class TensorArg(Arg):
    """
    Describes a tensor-valued argument.
    """
    dtype: torch.dtype
    device: torch.device
    shape: TensorShapes

    def __init__(self, dtype: torch.dtype, device: torch.device, shape: TensorShapes):
        self.dtype = dtype
        self.device = device
        self.shape = shape

    def __repr__(self) -> str:
        return f"Tensor({self.dtype}{self.shape}{self.device})"

    def get_type(self) -> type: return torch.Tensor
    def get_torch_type(self) -> 'torch._C.JitType': return torch.TensorType.get().with_dtype(self.dtype) #.with_sizes(self.get_torch_sizes())
    def get_dtype(self) -> Optional[torch.dtype]: return self.dtype
    def get_device(self) -> Optional[torch.device]: return self.device
    def get_shape(self) -> Optional['TensorShapes']: return self.shape

class OptionalArg(Arg):
    """
    Describes an argument that can be either None or another value, representing
    optional or defaulted values.
    """
    value: Arg

    def __init__(self, value: Arg):
        self.value = value

    def __repr__(self) -> str:
        return f"Optional({self.value})"

    def is_optional(self) -> bool: return True
    def non_optional(self) -> 'Arg': return self.value
    def get_type(self) -> type: return self.value.get_type()
    def get_torch_type(self) -> 'torch._C.JitType': return torch.OptionalType(self.value.get_torch_type())
    def get_dtype(self) -> Optional[torch.dtype]: return self.value.get_dtype()
    def get_device(self) -> Optional[torch.device]: return self.value.get_device()
    def get_shape(self) -> Optional['TensorShapes']: return self.value.get_shape()

class TupleArg(Arg):
    """
    Fixed-length, non-homogeneous tuple.
    """

    values: List[Arg]

    def __init__(self, values: List[Arg]):
        self.values = values

    def __repr__(self) -> str:
        return f"Tuple({self.values})"

    def get_type(self) -> type: return tuple
    def get_torch_type(self) -> 'torch._C.JitType': return torch.TupleType([v.get_torch_type() for v in self.values])

class ListTupleArg(Arg):
    """
    Variable length, homogeneous tuple.
    """

    length: ShapeRange
    value: Arg

    def __init__(self, length: ShapeRange, value: Arg):
        self.length = length
        self.value = value

    def __repr__(self) -> str:
        return f"ListTuple({self.value}{self.length})"

    def get_type(self) -> type: return tuple
    def get_torch_type(self) -> 'torch._C.JitType': return torch.TupleType([self.value.get_torch_type()])

@dataclass
class ArgumentData:
    torch_type: 'torch._C.JitType'
    count: int = 0
    types: Dict[type, int] = field(default_factory = dict)
    tensor_dtypes: Dict[torch.dtype, int] = field(default_factory = dict)
    tensor_devices: Dict[torch.device, int] = field(default_factory = dict)
    tensor_shapes: Dict[torch.Size, int] = field(default_factory = dict)
    tensor_values: Dict[torch.Tensor, int] = field(default_factory = dict, repr=False)
    tuple_lengths: Dict[int, int] = field(default_factory=dict)
    tuple_args: List['ArgumentData'] = field(default_factory=list)
    values: Dict[Any, int] = field(default_factory = dict, repr=False)

    def _non_optional_torch_type(self) -> 'torch._C.JitType':
        """
        Returns the torch_type, ignoring optional annotations.
        """
        if isinstance(self.torch_type, torch.OptionalType):
            return self.torch_type.getElementType()
        else:
            return self.torch_type

    def is_homogeneous(self, other: 'ArgumentData'):
        """
        Tells us whether this and the other type are homogeneous and can be combined without
        much loss of generality.
        """

        #print("is_homogeneous", str(self.summarize()), str(other.summarize()))

        return str(self.summarize()) == str(other.summarize())

    def make_homogeneous(self, other: 'ArgumentData') -> 'ArgumentData':
        """
        Make a version that maps onto all of the others.
        """
        v = ArgumentData()
        v.combine(self)
        v.combine(other)
        return v

    def combine(self, other: 'ArgumentData'):
        """
        Combine the two arguments, producing one which can accept either of the inputs.
        """
        self.count += other.count
        
        def combine_types(t1: 'torch._C.JitType', t2: 'torch._C.JitType') -> 'torch._C.JitType':
            if t1 == t2:
                return t1
            raise RuntimeError(f"TODO: combine_types {t1} {t2}")

        self.type = combine_types(self.torch_type, other.torch_type)

        def update_counts(v1: Dict[Any, int], v2: Dict[Any, int]):
            for k,v in v2.items():
                v1[k] = v1.get(k, 0) + v

        update_counts(self.types, other.types)
        update_counts(self.tensor_dtypes, other.tensor_dtypes)
        update_counts(self.tensor_devices, other.tensor_devices)
        update_counts(self.tensor_shapes, other.tensor_shapes)
        update_counts(self.tensor_values, other.tensor_values)
        update_counts(self.tuple_lengths, other.tuple_lengths)
        update_counts(self.values, other.values)

        for i in range(len(other.tuple_args)):
            if i < len(self.tuple_args):
                self.tuple_args[i].combine(other.tuple_args[i])
            else:
                self.tuple_args.append(copy.copy(other.tuple_args[i]))

    def add(self, a: Any, tt: 'torch._C.JitType'):
        """
        Add the given argument instance to the analysis.
        """
        at = type(a)

        self.count += 1
        self.types[at] = self.types.get(at, 0) + 1

        if isinstance(a, torch.Tensor):
            sh = a.shape
            dt = a.dtype
            dv = a.device
            self.tensor_shapes[sh] = self.tensor_shapes.get(sh, 0) + 1
            self.tensor_dtypes[dt] = self.tensor_dtypes.get(dt, 0) + 1
            self.tensor_devices[dv] = self.tensor_devices.get(dv, 0) + 1
            self.tensor_values[a] = self.tensor_values.get(a, 0) + 1

            if len(sh) == 0:  # scalar
                pass
        elif isinstance(a, dict):
            raise RuntimeError("TODO: dict type")
            pass
        elif isinstance(a, list):
            raise RuntimeError("TODO: list type")
            pass
        elif isinstance(a, tuple):
            print("doing tuple", typeify(a))

            tl = len(a)
            self.tuple_lengths[tl] = self.tuple_lengths.get(tl, 0) + 1

            non_optional = self._non_optional_torch_type()
            print("torch_type", self.torch_type)
            print("non_optional", non_optional)

            assert isinstance(non_optional, torch.TupleType)
            element_types = non_optional.elements()
            print("element_types", element_types)
            print("tuple_args", self.tuple_args)
            print("len(element_types)", len(element_types))
            print("len(a)", len(a))

            while len(self.tuple_args) < tl:
                if len(element_types) == 1:
                    element_type = element_types[0]
                else:
                    element_type = element_types[len(self.tuple_args)]
                self.tuple_args.append(ArgumentData(torch_type=element_type))

            for i in range(tl):
                if len(element_types) == 1:
                    element_type = element_types[0]
                else:
                    element_type = element_types[i]

                print(f"adding element {i} el_type {element_type} a {typeify(a[i])}")
                self.tuple_args[i].add(a[i], element_type)
        elif isinstance(a, (float, int, str, bool)):
            self.values[a] = self.values.get(a, 0) + 1
            pass

    def summarize(self) -> Arg:
        if len(self.types) == 0:
            return UnknownArg()

        non_optional_types = self.types.copy()

        def identity(x: Arg) -> Arg:
            return x

        def make_optional(x: Arg) -> Arg:
            return OptionalArg(x)

        wrapper = identity

        if type(None) in self.types:
            if len(self.types) == 1:
                return ConstantArg(None)
            del non_optional_types[type(None)]
            wrapper = make_optional
        
        if len(non_optional_types) > 1:
            raise NotImplementedError(f"Can't handle multiple types {self.types} yet")
        first_type: type = list(non_optional_types.keys())[0]

        if len(self.values) == 1:
            first_value: Any = list(self.values.keys())[0]
            return ConstantArg(first_value)

        if issubclass(first_type, torch.Tensor):
            if len(self.tensor_dtypes) > 1:
                raise NotImplementedError("Can't handle multiple tensor types yet")
            tensor_dtype = list(self.tensor_dtypes)[0]

            if len(self.tensor_devices) > 1:
                raise NotImplementedError("Can't handle multiple tensor devices yet")
            tensor_device = list(self.tensor_devices)[0]

            shapes = TensorShapes()

            for sh,_ in self.tensor_shapes.items():
                shapes.add(TensorShape(sh))

            return wrapper(TensorArg(tensor_dtype, tensor_device, shapes))

        elif issubclass(first_type, tuple):
            if len(self.tuple_lengths) > 1:

                # Assume the tuple is like a list
                lens = ShapeRange()
                for k,l in self.tuple_lengths.items():
                    lens.do(l)

                # Homogenize the tuple length data
                arg = ArgumentData()
                for a in self.tuple_args:
                    arg.combine(a)

                return wrapper(ListTupleArg(lens, arg.summarize()))
            else:
                if len(self.tuple_args) == 0:
                    return wrapper(TupleArg([]))

                        
                try:
                    def get_homogeneous(a1: ArgumentData, a2: ArgumentData):
                        if a1.is_homogeneous(a2):
                            return a1.make_homogeneous(a2)
                        else:
                            raise RuntimeError("can't make homogeneous")

                    # Try to get a homogenous type for the tuple arguments
                    common = self.tuple_args[0]
                    for a in self.tuple_args:
                        common = get_homogeneous(common, a)
                    lens = ShapeRange()
                    lens.do(len(self.tuple_args))
                    return wrapper(ListTupleArg(lens, common.summarize()))
                except:
                    pass

                # Homogeneous length tuple, assume it's differently typed
                args = [a.summarize() for a in self.tuple_args]
                return wrapper(TupleArg(args))

        raise NotImplementedError(f"Summarize of argument of type {first_type}")

@dataclass
class VariableInfo:
    """
    Holds the static information known about a variable.
    """
    name: str = ""
    origin: Origin = Origin.UNKNOWN

    is_const: bool = False
    is_optional: bool = False
    const_type: Optional[type] = None

    # The following are for Tensors.  We model the data type, device and shape separately.
    const_dtype: Optional[torch.dtype] = None
    const_device: Optional[torch.device] = None
    tensor_shape: Optional[TensorShapes] = None

    # The following are for sequences (lists and tuples).  These are modelled
    # with:
    # 1. seq_length, which describes the length of the sequence (it can be a range, or fixed)
    # 2. seq_els, which describes the VariableInfo for the first elements
    # 3. seq_el, which descrives the VariableInfo for any which aren't covered by seq_els
    seq_length: Optional[ShapeRange] = None
    seq_els: Optional[List['VariableInfo']] = None
    seq_el: Optional['VariableInfo'] = None
    const_value: Optional[Any] = None

    # Which node produced, and which nodes read, this value.  The integers are the sequence
    # of nodes in the graph.
    produced_by: Optional[Node] = None
    produced: int = 0
    first_consumed: int = 10000
    last_consumed: int = 0

    def typed_const_value(self, tp: Type[Tp]) -> Optional[Tp]:
        """
        If this variable is a constant, check that its and instance of the given type and return it.
        If it's not a constant, return None.
        Used to extract values of constant arguments from VariableInfo during constant propagation.
        """
        if not self.is_const:
            return None
        if not isinstance(self.const_value, tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value)} of value {self.const_value}")
        return self.const_value

    def typed_const_nonnull_value(self, tp: Type[Tp]) -> Tp:
        """
        Assert that this variable is a constant, of the given type, and return that value.
        Used to extract values of constant arguments from VariableInfo during constant propagation.
        """
        if not self.is_const:
            raise RuntimeError(f"Expected constant value but got {self}")
        if not isinstance(self.const_value, tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value)} of value {self.const_value}")
        return self.const_value

    def typed_default_value(self, default: Tp) -> Optional[Tp]:
        """
        If this variable is a constant and not None, check that its and instance of the given type and return it.
        If it's a constant and None, then return the default.
        If it's not a constant, return None.
        Used to extract values of defaulted constant arguments from VariableInfo during constant propagation.
        """
        tp = type(default)
        if not self.is_const:
            return None
        if self.const_value is None:
            return default
        if not isinstance(self.const_value, tp):
            raise RuntimeError(f"Expected type {tp} but got {type(self.const_value)} of value {self.const_value}")
        return self.const_value

    def deepcopy(self) -> 'VariableInfo':
        """
        Deep copy operator.  Needed because deepcopy(x) doesn't work and some objects are
        mutable.
        """
        result = copy.copy(self)
        if self.seq_els is not None:
            result.seq_els = [el.deepcopy() for el in self.seq_els]
        if self.seq_length is not None:
            result.seq_length = copy.copy(self.seq_length)
        if self.tensor_shape is not None:
            result.tensor_shape = copy.deepcopy(self.tensor_shape)
        return result

    def renamed(self, new_name: str) -> 'VariableInfo':
        res = self.deepcopy()
        res.name = new_name
        return res

    def is_homogeneous(self, other: 'VariableInfo'):
        """
        Are these homogeneous, meaning that they can be combined into a single
        VariableInfo that covers both of them.
        """
        if self.const_type != other.const_type:
            return False
        if self.is_const and other.is_const:
            return self.const_value == other.const_value
        if self.const_type == torch.Tensor:
            type1 = self.const_dtype
            type2 = other.const_dtype

            if type1 != type2:
                # TODO: they may be homogeneous; there may be a type that covers both
                return False

            device1 = self.const_device
            device2 = other.const_device

            if device1 != device2:
                return False

            return True
        elif self.const_type == tuple or self.const_type == list:
            raise RuntimeError("is_homogeneous for sequence")

        return True

    def combine(self, other: 'VariableInfo'):
        """
        Combine the two VariableInfos together to create one that covers both.
        """

        if other.is_optional:
            self.is_optional = True

        if other.const_type != self.const_type:
            raise RuntimeError("TODO: combine with two different types")

        if self.is_const and not other.is_const:
            self.const_value = None
            self.is_const = False

        if self.const_type == torch.Tensor:
            if self.const_dtype != other.const_dtype:
                self.const_dtype = None
            if self.const_device != other.const_device:
                self.const_device = None
            if other.tensor_shape is not None and self.tensor_shape is not None:
                for sh in other.tensor_shape.lengths.values():
                    self.tensor_shape.add(sh)
        elif self.const_type == tuple or self.const_type == list:
            raise RuntimeError("TODO: combine sequences")

        self.produced = min(self.produced, other.produced)
        self.first_consumed = min(self.first_consumed, other.first_consumed)
        self.last_consumed = max(self.last_consumed, other.last_consumed)

    @staticmethod
    def constant(*, name: str, origin: Origin, value: Any, produced_by: Node, produced: int) -> 'VariableInfo':
        """
        Return the variable info for a constant.  This will fill in all of the ancilliary
        information.
        """
        dtype: Optional[torch.dtype] = None
        device: Optional[torch.device] = None
        shape: Optional[TensorShapes] = None

        if isinstance(value, torch.Tensor):
            dtype = value.dtype
            device = value.device
            shape = TensorShapes.from_tensor(value)

        return VariableInfo(name=name, origin=origin, is_const=True,
                            const_type=type(value), const_value=value,
                            const_dtype=dtype, const_device=device, tensor_shape=shape,
                            produced_by=produced_by, produced=produced)

    @staticmethod
    def argument(*, name: str, produced_by: Node, observed: 'ArgumentData', torch_type: 'torch._C.JitType') -> 'VariableInfo':
        """
        Return the variable info for an argument.  This will specialize based on the observed
        values.
        """

        summary = observed.summarize()

        dtype: Optional[torch.dtype] = summary.get_dtype()
        device: Optional[torch.device] = summary.get_device()
        shape: Optional[TensorShapes] = summary.get_shape()
        tp: Optional[Type] = summary.get_type()
        ttp: Optional['torch._C.JitType'] = summary.get_torch_type()

        tp,ttp,dtype,device,shape = _unify_types(summary, torch_type)

        result = VariableInfo(name=name, origin=Origin.ARG, is_const=False,
                              const_type=tp,
                              const_dtype=dtype, const_device=device, tensor_shape=shape,
                              produced_by=produced_by, produced=-1)

        return result

    @staticmethod
    def local(*, name: str, origin: Origin, tp: Type, produced_by: Node, produced: int) -> 'VariableInfo':
        """
        Return the variable info for a local variable.  This should not be used for
        tensors.
        """
        assert not issubclass(tp, torch.Tensor), "Tensors should use the tensor method, not local"

        return VariableInfo(name=name, origin=origin, is_const=False, const_type=tp,
                            produced_by=produced_by, produced=produced)

    @staticmethod
    def tensor(*, name: str, origin: Origin,
               dtype: Optional[torch.dtype], device: Optional[torch.device], shape: Optional[TensorShapes],
               produced_by: Node, produced: int) -> 'VariableInfo':
        """
        Return the variable info for a tensor valued variable.
        """
        return VariableInfo(name=name, origin=origin, is_const=False, const_type=torch.Tensor,
                            const_dtype=dtype, const_device=device, tensor_shape=shape,
                            produced_by=produced_by, produced=produced)

    @staticmethod
    def any(*, name: str, origin: Origin, produced_by: Node, produced: int) -> 'VariableInfo':
        """
        Return the variable info for something that could be any type (nothing static is known
        about it).
        """
        return VariableInfo(name=name, origin=origin, is_const=False,
                            produced_by=produced_by, produced=produced)

    @staticmethod
    def homogeneous_sequence(*, name: str, origin: Origin, tp: Type[Sequence], produced_by: Node, produced: int,
                             length: ShapeRange, values: 'VariableInfo') -> 'VariableInfo':
        """
        Create the VariableInfo for a homogeneous sequence (tuple or list) with a fixed or
        variable length content of an instance of a single type.
        """
        return VariableInfo(name=name, origin=origin, is_const=False, const_type=tp, seq_length=length, seq_el=values,
                            produced_by=produced_by, produced=produced)

    @staticmethod
    def inhomogeneous_sequence(*, name: str, origin: Origin, tp: Type[Sequence], produced_by: Node, produced: int,
                               values: List['VariableInfo']) -> 'VariableInfo':
        """
        Create the VariableInfo for an inhomogeneous sequence (tuple or list) with a fixed
        length and each element having a different type.
        """
        print("inhomogeneous sequence")
        for i,v in enumerate(values):
            print(_print_var(str(i), v))
        seq_length = ShapeRange(len(values))
        print("seq_length", seq_length)
        return VariableInfo(name=name, origin=origin, is_const=False, const_type=tp, seq_length=seq_length, seq_els=values,
                            produced_by=produced_by, produced=produced)

def _print_var_fields(name: str, origin: Any, const: Any, produced: Any, first_consumed: Any, last_consumed: Any, produced_by: Any, tp: Any, const_value: Any) -> str:
    return f"{name:20} {origin:10} {const:6} {produced:5} {first_consumed:5} {last_consumed:5} {produced_by:15} {tp:12} {const_value}"

def _print_var_names() -> str:
    return _print_var_fields("name", "origin", "const", "prod", "first", "last", "node", "type", "value")

def _print_tensor_info(info: VariableInfo) -> str:
    """
    Return information of a tensor: dtype, shape, device
    """
    if info.const_dtype is None:
        value_str = "<dtype?>"
    else:
        value_str = _short_dtype(info.const_dtype)

    if info.tensor_shape is None:
        value_str += "<shape?>"
    else:
        value_str += str(info.tensor_shape)

    if info.const_device is None:
        value_str += "<device?>"
    else:
        value_str += str(info.const_device)
    return value_str

def _short_info_str(info: VariableInfo) -> str:
    if info.is_const:
        return _print_value(info.const_value)
    elif info.tensor_shape is not None:
        return _print_tensor_info(info)
    elif info.const_type == list or info.const_type == tuple:
        value_str = ""
        open = "[" if info.const_type == list else '('
        close = "]" if info.const_type == list else ')'
        if info.seq_els is not None:
            el_strs: List[str] = []
            for el in info.seq_els:
                el_strs.append(_short_info_str(el))
            value_str = open + ", ".join(el_strs) + close
        elif info.seq_length is not None:
            assert info.seq_el is not None
            if info.const_type == list:
                value_str = _short_info_str(info.seq_el) + str(info.seq_length)
            else:
                value_str = "(" + ",".join([_short_info_str(info.seq_el)] * info.seq_length.min)
                if info.seq_length.max > info.seq_length.min:
                    value_str += "..." + ")" + str(info.seq_length)
                else:
                    value_str += ")"
        else:
            print("unhandled list/sequence: info", info)
            raise RuntimeError("TODO: print unhandled list/sequence case")
        return value_str
    else:
        assert info.const_type is not None
        return "<" + info.const_type.__name__ + ">"

def _print_var(name: str, info: VariableInfo) -> str:
    produced_kind = ''
    if info.produced_by is not None:
        produced_kind = info.produced_by.kind().replace("aten::", "").replace("prim::", "")
    type_str = ''
    if info.const_type is not None:
        type_str = info.const_type.__name__

    value_str: str = ""
    if info.is_const:
        value_str = _print_value(info.const_value)
    elif info.const_type == torch.Tensor:
        value_str = _print_tensor_info(info)
    elif info.const_type == list or info.const_type == tuple:
        open = "[" if info.const_type == list else '('
        close = "]" if info.const_type == list else ')'
        if info.seq_els is not None:
            el_strs: List[str] = []
            for el in info.seq_els:
                el_strs.append(_short_info_str(el))
            value_str = open + ", ".join(el_strs) + close
        elif info.seq_length is not None:
            assert info.seq_el is not None
            if info.const_type == list:
                value_str = _short_info_str(info.seq_el) + str(info.seq_length)
            else:
                value_str = "(" + ",".join([_short_info_str(info.seq_el)] * info.seq_length.min)
                if info.seq_length.max > info.seq_length.min:
                    value_str += "..." + ")" + str(info.seq_length)
                else:
                    value_str += ")"
        else:
            print("unhandled list/sequence: info", info)
            raise RuntimeError("TODO: print unhandled list/sequence case")

    else:
        pass

    return _print_var_fields(name, info.origin.name, info.is_const, info.produced, info.first_consumed,
                             info.last_consumed, produced_kind, type_str, value_str)

@dataclass
class Variables:
    vars: OrderedDict[str, VariableInfo] = field(default_factory=OrderedDict)

    def __len__(self) -> int:
        return len(self.vars)

    def add(self, v: VariableInfo):
        assert v.produced_by is None or isinstance(v.produced_by, Node)
        assert len(v.name) > 0
        assert v.name not in self.vars
        self.vars[v.name] = v

    def add_constant(self, n: str, v: Any, node: Node, i: int):
        assert isinstance(node, Node)
        info = VariableInfo(n, Origin.CONST_PROP, True, v, node, i)
        self.add(info)

    def get(self, n: str, i: int) -> VariableInfo:
        result = self.vars[n]
        if i < result.first_consumed:
            result.first_consumed = i
        if i > result.last_consumed:
            result.last_consumed = i
        return result

    def dump_vars(self, indent: str = '', start_at: int = 0):
        print(f"{indent} {fg.boldblack}{_print_var_names()}{reset}")
        for i,(name,info) in enumerate(self.vars.items()):
            if i < start_at:
                continue
            print(indent, _print_var(name, info))

def _unify_types(summary: Arg, torch_type: 'torch._C.JitType') -> Tuple[Type, 'torch._C.JitType', Optional[torch.dtype], Optional[torch.device], Optional[TensorShapes]]:

    print("torch_type", type(torch_type), torch_type, dir(torch_type), torch_type.annotation_str)
    print("summary", summary)

    if isinstance(torch_type, torch.TensorType):
        tp: Optional[Type] = summary.get_type()
        ttp: Optional['torch._C.JitType'] = summary.get_torch_type()
        dtype: Optional[torch.dtype] = summary.get_dtype()
        device: Optional[torch.device] = summary.get_device()
        shape: Optional[TensorShapes] = summary.get_shape()
        return tp,ttp,dtype,device,shape
    elif isinstance(torch_type, torch.OptionalType):
        contained = torch_type.getElementType()

        print("contained", torch_type.getElementType())
        tp,ttp,dtype,device,shape = _unify_types(summary.non_optional(), contained)
        print(f"tp {tp} torch_type {torch_type} dtype {dtype} device {device} shape {shape}")
        if summary.is_optional():
            return object,torch_type,None,None,None
            # Optional...
            raise RuntimeError("TOOD: Both Optional")
        else:
            # Not actually optional, as we've never observed without a value
            return _unify_types(summary, contained)
            raise RuntimeError("TODO: Optional")
    elif isinstance(torch_type, torch.TupleType):
        contained = torch_type.containedTypes()
        print("contained", contained)
        print("contained dir", dir(contained))
        if isinstance(summary, ListTupleArg):
            # Unify tuple with each argument
            for i in range(len(contained)):
                st = contained[i]
                tp,ttp,dtype,device,shape = _unify_types(summary.value, st)
                # TODO: return something meaningful here...
            return tuple,torch_type,None,None,None
        elif isinstance(summary, TupleArg):
            # Unify each element of the tuple
            assert len(summary.values) == len(contained)
            for c,v in zip(contained,summary.values):
                _unify_types(v,c)
            return tuple,torch_type,None,None,None
        else:
            raise RuntimeError(f"Torch is tuple but argument is {summary}")
    else:
        raise RuntimeError(f"Unknown torch_type {torch_type}")
