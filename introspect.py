import torch
import torch.jit as jit
from torch.jit import RecursiveScriptModule, ScriptModule
from torch.nn import Module
from torch.fx import symbolic_trace
from typing import Dict, List, Any, Tuple, Optional, Callable, Sequence, SupportsInt, Union, Iterator, get_origin, get_args, Set, overload
import inspect
from dataclasses import dataclass, field
import time
import copy
from runtimes import print_elapsed
from collections import defaultdict, OrderedDict
from torch.utils.hooks import RemovableHandle
from utils import _short_dtype, _print_value, _print_param, _print_value, typeify

def _is_optional(field):
    """
    Returns whether the annotation is generated by typing.Optional[...]
    """
    return get_origin(field) is Union and \
           type(None) in get_args(field)

def _to_torch_type(ann: Any, samples: List[Any]) -> 'torch._C.JitType':
    """
    Converts the given parameter into its corresponding Torch JIT type
    """

    #print(p)

    origin = get_origin(ann)
    args = get_args(ann)

    if origin is None:
        if ann is None or ann is inspect._empty:
            # Do it based on the type seen
            if len(samples) == 0:
                # NO samples and no annotation
                return torch.AnyType()
            else:
                def get_type(sample) -> 'torch._C.JitType':
                    if isinstance(sample, torch.Tensor):
                        return torch.TensorType.create_from_tensor(sample)
                    elif sample is None:
                        return torch.NoneType.get()

                    raise RuntimeError(f"TODO: get_type {type(sample)}")
                
                current: Optional['torch._C.JitType'] = None
                for t in [get_type(sample) for sample in samples]:
                    if current is None:
                        current = t
                    else:
                        if str(current) != str(t):
                            raise RuntimeError(f"wrong type {current} {t}")

                assert current is not None
                return current

            raise RuntimeError(f"empty {_print_value(samples)}")
        elif issubclass(ann, torch.Tensor):
            return torch.TensorType.create_from_tensor(ann())  
        elif issubclass(ann, torch.FloatTensor):
            return torch.TensorType.create_from_tensor(ann())
        elif ann is bool:
            return torch.BoolType.get()
        else:
            raise RuntimeError(f"TODO: no origin for {ann}")
    elif _is_optional(ann):
        contained,_ = args
        sub_samples = [s for s in samples if s is not None]
        #print("contained", contained)
        return torch.OptionalType(_to_torch_type(contained, sub_samples))
    elif origin == tuple:
        def get_el_type(i: int) -> 'torch._C.JitType':
            sub_samples = [s[i] for s in samples]
            return _to_torch_type(args[i], sub_samples)

        return torch.TupleType([get_el_type(i) for i in range(len(args))])

        raise RuntimeError(f"TODO: handle optional {contained}")


    print(ann, type(ann), get_origin(ann), get_args(ann))

    raise RuntimeError(f"_to_torch_type for {ann} {get_origin(ann)} {get_args(ann)}")

def introspect_model(m: Module):
    """
    Introspect the model, looking for what is scriptable and getting information on the
    operations used.
    """


    moduleCounts: Dict[str, int] = {}

    def recurse_module(m: Module, recursion: int, path: str):
        indent: str = ' ' * recursion * 4
        #print(f'{indent}got {m._get_name()} at {path} recursion {recursion}')
        print(f'{indent}{path} {m._get_name()}{inspect.signature(m.forward)}')
        for name,buffer in m.named_buffers(path, recurse=False):
            print(f'{indent}    buffer {name} is {_print_param(buffer.shape, buffer.dtype, buffer.device)}')
        for name,param in m.named_parameters(path, recurse=False):
            print(f'{indent}    parameter {name} is {_print_param(param.shape, param.dtype, param.device)}')

        gotScripted = None
        print_details = m._get_name() == "Embedding"

        try:
            scripted: RecursiveScriptModule = jit.script(m)
            gotScripted = scripted
            print(f"{indent}    (scripted)")
            if print_details:
                print(scripted.code)
                optimized: ScriptModule = jit.optimize_for_inference(scripted)
                print(optimized.graph)
                gotScripted = optimized
        except Exception as e:
            print(f"{indent}    (not scripted): {e}")
            pass

        n = m._get_name()
        if n in moduleCounts:
            moduleCounts[n] += 1
        else:
            moduleCounts[n] = 1

        if gotScripted is not None:
            return

        for name,child in m.named_children():
            recurse_module(child, recursion + 1, path + "." + name)

    recurse_module(m, 0, '')

    print("module counts")
    for name,count in sorted(moduleCounts.items()):
        print(f"{name:>40}:{count:5}")


class Arg:
    """
    Base class for information about an argument to a function.
    """

    def get_type(self) -> type:
        """
        Return the type of this argument, or a superclass if the type can vary.
        This should not return NoneType unless the argument is a constant None;
        instead, is_optional() should return true.
        """
        raise RuntimeError(f"Class {self.__class__.__name__} doesn't override get_type()")

    def get_torch_type(self) -> 'torch._C.JitType':
        """
        Return the torch JIT type of this argument, or a superclass if the type can vary.
        """
        raise RuntimeError(f"Class {self.__class__.__name__} doesn't override get_torch_type()")

    def is_optional(self) -> bool:
        """
        Return true if this is optional; in other words, if None is one of the possible
        values for the argument.  The other methods return information for the non-optional
        case.
        """
        return False

    def non_optional(self) -> 'Arg':
        """
        Returns the non-optional version of this type.  Default checks that is_optional() is
        false and returns self (which works for all non-optional types).
        """
        assert not self.is_optional()
        return self

    def get_dtype(self) -> Optional[torch.dtype]:
        """
        Return the dtype of this argument, None if it's not a tensor.
        """
        return None

    def get_device(self) -> Optional[torch.device]:
        """
        Return the device of this argument, None if it's not a tensor.
        """
        return None

    def get_shape(self) -> Optional['TensorShapes']:
        """
        Return the shape of this argument, None if it's not a tensor.
        """
        return None

class UnknownArg(Arg):
    def __str__(self) -> str:
        return "Unknown()"

    def get_type(self) -> type: return object

    def get_torch_type(self) -> 'torch._C.JitType': return torch.AnyType()

class ConstantArg(Arg):
    value: Any

    def __init__(self, value: Any):
        self.value = value

    def __repr__(self) -> str:
        return f"Constant({self.value})"

    def get_type(self) -> type: return type(self.value)

    def get_torch_type(self) -> 'torch._C.JitType': return _to_torch_type(None, [self.value])

@dataclass
class ShapeRange:
    min: int = 10000000000
    max: int = 0

    def is_empty(self) -> bool:
        """
        Is this an empty shape range?
        """
        return self.max < self.min

    def is_const(self) -> bool:
        """
        Is this shape a constant int value?
        """
        return self.min == self.max

    def const_value(self) -> int:
        """
        Return the constant shape for this dimension.

        PRE: is_const() is true.
        """
        if self.min != self.max:
            raise RuntimeError("asked for constant value with is_const() false")
        return self.min

    def broadcast_from(self, other: 'ShapeRange') -> None:
        """
        Updates this shape to broadcast from the other shape.  If they are incompatible for
        broadcasting, then it will throw.
        """
        if self.is_empty():
            self.min,self.max = other.min,other.max
            return
        if other.is_empty():
            return

        if self.min == 1 and self.max == 1:
            # This is a broadcastable dimension
            self.min,self.max = other.min,other.max
        elif other.min == 1 and other.max == 1:
            return
        elif self.min == other.min and self.max == other.max:
            return
        else:
            raise RuntimeError(f"non-broadcastable dimensions: {self} and {other}")

    def do(self, val: int):
        self.min = min(self.min, val)
        self.max = max(self.max, val)

    def add(self, val: 'ShapeRange'):
        if val.is_empty():
            return
        self.do(val.min)
        self.do(val.max)

    def __init__(self, val:Optional[Union[SupportsInt,'ShapeRange']] = None):
        if isinstance(val, ShapeRange):
            self.min = val.min
            self.max = val.max
        elif val is None:
            pass
        else:
            self.min = self.max = int(val)

    def __repr__(self) -> str:
        if self.is_empty():
            return "[*]"
        elif self.max == self.min:
            return f"[{self.max}]"
        else:
            return f"[{self.min}-{self.max}]"

@dataclass
class TensorShape:
    """
    Shape range for a fixed number of dimensions
    """
    dims: List[ShapeRange] = field(default_factory=list)

    def __init__(self, dims: Sequence[ShapeRange|SupportsInt]):
        self.dims = [ShapeRange(s) for s in dims]

    def __len__(self) -> int:
        return len(self.dims)

    @overload
    def __getitem__(self, item: int) -> ShapeRange: ...

    @overload
    def __getitem__(self, item: slice) -> Sequence[ShapeRange]: ...

    def __getitem__(self, item):
        return self.dims[item]

    def __iter__(self) -> Iterator[ShapeRange]:
        return iter(self.dims)

    def __repr__(self) -> str:
        return ''.join([str(s) for s in self.dims])

    def do(self, shape: List[int]):
        assert len(shape) == len(self)
        for dim,sh in zip(self.dims, shape):
            dim.do(sh)

    def add(self, shape: 'TensorShape'):
        assert len(shape) == len(self)
        for dim,sh in zip(self.dims, shape.dims):
            dim.add(sh)

    @staticmethod
    def from_tensor(t: torch.Tensor) -> 'TensorShape':
        """
        Return a TensorShape object from a single tensor.
        """
        return TensorShape(t.size())

    @staticmethod
    def scalar() -> 'TensorShape':
        """
        Construct a new TensorShapes object that represents a scalar.
        """
        return TensorShape([])

@dataclass
class TensorShapes:
    """
    Shape range for a variable number of dimensions.  For when something is called
    with multiple tensor dimensions.
    """

    lengths: Dict[int, TensorShape] = field(default_factory=dict)

    def add(self, shape: TensorShape):
        l = len(shape.dims)
        if l not in self.lengths:
            self.lengths[l] = shape
        else:
            self.lengths[l].add(shape)

    def do(self, size: Sequence[SupportsInt]):
        shape = TensorShape(size)
        l = len(shape)
        if l in self.lengths:
            self.lengths[l].add(shape)
        else:
            self.lengths[l] = shape

    @staticmethod
    def from_tensor(t: torch.Tensor) -> 'TensorShapes':
        """
        Return a TensorShapes object from a single tensor.
        """
        result = TensorShapes()
        result.add(TensorShape.from_tensor(t))
        return result

    @staticmethod
    def from_shape(s: TensorShape | Sequence[ShapeRange | SupportsInt]) -> 'TensorShapes':
        """
        Construct a new TensorShapes object from a single TensorShape or dimension range.
        """
        if isinstance(s, TensorShape):
            return TensorShapes({len(s): s})
        else:
            return TensorShapes({len(s): TensorShape(s)})

    @staticmethod
    def scalar() -> 'TensorShapes':
        """
        Construct a new TensorShapes object that represents a scalar.
        """
        return TensorShapes({0: TensorShape.scalar()})

    def __repr__(self) -> str:
        return ' | '.join(str(shape) for len,shape in sorted(self.lengths.items()))


class TensorArg(Arg):
    """
    Describes a tensor-valued argument.
    """
    dtype: torch.dtype
    device: torch.device
    shape: TensorShapes

    def __init__(self, dtype: torch.dtype, device: torch.device, shape: TensorShapes):
        self.dtype = dtype
        self.device = device
        self.shape = shape

    def __repr__(self) -> str:
        return f"Tensor({self.dtype}{self.shape}{self.device})"

    def get_type(self) -> type: return torch.Tensor
    def get_torch_type(self) -> 'torch._C.JitType': return torch.TensorType.get().with_dtype(self.dtype) #.with_sizes(self.get_torch_sizes())
    def get_dtype(self) -> Optional[torch.dtype]: return self.dtype
    def get_device(self) -> Optional[torch.device]: return self.device
    def get_shape(self) -> Optional['TensorShapes']: return self.shape

class OptionalArg(Arg):
    """
    Describes an argument that can be either None or another value, representing
    optional or defaulted values.
    """
    value: Arg

    def __init__(self, value: Arg):
        self.value = value

    def __repr__(self) -> str:
        return f"Optional({self.value})"

    def is_optional(self) -> bool: return True
    def non_optional(self) -> 'Arg': return self.value
    def get_type(self) -> type: return self.value.get_type()
    def get_torch_type(self) -> 'torch._C.JitType': return torch.OptionalType(self.value.get_torch_type())
    def get_dtype(self) -> Optional[torch.dtype]: return self.value.get_dtype()
    def get_device(self) -> Optional[torch.device]: return self.value.get_device()
    def get_shape(self) -> Optional['TensorShapes']: return self.value.get_shape()

class TupleArg(Arg):
    """
    Fixed-length, non-homogeneous tuple.
    """

    values: List[Arg]

    def __init__(self, values: List[Arg]):
        self.values = values

    def __repr__(self) -> str:
        return f"Tuple({self.values})"

    def get_type(self) -> type: return tuple
    def get_torch_type(self) -> 'torch._C.JitType': return torch.TupleType([v.get_torch_type() for v in self.values])

class ListTupleArg(Arg):
    """
    Variable length, homogeneous tuple.
    """

    length: ShapeRange
    value: Arg

    def __init__(self, length: ShapeRange, value: Arg):
        self.length = length
        self.value = value

    def __repr__(self) -> str:
        return f"ListTuple({self.value}{self.length})"

    def get_type(self) -> type: return tuple
    def get_torch_type(self) -> 'torch._C.JitType': return torch.TupleType([self.value.get_torch_type()])

@dataclass
class Invocation:
    args: Tuple
    kwargs: OrderedDict[str, Any] = field(default_factory = OrderedDict)
    output: Tuple = field(default_factory = tuple)
    elapsed: float = 0

    def __str__(self) -> str:
        def summarize_arg(arg: Any) -> str:
            if isinstance(arg, dict):
                return str(dict({k: summarize_arg(v) for k,v in arg.items()}))
            elif isinstance(arg, tuple):
                return str(tuple(summarize_arg(v) for v in arg))
            elif isinstance(arg, list):
                return str(list([summarize_arg(v) for v in arg]))
            elif isinstance(arg, torch.Tensor):
                if arg.numel() < 10:
                    return str(arg)
                else:
                    return _print_param(arg.size(), arg.dtype, arg.device) + " " + str(arg.device)
            else:
                return str(arg)

        summarized_args = list(map(summarize_arg, self.args))
        summarized_kwargs = {k: summarize_arg(v) for k,v in self.kwargs.items()}

        return f"Invocation(elapsed={print_elapsed(self.elapsed)} args={summarized_args} kwargs={summarized_kwargs})"

@dataclass
class ArgumentData:
    torch_type: 'torch._C.JitType'
    count: int = 0
    types: Dict[type, int] = field(default_factory = dict)
    tensor_dtypes: Dict[torch.dtype, int] = field(default_factory = dict)
    tensor_devices: Dict[torch.device, int] = field(default_factory = dict)
    tensor_shapes: Dict[torch.Size, int] = field(default_factory = dict)
    tensor_values: Dict[torch.Tensor, int] = field(default_factory = dict, repr=False)
    tuple_lengths: Dict[int, int] = field(default_factory=dict)
    tuple_args: List['ArgumentData'] = field(default_factory=list)
    values: Dict[Any, int] = field(default_factory = dict, repr=False)

    def _non_optional_torch_type(self) -> 'torch._C.JitType':
        """
        Returns the torch_type, ignoring optional annotations.
        """
        if isinstance(self.torch_type, torch.OptionalType):
            return self.torch_type.getElementType()
        else:
            return self.torch_type

    def is_homogeneous(self, other: 'ArgumentData'):
        """
        Tells us whether this and the other type are homogeneous and can be combined without
        much loss of generality.
        """

        #print("is_homogeneous", str(self.summarize()), str(other.summarize()))

        return str(self.summarize()) == str(other.summarize())

    def make_homogeneous(self, other: 'ArgumentData') -> 'ArgumentData':
        """
        Make a version that maps onto all of the others.
        """
        v = ArgumentData()
        v.combine(self)
        v.combine(other)
        return v

    def combine(self, other: 'ArgumentData'):
        """
        Combine the two arguments, producing one which can accept either of the inputs.
        """
        self.count += other.count
        
        def combine_types(t1: 'torch._C.JitType', t2: 'torch._C.JitType') -> 'torch._C.JitType':
            if t1 == t2:
                return t1
            raise RuntimeError(f"TODO: combine_types {t1} {t2}")

        self.type = combine_types(self.torch_type, other.torch_type)

        def update_counts(v1: Dict[Any, int], v2: Dict[Any, int]):
            for k,v in v2.items():
                v1[k] = v1.get(k, 0) + v

        update_counts(self.types, other.types)
        update_counts(self.tensor_dtypes, other.tensor_dtypes)
        update_counts(self.tensor_devices, other.tensor_devices)
        update_counts(self.tensor_shapes, other.tensor_shapes)
        update_counts(self.tensor_values, other.tensor_values)
        update_counts(self.tuple_lengths, other.tuple_lengths)
        update_counts(self.values, other.values)

        for i in range(len(other.tuple_args)):
            if i < len(self.tuple_args):
                self.tuple_args[i].combine(other.tuple_args[i])
            else:
                self.tuple_args.append(copy.copy(other.tuple_args[i]))

    def add(self, a: Any, tt: 'torch._C.JitType'):
        """
        Add the given argument instance to the analysis.
        """
        at = type(a)

        self.count += 1
        self.types[at] = self.types.get(at, 0) + 1

        if isinstance(a, torch.Tensor):
            sh = a.shape
            dt = a.dtype
            dv = a.device
            self.tensor_shapes[sh] = self.tensor_shapes.get(sh, 0) + 1
            self.tensor_dtypes[dt] = self.tensor_dtypes.get(dt, 0) + 1
            self.tensor_devices[dv] = self.tensor_devices.get(dv, 0) + 1
            self.tensor_values[a] = self.tensor_values.get(a, 0) + 1

            if len(sh) == 0:  # scalar
                pass
        elif isinstance(a, dict):
            raise RuntimeError("TODO: dict type")
            pass
        elif isinstance(a, list):
            raise RuntimeError("TODO: list type")
            pass
        elif isinstance(a, tuple):
            print("doing tuple", typeify(a))

            tl = len(a)
            self.tuple_lengths[tl] = self.tuple_lengths.get(tl, 0) + 1

            non_optional = self._non_optional_torch_type()
            print("torch_type", self.torch_type)
            print("non_optional", non_optional)

            assert isinstance(non_optional, torch.TupleType)
            element_types = non_optional.elements()
            print("element_types", element_types)
            print("tuple_args", self.tuple_args)
            print("len(element_types)", len(element_types))
            print("len(a)", len(a))

            while len(self.tuple_args) < tl:
                if len(element_types) == 1:
                    element_type = element_types[0]
                else:
                    element_type = element_types[len(self.tuple_args)]
                self.tuple_args.append(ArgumentData(torch_type=element_type))

            for i in range(tl):
                if len(element_types) == 1:
                    element_type = element_types[0]
                else:
                    element_type = element_types[i]

                print(f"adding element {i} el_type {element_type} a {typeify(a[i])}")
                self.tuple_args[i].add(a[i], element_type)
        elif isinstance(a, (float, int, str, bool)):
            self.values[a] = self.values.get(a, 0) + 1
            pass

    def summarize(self) -> Arg:
        if len(self.types) == 0:
            return UnknownArg()

        non_optional_types = self.types.copy()

        def identity(x: Arg) -> Arg:
            return x

        def make_optional(x: Arg) -> Arg:
            return OptionalArg(x)

        wrapper = identity

        if type(None) in self.types:
            if len(self.types) == 1:
                return ConstantArg(None)
            del non_optional_types[type(None)]
            wrapper = make_optional
        
        if len(non_optional_types) > 1:
            raise NotImplementedError(f"Can't handle multiple types {self.types} yet")
        first_type: type = list(non_optional_types.keys())[0]

        if len(self.values) == 1:
            first_value: Any = list(self.values.keys())[0]
            return ConstantArg(first_value)

        if issubclass(first_type, torch.Tensor):
            if len(self.tensor_dtypes) > 1:
                raise NotImplementedError("Can't handle multiple tensor types yet")
            tensor_dtype = list(self.tensor_dtypes)[0]

            if len(self.tensor_devices) > 1:
                raise NotImplementedError("Can't handle multiple tensor devices yet")
            tensor_device = list(self.tensor_devices)[0]

            shapes = TensorShapes()

            for sh,_ in self.tensor_shapes.items():
                shapes.add(TensorShape(sh))

            return wrapper(TensorArg(tensor_dtype, tensor_device, shapes))

        elif issubclass(first_type, tuple):
            if len(self.tuple_lengths) > 1:

                # Assume the tuple is like a list
                lens = ShapeRange()
                for k,l in self.tuple_lengths.items():
                    lens.do(l)

                # Homogenize the tuple length data
                arg = ArgumentData()
                for a in self.tuple_args:
                    arg.combine(a)

                return wrapper(ListTupleArg(lens, arg.summarize()))
            else:
                if len(self.tuple_args) == 0:
                    return wrapper(TupleArg([]))

                        
                try:
                    def get_homogeneous(a1: ArgumentData, a2: ArgumentData):
                        if a1.is_homogeneous(a2):
                            return a1.make_homogeneous(a2)
                        else:
                            raise RuntimeError("can't make homogeneous")

                    # Try to get a homogenous type for the tuple arguments
                    common = self.tuple_args[0]
                    for a in self.tuple_args:
                        common = get_homogeneous(common, a)
                    lens = ShapeRange()
                    lens.do(len(self.tuple_args))
                    return wrapper(ListTupleArg(lens, common.summarize()))
                except:
                    pass

                # Homogeneous length tuple, assume it's differently typed
                args = [a.summarize() for a in self.tuple_args]
                return wrapper(TupleArg(args))

        raise NotImplementedError(f"Summarize of argument of type {first_type}")

@dataclass
class SummaryData:
    arg_lengths: Dict[int, int] = field(default_factory = dict)
    args: List[ArgumentData] = field(default_factory = list)
    kwargs: Dict[str, ArgumentData] = field(default_factory = dict)

    def print_args(self, indent: int = 0):
        ind = ' ' * indent
        for i in range(len(self.args)):
            arg = self.args[i]
            print(f"{ind}{i}: {arg.summarize()}")

        for kw,arg in self.kwargs.items():
            print(f"{ind}{kw}: {arg.summarize()}")

    def add(self, other: 'SummaryData'):
        """
        Add another summary to make a combined summary.
        """
        for l,n in other.arg_lengths.items():
            self.arg_lengths[l] = self.arg_lengths.get(l, 0) + n

        for i,ad in enumerate(other.args):
            if i < len(self.args):
                self.args[i].combine(ad)
            else:
                self.args.append(ad)

        for n,ad in other.kwargs.items():
            if n in self.kwargs:
                self.kwargs[n].combine(ad)
            else:
                self.kwargs[n] = ad

@dataclass
class Invocations:
    m: Module
    path: str
    sig: inspect.Signature
    calls: List[Invocation] = field(default_factory = list)
    children: Dict[str, 'Invocations'] = field(default_factory = dict)

    def __init__(self, m: Module, path: str, *,
                 sig: Optional[inspect.Signature] = None,
                 calls: Optional[List[Invocation]] = None,
                 children: Optional[Dict[str, 'Invocations']] = None):
        self.m = m
        self.path = path
        self.sig = inspect.signature(m.forward) if sig is None else sig
        self.calls = [] if calls is None else calls
        self.children = {} if children is None else children

    def total_runtime(self) -> float:
        return sum([c.elapsed for c in self.calls])

    def __str__(self) -> str:
        return f"Invocations(path={self.path} module={self.m._get_name()} runtime={print_elapsed(self.total_runtime())} ncalls={len(self.calls)} nchildren={len(self.children)})" # sig={inspect.signature(self.m.forward)})"

    def summarize(self) -> SummaryData:
        result = SummaryData()

        max_nargs = 0
        all_kwargs: Set[str] = set()
        for c in self.calls:
            nargs = len(c.args)
            result.arg_lengths[nargs] = result.arg_lengths.get(nargs, 0) + 1
            max_nargs = max(max_nargs, nargs)
            all_kwargs.update(c.kwargs.keys())

        print("max_nargs", max_nargs)
        print("parameters=", self.sig.parameters)
        ordered_params: List[Tuple[str, inspect.Parameter]] = list(self.sig.parameters.items())

        for i in range(max_nargs):
            name,param = ordered_params[i]
            print("param", name, param)
            samples = [c.args[i] for c in self.calls if i < len(c.args[i])]
            torch_type = _to_torch_type(param.annotation, samples)
            result.args.append(ArgumentData(torch_type=torch_type))

        for k in all_kwargs:
            param = self.sig.parameters[k]
            samples = [c.kwargs.get(k) for c in self.calls if k in c.kwargs]
            torch_type = _to_torch_type(param.annotation, samples)
            result.kwargs[k] = ArgumentData(torch_type=torch_type)

        for c in self.calls:
            for i in range(len(c.args)):
                a = c.args[i]
                ad = result.args[i]
                name,param = ordered_params[i]
                #print("param", name, param)
                torch_type = _to_torch_type(param.annotation, [a])
                ad.add(a, torch_type)

            for k,a in c.kwargs.items():
                #print("parameter", k)
                param = self.sig.parameters[k]
                torch_type = _to_torch_type(param.annotation, [a])
                ad = result.kwargs[k]
                ad.add(a, torch_type)

        return result

def record_invocations(model: Module) -> Tuple[Invocations, Callable]:
    """
    Records the calls to the model and the arguments that are passed.
    """

    handles: List[RemovableHandle] = []

    def recurse_module(m: Module, recursion: int, path: str) -> Invocations:
        invs = Invocations(m, path)

        def pre_hook(module: Module, args: Tuple, kwargs: OrderedDict[str,Any]):
            #inv = Invocation(copy.deepcopy(args), copy.deepcopy(kwargs), (), time.time())
            inv = Invocation(args, kwargs, (), time.time())
            invs.calls.append(inv)
            return None

        def post_hook(module: Module, args: Tuple[Any], kwargs: Dict[str, Any], output: Tuple):
            inv = invs.calls[-1]
            inv.output = output
            inv.elapsed = time.time() - inv.elapsed
            return None

        handle1 = m.register_forward_pre_hook(pre_hook, with_kwargs=True)
        handle2 = m.register_forward_hook(post_hook, with_kwargs = True)

        for name,child in m.named_children():
            invs.children[name] = recurse_module(child, recursion + 1, path + "." + name)

        handles.append(handle1)
        handles.append(handle2)

        return invs

    def remove_hooks():
        for h in handles:
            try:
                h.remove()
            except:
                pass

    try:
        return recurse_module(model, 0, ''), remove_hooks
    except:
        remove_hooks()
        raise